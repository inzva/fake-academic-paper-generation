
%% bare_conf.tex
%% V1.4
%% 2012/12/27
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex,
%%                    bare_jrnl_transmag.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage[pdftex]{graphicx}
\usepackage{amsmath}
\usepackage{microtype}
\usepackage{textcomp}
\usepackage{multicol}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning,shapes.geometric}
\IEEEoverridecommandlockouts 
\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}



\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{Traffic Sign Classification Using Deep Inception Based Convolutional Networks}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Mrinal Haloi}
\IEEEauthorblockA{IIT Guwahati$ ^1$\\mrinal.haloi11@gmail.com}
\thanks{$ ^1$Indian Institute of Technology, Guwahati.\
\textcopyright~2015 Mrinal Haloi}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
In this work, we propose a novel deep network for traffic sign classification that achieves outstanding performance on GTSRB surpassing all previous methods. Our deep network consists of spatial transformer layers and a modified version of inception module specifically designed for capturing local and global features together. This features adoption allows our network to classify precisely intraclass samples even under deformations. Use of spatial transformer layer makes this network more robust to deformations such as translation, rotation, scaling of input images. Unlike existing approaches that are developed with hand-crafted features, multiple deep networks with huge parameters and data augmentations, our method addresses the concern of exploding parameters and augmentations. We have achieved the state-of-the-art performance of 99.81\% on GTSRB dataset.   


Keywords: Advanced Driver Assistant System, Image Classification, Traffic Sign, Deep Networks


\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
Traffic signs classification is one of the foremost important integral parts of autonomous vehicles and advanced driver assistance systems (ADAS) \cite{haloi1, trivedisurvey, haloi2, haloi3, activity}. Most of the time driver missed traffic 
signs due to different obstacles and lack of attentiveness. Automating the process of classification of the traffic signs would help reducing accidents.
Traditional computer vision and machine learning based methods were widely used for traffic signs 
classification \cite{le,plsa}, but those methods were soon replaced by deep learning based classifiers. Recently deep convolutional networks have surpassed traditional learning methods in traffic signs classification. With the rapid advances of deep learning algorithm structures and feasibility of its high performance implementation with graphical processing units (GPU), it is advantageous to relook the traffic signs classification problems from the efficient deep learning perspective.       
 Classification of traffic signs is not so simple task, images are effected to adverse variation due to illumination, orientation, the speed variation of vehicles etc. Normally wide angle camera is mounted on the top of a vehicle to capture traffic signs and other related visual features for ADAS. This images are distorted due to several external factors including vehicles speed, sunlight, rain etc. Sample images from GTSRB dataset are shown in Fig.~\ref{fig:intro}.
 \begin{figure}
  \centering
      \includegraphics[width=3.3in,height=2.0in]{intro}
\caption{Sample images of GTSRB dataset}
\label{fig:intro}
\end{figure}

 In this work, we have developed a new system for classification on the top of existing deep learning methods. We aim to address the traditional hand crafted data augmentation problem and also the reduction of an overwhelming number of parameters need to be learned by the system. With the reduction of a number of parameters, lesser number of computations and memory will be used. Instead of using a single sized filter for one convolutional layer, we would use multiple sized filters and concatenate those convolutional filters response to get more abstract representations in one layer. Also, there are disadvantages of using hand crafted data augmentation on some specific datasets for learning parameters that would be used for general purpose. Hence learning parameters for data modification inside network is a far better way to improve the accuracy of classification.   
 A modified inception module suitable for traffic sign classification is proposed to use with GoogLeNet \cite{googlenet} and to get way with traditional data augmentation we have incorporated spatial transformer network \cite{stn} layer with our proposed network. In terms of accuracy and number of parameters, this method has surpassed state of the art method for traffic sign classification.

 
Traffic sign classification becomes a mature area with the increasing focus on autonomous driving research. Notable research work exists on detection and classification traffic signs for advanced driver assistance systems. Most of the works attempted to address the challenged involved in
real life problems due to scaling, rotation, blurring etc. We will go through the overview  of  some relevant works since it is not possible to discuss all those research works. Most  of  the  works based on computer vision and machine learning algorithms which use data from several camera sensors mounted on the car roof at different angles. In some of the work, researchers explore
detection based on colour features, such as converting the colour space from RGB to HSV and then using colour thresholding method for detection and classification by using support vector  machine. In colour thresholding approach morphological operation like connected component
analysis was done for accurate location. Bahlmann et al\cite{bah} have used colour, shape, motion information and haar wavelet based features for detection, classification of the traffic sign. By using SVM based colour classification on a block of pixels Le et al \cite{le} addressed the problems of weather variation. German Traffic Sign Recognition Benchmark (GTSRB) is one of the reliable datasets for testing and validating traffic sign classification and detection algorithms. In the competition of GTSRB, top-performing algorithm exceeds best human classification accuracy. By using committee of neural networks Ciresan et al \cite{committe} achieved highest ever
performance of 99.46\%, which surpassed the best human performance of 98.84\%. Their proposed committee composed of 25 networks each having 3 convolutional and 2 fully connected networks with traditional data augmentations and jittering. The main disadvantages of this committe are multiples networks, a huge number of parameters ( around 90Millions) and dataset dependent handcrafted augmentations. Sermanet et al. proposed multi-scale convolutional network \cite{multi} with 2 different features stages, which has achieved 98.31\% accuracy in this dataset. In our previous work \cite{plsa} a probabilistic latent semantic analysis based model was proposed, which was built upon traditional handcrafted features extraction methods. Also, other algorithm based on k-d trees and random forest \cite{rf} achieved significant accuracy.

\section{Method}
Traffic signs classification are affected due to contrast variation, rotational and translational changes. It is possible
to nullify the effect of spatial transformations in an image undergo due to varying speed of vehicles camera by using multiple
transformations to the input image. But these handcrafted transformations are not effective always and vary with scenarios.
In this work, a spatial transformer network \cite{stn} capable of generating automatic transformation of input image is used to
make classification more robust and accurate along with a modified version of GoogLeNet \cite{googlenet}.

\subsection{Transformation invariant}
Due to a moving camera, a image undergoes deformation like blurring, translational deformation, rotational
and scale deformations, skew etc. For classification feature map (include input image batch) would be passed through layers of 
spatial transformers. Spatial transformations modules are differentiable hence could be used with 
backpropagation algorithm for training. Spatial transformer layers consist of three parts such as 
localisation network, grid generator and the sampling unit. Figure~\ref{fig:loc} shows spatial transformer network with its components. This layer could be inserted at any point 
into the CNN network and it is efficient to deal with due to its very low computational overhead. Using
this layer with CNN obviated the use of handcrafted data augmentation such as translation, rotation etc.
and allows the network to learn active transformation of features map. Localisation network can be fully connected or convolutional 
neural network with one mandatory final regression layer to generate parameters $\theta$. Dimension of 
$ \theta $ depends on the parameterized transformation $ \tau_{\theta} $ type. To compute parameters 
localisation network may take input image or input feature map $ U \in \Re^{H \times W \times C} $ with 
$ W, H, C $ are the width, height and channels respectively. Localisation network can deal with multiples 
channels. Also localisation network Fig.~\ref{fig:locnet} may take any number of convolutional and fully connected layers as per application requirement. Using parameters produced by localisation network, grid generator creates set of points known 
as sampling grid. Sampling grid and input feature map (or input image) is used by a sampler to generate 
the transformed output map. Each pixel of the output feature map is computed using sampling kernel
centred at a definite input feature map location.
\begin{equation}
\label{eq:relation}
	\begin{aligned}
		\theta = f_{loc}(U) \\
		U \in \Re^{H \times W \times C}
	\end{aligned}
\end{equation}
For a input feature map pixel $ (x_{i}, y_{i})$ and learned 2D affine transformation parameters $ \tau_{\theta} $, the output 
feature map pixel $ (x_{i}^{o}, y_{i}^{o})$ is computed as follows
\begin{equation}
\label{eq:affine}
	\begin{aligned}
		\colvec{2}{x_{i}}{ y_{i}} =  \tau_{\theta}\colvec{3}{x_{i}^{o}}{y_{i}^{o}}{1}\\
		\tau_{\theta} = 
		\begin{bmatrix}
			\theta_{11} & \theta_{12} & \theta_{13}\\
			\theta_{21} & \theta_{22} & \theta_{23}
		\end{bmatrix}
	\end{aligned}
\end{equation}
Using the transformation defined in \eqref{eq:affine} we can perform operations such as translation, rotation, scale, skew and cropping in the input feature map. Parameters $ \theta$ are computed using equation \eqref{eq:relation}. Interestingly this transformation requires only 6 parameters to be learned by localisation network.

\begin{figure}
  \centering
      \includegraphics[width=3.3in,height=2.0in]{loc}
\caption{Transformer Layer \cite{stn}}
\label{fig:loc}
\end{figure}


\begin{figure}
\begin{tikzpicture}[>=latex']
        \tikzset{block/.style= {draw, rectangle, align=center,minimum width=1.2cm,minimum height=1cm},
        rblock/.style={draw, shape=rectangle,rounded corners=1.5em,align=center,minimum width=2cm,minimum height=1cm},
        input/.style={ % requires library shapes.geometric
        draw,
        trapezium,
        trapezium left angle=60,
        trapezium right angle=120,
        minimum width=1cm,
        align=center,
        minimum height=.5cm
    },
        }
        \node[rblock]  (start) {Feature Map};
        \node[block, right =.8cm of start] (acquire) {conv};
        \node[block, right =.8cm of acquire] (rgb2gray) {max};
        \node[block, below right =1.5cm and -1cm of start] (otsu) {conv};
        \node[block, right =.5cm of otsu] (gc) {max};
        \node[block, right =.5cm of gc] (gchannel) {fc};
        \node[block, right =.5cm of gchannel] (closing) {fc};
        \node[block, below =.9cm of closing] (out) {$ [\theta] $};
        \node[coordinate, below right =1cm and 1cm of rgb2gray] (right) {};
        \node[coordinate,above left =1cm and 1cm of otsu] (left) {};

%% paths
        \path[draw,->] (start) edge (acquire)
                    (acquire) edge (rgb2gray)
                    (rgb2gray.east) -| (right) -- (left) |- (otsu)
                    (otsu) edge (gc)
                    (gc) edge (gchannel)
                    (gchannel) edge (closing)
                    (closing) edge (out)
                    ;
 \end{tikzpicture}
       \caption{Localisation Network}
       \label{fig:locnet}
\end{figure}




\begin{figure}
  \centering
      \includegraphics[width=3.3in,height=2.0in]{inception}
\caption{Google Proposed Inception module}
\label{fig:gincept}
\end{figure}

\begin{figure}
  \centering
      \includegraphics[width=3.3in,height=2.0in]{myinception}
\caption{Our Modified Inception module for Traffic Signs Classification}
\label{fig:myincept}
\end{figure}

\subsection{Proposed Pipeline}
A modified version of GoogLeNet \cite{googlenet} with batch normalization \cite{batch} is used as parent network for the classification task. GoogLeNet is based on the Inception architecture. Several Inception modules stacked upon each other to produce the final output. At the inception module varied size of convolutional filters were use to capture features of different abstraction. High level of abstraction is captured with filters of higher size and that of a lower level using small size filters. Processing
visual information at different scales and aggregating them result in an efficient level of abstraction. Since directly applying more
convolutional filters with image data and concatenating them is computationally expensive, so in the final Inception model a dimensionality reduction filters was used very applying abstraction level filters. For dimensionality reduction $ 1 \times 1 $ convolutional filters are used. Besides being very successful for dimensionality reduction, this filters also come to be useful as rectified linear activation. Inception architecture is efficient in terms of computational complexity with respect to number of units at each stage. 
 For our classification task a modified version of Inception module is used. For traffic sign classification local abstract features play important role. Signs belonging to same group have slight difference in local structure with each other, which make it hard to distinguish. A extra $ 3 \times 3 $ convolutional reduction kernel is added with max pooling at the top of it to capture discriminative local structure at the beginning itself. Signs belonging to different groups has global abstraction which can be captured using $ 5 \times 5 $ convolutional reduction kernel. Improved performance is observed with this architecture over normal Inception module. Figure~\ref{fig:gincept} shows Google inception module and Fig.~\ref{fig:myincept} shows our proposed inception module.





\begin{figure*}
  \centering
      \includegraphics[width=6.3in,height=2.0in]{network}
\caption{Proposed network with Modified Inception}
\label{fig:network}
\end{figure*}



% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results.}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.


\begin{table*}[t]
\caption{Detailed description of deep networks parameters}
\label{tab:network}
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
 type & RF size/stride & output size & \#1x1 & \#3x3 reduce & \#3x3 & \#5x5 reduce & \#5x5 & \#3x3 reduce pool & \#3x3 & params \\
 \hline
 \hline
 ST1 & & & & & & & & & & 3M \\
 \hline
 conv & 5x5/2 &64x64x64 &  &   &   &   &   &  & & 1.6K \\
 \hline
 max-pool & 3x3/2 & 32x32x64&  &   &   &   &   & &  &  \\
 \hline
 ST2 & & & & & & & & & & 891K\\
 \hline
 conv & 3x3/1 &  32x32x192 &  &    &  192 &   &   &  & & 110K \\
 \hline
 max-pool & 3x3/2 &  16x16x192  &  &   &   &   &   &  & &  \\
 \hline
 ST3 & & & & & & & & & & 1M\\
 \hline
 mIncept(3a) &  &  16x16x288  & 64  & 96  & 128  & 16  & 32  & 64 & 64 & 206K \\
 \hline
 ST3 & & & & & & & & & & 1M\\
 \hline
 mIncept(3b) & &   16x16x480 &  128 & 128  & 192  & 32  & 96  & 64 & 64 & 436K \\
 \hline
 max-pool & 3x3/2 &  8x8x480 &  &   &   &   &   &   & & \\
 \hline
 mIncept(4a) &  &  8x8x512  & 192  & 96  & 208  & 16  & 48  & 48 & 64 & 395K \\
 \hline
 mIncept(4b) &  &    8x8x512 & 160 & 112  & 224  & 24  & 64  & 48 & 64 & 467K \\
 \hline
 mIncept(4c) &  &    8x8x512 & 128 & 128  & 256  & 24  & 64  &  64& 64& 546K \\
 \hline
 mIncept(4d) &  &    8x8x528 & 112 & 144  & 288  & 32  & 64  & 48 & 64 & 624K \\
 \hline
 mIncept(4e) &  &   8x8x832 & 256 & 160  &  320 & 32  & 128  & 48 & 128 & 880K \\
 \hline
 max-pool & 3x3/2 & 4x4x832   &  &   &   &   &   &  & &  \\
 \hline
 mIncept(5a) & &  4x4x832 & 256  & 160  & 320  & 32  & 128 & 48& 128& 965K \\
 \hline
 mIncept(5b) &  &    4x4x1024 & 320 & 192  & 320  & 48  & 128  & 32 & 256 & 1.2M \\
 \hline
 avg-pool & 4x4/1 &  1x1x1024  &  &   &   &   &   &  & &  \\
 \hline
 dropout &  &  1x1x1024   &  &   &   &   &   &  & &  \\
 \hline
 linear &  &  1x1x43  &  &   &   &   &   &  &  & 44K \\
 \hline
 softmax &  &  1x1x43  &  &   &   &   &   &  &  &  \\

\hline
 \end{tabular}
\end{table*}

\section{Results and Discussions}
We extensively evaluate our proposed deep networks on GTSRB (German Traffic Sign Recognition Benchmark) \cite{gtsrb} using our modified networks and also with original GoogLeNet. GTSRB is the standard state-of-the-art revelation benchmark for traffic sign recognition/classification. There are significant similarities of German traffic sign with other European countries and with Indian conventions, which make it suitable to explore. 
\subsection{Training and Datasets}
The proposed network was trained and tested using the machine learning library Torch \cite{torch} and two NVIDIA Tesla K40c GPU. For the implementation of the spatial transformer network, stn \cite{stnp} package was used.

For training and testing, GTSRB dataset contains 51839 images in 43 classes. We have selected 39,209 images for training and rest for testing. Images with deformation due to viewpoint variation, occlusion due to obstacles like trees, building etc., natural degrading, weather condition are considered in this dataset. We have resized all input images to $ 128 \times 128 $ using cubic interpolation method. 

For training, we have used SGD with momentum, with minibatch size of 20 images and learning rate of 0.00032. Dropout (40\%) was used for the fully connected layer. For SGD we have used momentum 0.9 with weight decay of 0.0918. Also, it has been observed that learning rate primarily influence training process. For activation Parametric Rectified Linear Unit (PReLU) \cite{prelu} is used. Instead of using parameter free ReLU, we have used PReLU for better accuracy. Parameters of PReLU are learned during training of the network. Also, networks weights are initialized using MSRA \cite{prelu} methods, which proved to be useful for PReLU activation unit based networks.
In Table~\ref{tab:network} a detailed description of proposed network is given. Number of $ 1 \times 1$ filters used for dimensionality reduction before $ 3 \times 3$ and $ 5 \times 5$ convolutions are referred as ``\#$ 3\times 3$ reduce" and ``\#$ 5\times 5$ reduce". Also ``\#$ 3\times 3$ reduce pool" refers to number of $ 1 \times 1$ filters used before $ 3 \times 3$ convolution and max pooling. In addition to that in Table~\ref{tab:network} $mIncept(..) $ refers to our proposed inception module. It's notable that apart from the inception module, proposed method primary network also have slight difference from original GoogLeNet. The proposed network is 21 layers deep including only the layers with parameters, excluding pooling layers and spatial transformer layers (have parameters). Since spatial transformer layer (ST) is a features transformer layer with its own network parameters, it doesn't impact the features learning process of rest of the network. If pooling layers are included, then our $ mIncept(..)$ module would have a depth of 3. Including pooling and ST the network is 39 layers deep. We have used four spatial transformer layers (networks), two of them before two convolutional layers and other two before modified inception modules. Also the network configurations of spatial transformer layers ST1, ST2 and ST3 are different and detailed information is given in Table~\ref{tab:stn}.  




\begin{table}[h]
\caption{Spatial Transformer layer(Networks) configurations}
\label{tab:stn}
  \begin{center}
  \begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Net & conv/stride & max-pool & conv/stride & max-pool & fc & fc \\
\hline
ST1 & 128,5x5/2 & yes & 192, 5x5/2 & yes & 192 & 192 \\
\hline
ST2 & 128, 5x5/2 & yes & 192, 5x5/2 & no & 192 & 192 \\
\hline
ST3 & 128, 3x3/2 & no & 192, 3x3/1 & yes & 192 & 192 \\
\hline
 \end{tabular}
 \end{center}
\end{table}


\begin{table*}[t]
\caption{Comparions of groups accuracy (Top-1) with state-of-the-arts}
\label{tab:group}
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 Algorithm & Speed Limits & Prohibitions & Derestrictions & Mandatory & Danger & Unique\\
 \hline
 \hline
\textbf{Proposed Method} & 99.86 & 100 & 99.95 & 99.72 & 99.89 & 99.87\\
\hline
Committee of CNNs \cite{committe} & 99.47 & 99.93 & 99.72 &99.89& 99.07 & 99.22\\
\hline
Human \cite{gtsrb} & 98.32 & 99.87 & 98.89 & 100.00 & 99.21 & 100.00\\
\hline
Multi-Scale CNN \cite{multi} & 98.61 & 99.87 & 94.44 & 97.18 & 98.3 & 98.63\\
\hline
pLSA \cite{plsa} \& 98.82 & 98.27 & 97.93 & 96.86 & 96.95 & 100.00\\
\hline
Random Forest \cite{rf} & 95.95 & 99.13 & 87.50 & 99.27 & 92.08 & 98.73\\
\hline
 \end{tabular}
\end{table*}


\subsection{Comparions with State-of-the-Art}
This method has several advantages over existing state of the art methods in terms of performance, scalability and memory requirement. Recent high performed method Committee of CNNs\cite{committe} have used 25 networks with 3 convolutional layers and 2 fully connected layers along with manual data augmentation. On original dataset they have modified each image using translation, rotation etc., to get five modified version of that image. Committee of CNNs end up with total around ~90 Million parameters whereas, in our method, we have around 10.5 Million parameters. \\
 Overall accuracy comparisons with different high performing approaches are shown in Table~\ref{tab:accuracy}. We have also reported the accuracy of our deep networks with the Google inception module, which is slightly lower than the accuracy obtained using our modified inception module.\\
GTSRB dataset composed of mainly 6 high-level groups. Classification of images belong to different groups is easier than that of same group images. We have reported the accuracy obtained for each group in Table~\ref{tab:group}. Also, comparisons with other state-of-the-arts methods are presented in Table~\ref{tab:group}.  




\begin{table}[h]
\caption{Comparions of Accuracy (Top-1) with state-of-the-arts}
\label{tab:accuracy}
\begin{center}
\begin{tabular}{|c|c|}
\hline
Algorithm & Accuracy(\%)\\
\hline
\textbf{Proposed Method} & 99.81 \\
\hline
\textbf{Proposed Method with Google Inception} & 99.57\\
\hline
Committe of CNNs \cite{committe} & 99.46\\
\hline
Human Performance \cite{gtsrb} & 98.84\\
\hline
Multi-Scale CNN \cite{multi} & 98.31\\
\hline
pLSA \cite{plsa} & 98.14\\
\hline
Random Forest \cite{rf} & 96.14\\
\hline
\end{tabular}
\end{center}
\end{table}


\section{Conclusion}
 This paper proposes a deep convolutional network with a fewer number of parameters and memory requirements in comparisons to existing methods. The presented network doesn't need data jittering and handcrafted data augmentations. Our main contribution includes the development of modified inception module and a deep network using spatial transformer layer for traffic sign classification.


\section*{Acknowledgment}
I would like to show deep acknowledgement to my professors at IIT Guwahati and IIIT Bangalore for valuable suggestion during this work. Also NVIDIA corporation for donating GPUs for this work.




% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{gtsrb} J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The German Traffic Sign Recognition Benchmark: A multi-class classification competition. In Proceedings of the IEEE International Joint Conference on Neural Networks, pages 1453â€“1460. 2011. 

\bibitem{stn} Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K. "Spatial Transformer Networks". arXiv preprint arXiv:1506.02025 (2015)

\bibitem{plsa} Haloi, M. A novel pLSA based Traffic Signs Classification System. arXiv preprint arXiv:1503.06643 (2015).

\bibitem{haloi3} Haloi, M., Jayagopi, D. B. Characterizing driving behavior using automatic visual analysis. In Proceedings of the 6th IBM Collaborative Academia Research Exchange Conference (I-CARE) on I-CARE 2014 (pp. 1-4). ACM. 2014.

\bibitem{trivedisurvey} McCall, J. C.,  Trivedi, M. M. Video-based lane estimation and tracking for driver assistance: survey, system, and evaluation. IEEE transactions on intelligent transportation systems, 7(1), 20-37. (2006).

\bibitem{torch} http://torch.ch/

\bibitem{stnp} https://github.com/qassemoquab/stnbhwd

\bibitem{haloi1}Haloi, M.,  Jayagopi, D. B. A robust lane detection and departure warning system. In 2015 IEEE Intelligent Vehicles Symposium (IV) (pp. 126-131). (2015).

\bibitem{haloi2} Haloi, M.,  Jayagopi, D. B. Vehicle Local Position Estimation System. arXiv preprint arXiv:1503.06648. (2015).

\bibitem{activity}Braunagel, C., Kasneci, E., Stolzmann, W.,  Rosenstiel, W. Driver-activity recognition in the context of conditionally autonomous driving. In 2015 IEEE 18th International Conference on Intelligent Transportation Systems (pp. 1652-1657). IEEE. (2015).

\bibitem{googlenet}Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... Rabinovich, A. Going deeper with convolutions. arXiv preprint arXiv:1409.4842 (2014)

\bibitem{batch} Ioffe, S., Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015)

\bibitem{prelu} He, K., Zhang, X., Ren, S., Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. arXiv preprint arXiv:1502.01852 (2015)

\bibitem{multi} Sermanet, P., LeCun, Y. Traffic sign recognition with multi-scale convolutional networks. In Neural Networks (IJCNN), The 2011 International Joint Conference on (pp. 2809-2813). IEEE (2011)

\bibitem{committe} Sermanet, P., LeCun, Y. Traffic sign recognition with multi-scale convolutional networks. In Neural Networks (IJCNN), The 2011 International Joint Conference on (pp. 2809-2813). IEEE (2011)

\bibitem{rf} Zaklouta, F., Stanciulescu, B., Hamdoun, O. Traffic sign classification using kd trees and random forests. In Neural Networks (IJCNN), The 2011 International Joint Conference on (pp. 2151-2155). IEEE (2011)

\bibitem{bah}Bahlmann, C., Zhu, Y., Ramesh, V., Pellkofer, M., Koehler, T. A system for traffic sign detection, tracking, and recognition using color, shape, and motion information. In Intelligent Vehicles Symposium, 2005. Proceedings. IEEE (pp. 255-260). IEEE (2005)

\bibitem{le}Le, T. T., Tran, S. T., Mita, S., Nguyen, T. D. Real time traffic sign detection using color and shape-based features. In Intelligent Information and Database Systems (pp. 268-278). Springer Berlin Heidelberg (2010)

\end{thebibliography}


% that's all folks
\end{document}


