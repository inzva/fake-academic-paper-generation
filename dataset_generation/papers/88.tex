\documentclass[a4paper]{article}
\pdfoutput=1
\usepackage{hyperref}
\hypersetup{
  pdfinfo={
    Title={Show, Attend and Interact: Perceivable Human-Robot Social Interaction through Neural Attention Q-Network},
    Author={Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa and Hiroshi Ishiguro}
  }
}
\usepackage{pdfpages}

\begin{document}
\includepdf[pages=1-last]{ICRA_2017_Cam.pdf}
\end{document}


% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\title{\LARGE \bf
Show, Attend and Interact: Perceivable Human-Robot Social Interaction through Neural Attention Q-Network
}


\author{Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa and Hiroshi Ishiguro% <-this % stops a space
\thanks{A. H. Qureshi, Y. Nakamura, Y. Yoshikawa and H. Ishiguro are with Department of System Innovation, Graduate School of Engineering Science, Osaka University, 1-3 Machikaneyama, Toyonaka, Osaka, Japan. {\tt\small \{qureshi.ahmed, nakamura, yoshikawa, ishiguro\}@irl.sys.es.osaka-u.ac.jp}}%
\thanks{A. H. Qureshi, Y. Yoshikawa and H. Ishiguro are also with JST ERATO ISHIGURO Symbiotic Human-Robot Interaction Project.}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
For a safe, natural and effective human-robot social interaction, it is essential to develop a system that allows a robot to demonstrate the perceivable responsive behaviors to complex human behaviors. We introduce the Multimodal Deep Attention Recurrent Q-Network using which the robot exhibits human-like social interaction skills after 14 days of interacting with people in an uncontrolled real world. Each and every day during the 14 days, the system gathered robot interaction experiences with people through a hit-and-trial method and then trained the MDARQN on these experiences using end-to-end reinforcement learning approach. The results of interaction based learning indicate that the robot has learned to respond to complex human behaviors in a perceivable and socially acceptable manner.

%This paper aims to develop a robot that gathers data during its interaction with a human, and learns human interaction behavior from the high dimensional sensory information using end-to-end reinforcement learning. This paper demonstrates that the robot was able to learn basic interaction skills successfully, after 14 days of interacting with people.     
\end{abstract}
\section{Introduction}
Human-robot social interaction (HRSI) is an emerging field with an aim of bringing robots into our social world as our companions. For robots to coexist with humans, it is crucial for them to predict human intentions in order to respond to each and every one of the countless and complex human behaviors with utmost propriety \cite{breazeal2004designing}. 
%Building a socially interactive robot is challenging as it is difficult to pre-program all the responsive robot behaviors for each and everyone of the countless human behaviors.
%Human behaviors are driven by their intentions and community social norms, therefore, human social interaction depends on the understanding of community social standards and on the intention inference of their interacting partners through various factors such as their walking trajectory, face expression, gaze direction, body movements and any activity in progress. Hence, for robots to coexist with humans, it is crucial for them to understand human social norms and recognize human intentions from the aforementioned factors in order to respond appropriately to complex and countless human behaviors. Since, programming a robot to perform aforestated tasks simultaneously in not easy, therefore, it is essential to augment robots with a system which allows them to learn social interaction skills from high dimensional interaction experiences automatically. 

Human intention prediction is a challenging task \cite{wang2013probabilistic} as it depends on many intention depicting factors such as human walking trajectory, face expression, gaze direction, body movement or any ongoing activity. Therefore, programming a robot which can interpret and respond to complex human behaviors based on their intentions is notoriously hard. To solve this challenge we believe that it is essential to augment robots with a self-learning architecture \cite{breazeal2004social} which enables them to learn social interaction skills from high-dimensional interaction experiences automatically. 

%Deep learning architectures are known for their ability to learn complex models directly from the high dimensional sensory information and  have achieved various breakthrough in image and speech recognition tasks \cite{krizhevsky2012imagenet} \cite{hinton2012deep}. 
Recent advancements in machine learning has combined deep learning with reinforcement learning and has led to the development of Deep Q-Network (DQN) \cite{mnih2015human}. DQN utilizes deep convolutional neural network \cite{lecun1998gradient} for the approximation of Q-learning's action-value function. DQN has demonstrated its ability to play arcade video games at human and superhuman level by learning, through hit and trial method, from high dimensional visual data. However, the applicability of DQN to real world human-robot interaction problem was not explored until we, recently, proposed the multimodal deep Q network (MDQN) \cite{qureshi2016robot} for HRSI.   

MDQN uses dual stream convolutional neural networks for action-value function approximation. 
The dual stream structure processes the grayscale and depth images independently, and the Q-values from both streams are fused together for choosing the best possible action in the given scenario. By using MDQN the robot learned to greet people after 14 days of hit and trial method based interaction with people at different public places such as a cafeteria, common rooms, department entrance, etc (as shown in figure 1). The robot could perform only one of the four actions for an interaction and the action were waiting, looking towards human, waving hand and handshaking. Results showed that the robot augmented with MDQN learned to choose appropriate actions in the diverse real world scenarios. However, in \cite{qureshi2016robot}, the robot actions lacked perceivability as the robot could not indicate its attention. The research in \cite{bruce2002role} highlights that humans show more willingness to interact with a robot that can indicate its attention than with a robot that cannot.   
%The human express their mental states through attention and other bodily actions which make their interaction with other humans perceivable \cite{tuuri2009gestural}. Similarly, the indication of robot attention during HRSI is also crucial. 
Therefore, in this paper we propose a Multimodal Deep Attention Recurrent Q-Network (MDARQN) which adds perceivability to robot actions through a recurrent attention model (RAM) \cite{sorokin2015deep}.   
\begin{figure}
\centering     %%% not \center
\includegraphics[height=35mm, width=73mm]{f5.jpg}
\label{f1}
\caption{Robot learning social interaction skills from people.}
\end{figure}

RAM enables the Q-network to focus on certain parts of the input image instead of processing it entirely  at a fine scale. This region selection reduces the number of training parameters as well as the computational operations. Beside computational benefits, RAM provides information about where MDARQN is looking at, while taking any decision. In the proposed work, we utilize this visual attention information by RAM for realizing perceivable HRSI. 
%In our proposed work, the robot operates in a real uncontrolled cluttered environment and can be approached by a group of people for interaction but a robot can perform a physical interaction with just one person at a particular moment. Therefore it is important for the robot to drive its attention on a person out of many people with whom it actually intends to interact. 
%and to focus on the features which are important for taking a decision while ignoring the unnecessary clutter present in the environment. 
%One of the limitations of the deep networks, albeit various breakthroughs on the recognition tasks, is the requirement of the high computational resources as they process the whole high dimensional input image at a fine scale. One of the solutions to this problem is the visual attention models (VAM), VAM enables the network to focus on selective regions of the image thus reduces the number of training parameters as well as the computational operations.
%The rest of the paper is organized as follows. Section II provides discussion on the related work, section III provides a brief background of reinforcement learning and the DQN architecture, section IV explains the proposed multimodal deep Q-learner architecture while section V describes the experimental setup. Section VI and VII give the results of the experiment and the discussion on what the robot has learned through interaction with humans, respectively. Finally, section VIII concludes the paper and also suggests some future areas of research in this particular domain.    
\section{Related Work} 
The challenge of modeling responsive robot behaviors for a wide diversity of complex human behaviors has gained interest of many researchers. Recently, work by Lee et al. \cite{lee2010mimetic}, Amor et al.  \cite{ben2013learning} \cite{ben2014interaction} and Wang et al. \cite{wang2013probabilistic} addresses the said challenge. The proposed work in \cite{lee2010mimetic} \cite{ben2013learning} \cite{ben2014interaction} uses a motion capture system for recording the interaction between two persons and the responsive robot behavior is learned from the recorded data by imitating the behavior of human interaction partners. We believe that the motion capture system does not yield natural interaction behaviors as the participants are required to wear special skin-tight dress together with the track-able makers. In \cite{wang2013probabilistic}, the authors proposed a probabilistic graphical model using which the human intentions are inferred from the observed body movements. However, as mentioned earlier, intention prediction relies on various intention depicting factors, thus, inferring intention from body movements alone is not sufficient. Furthermore, aforestated prior art considers only one human interaction partner for the robot at any time but in proposed research the robot operates in natural uncontrolled environment where it can be approached by any number of people. The quest of an efficient intention predictor has also led to the deep learning based method \cite{vondrick2015anticipating}. In \cite{vondrick2015anticipating}, the authors used video data for training an intention predictor. However, in our work, the interactive behavior perception is crucial because the robot is an active agent in the environment and hence, it can alter the human intention by taking any action. Therefore, the robot needs to interpret the human behavior and its own existence under the human social norms before making any decision. Recently, we proposed MDQN \cite{qureshi2016robot} for interactive behavior perception. The robot augmented with MDQN does not perform perceivable interaction with people because of no attention mechanism. In our proposed work we utilize recurrent attention models (RAM) for perceivable robot actions. So far, recurrent attention models (RAM) have been applied successfully  to various tasks such as object tracking \cite{mnih2014recurrent}, image classification \cite{mnih2014recurrent}, machine translation \cite{bahdanau2014neural}, and image captioning \cite{xu2015show} . The research in \cite{sorokin2015deep}, integrate RAM into DQN and surpasses the previous performance of DQN on some of the Atari games. RAM provides insight into the behavior of the Q-network and in our proposed work, we utilize this insight for driving the robot attention onto the regions of an input scene where the Q-network is focusing on while making any decision. To the best of our knowledge, the applicability of RAM for the perceivable HRSI has not been explored yet. 

\section{Background}
In this work, the human-robot interaction problem is formalized as standard reinforcement Q-learning task in which the agent interacts with an environment $E$ through an action $a \in \mathcal{A}$ and gets a scalar reward $r$, where $\mathcal{A}=\{1,\cdots,K\}$ is the set of all legal actions.

The Q-learning agent learns an \textit{action-value} function which  maps an input state $s$ to an action $a$ under a policy $\pi$ i.e., $Q^\pi(s,a)=\mathbb{E}[R_\mathrm{t}|s_\mathrm{t}=s,a_\mathrm{t}=a,\pi]$. The objective of a Q-agent is to maximize the expected total return $R_\mathrm{t}=\sum_\mathrm{t'=t}^T{\gamma^\mathrm{t'-t}r^\mathrm{t'}} $, where $\gamma:[0,1]$ is a discount factor, $r$ is an immediate reward and $T$ is the terminal step. The maximum achievable expected total return under policy $\pi$ is determined by an optimal \textit{action-value} function $Q^*(s,a)=\mathrm{max} Q^\pi(s,a)$, and this function obeys a fundamental Bellman relation $Q^*(s,a)=\mathbb{E}[r+\gamma\mathrm{max}_{a_\mathrm{t+1}}Q^*(s_\mathrm{t+1},a_\mathrm{t+1})|s_\mathrm{t},a_\mathrm{t}]$. The Bellman relation can be interpreted as: for a sequence $s_\mathrm{t+1}$ at next time-step if the action-value function $Q^*(s_\mathrm{t+1},a_\mathrm{t+1})$ is deterministic for all possible actions $\mathcal{A}$ then the optimal policy is to choose an action $a_\mathrm{t+1}$ which maximizes the expected value of $r+\gamma Q^*(s_\mathrm{t+1},a_\mathrm{t+1})$.  

In practical Q-learning, the action-value function is approximated by a function estimator such as neural networks i.e., $Q(s,a)\approx Q(s,a,\theta)$ and the parameters of an estimator are adjusted iteratively towards the Bellman target. 
%\begin{equation}
%B_\mathrm{t}=r+\gamma Q^*(s_\mathrm{t+1},a_\mathrm{t+1};\theta^-)
%\end{equation}
Recently, a Deep Q-Network (DQN) is introduced which uses a deep convolutional neural networks (convnets) as a function approximator and the parameters of convnets are trained by minimizing the following loss function:    
{\small{\small{\small{
\begin{equation}
L_\mathrm{t}(\theta)=\mathbb{E} \Bigg[\bigg(r+\gamma\mathrm{max}_{a_\mathrm{t+1}} Q(s_\mathrm{t+1},a_\mathrm{t+1};\theta^-) - Q(s,a;\theta)\bigg)^2 \Bigg]
\end{equation}
}}}}
The DQN network uses two Q-networks for minimizing the loss function (Eq 1) i.e., the Bellman target $B_\mathrm{t}=r+\gamma Q^*(s_\mathrm{t+1},a_\mathrm{t+1};\theta^-)$ is computed by a target Q-network with old parameters $\theta^-$ while the training Q-network maintains the recently updated parameters $\theta$. The old parameters $\theta^-$ are updated to current parameters after every C steps. The gradient of loss function (Eq 1) with respect to parameters $\theta$ takes the following form:
{\small{\small{
\begin{equation}
\bigtriangledown L_\mathrm{t}(\theta)=\mathbb{E} \Bigg[\big( B_\mathrm{t} - Q(s,a;\theta)\big) \bigtriangledown_{\theta} Q(s,a;\theta)\Bigg] 
\end{equation}
}}}
In addition to maintaining two Q-networks, DQN also uses experience replay \cite{lin1993reinforcement} for training Q-networks. Finally, DQN follows an $\epsilon$-greedy strategy for interacting with Atari emulator i.e., with probability $1-\epsilon$ the agent takes greedy action by exploiting the Q-network while with probability $\epsilon$ the agent randomly picked an action $a \in \mathcal{A}$ for exploration.

%The previously proposed MDQN \cite{Qureshi2016} extended DQN by using two models of deep Q-network, one for processing the depth frames while other for grayscale frames. Both networks were trained independently by minimizing the loss function (Equation 1). The Q-values from both networks were fused (see section 4 for the definition of fusion function) together for taking the greedy action.
\begin{figure}
\centering     %%% not \center
\includegraphics[height=55mm, width=68mm]{MDARQN4.png}
\label{f1}
\caption{Multimodal Deep Attention Recurrent Q-Network}
\end{figure}
\section{The proposed MDARQN}
In this section, we describe our proposed neural model i.e., MDARQN using which the robot learns to do perceivable HRSI. The MDARQN architecture comprises of two streams of identically structured neural Q-networks, one for processing the grayscale frames while other for processing the depth frames. Each of these neural Q-network streams is trained independently of each other. Since two Q-network streams are identical, and trained independently, therefore, for simplicity; we only discuss the structure of a single stream of the dual stream Q-network. Each stream consists of three neural models: 1) Convnets; 2) Long Short-term Memory (LSTM) network; and 3) Attention network (G). The rest of the section explains these three neural models and the flow of information between them (as also shown in figure 2). 
\subsubsection{Convnets}
The convnets take pre-processed visual frame as an input at each time-step and transform it into $L$ feature vectors, each of which provides D-dimensional representation of a part of an input image i.e, $a_\mathrm{t}=\{\textbf{a}^1_\mathrm{t},\cdots,\textbf{a}^L_\mathrm{t}\}, \textbf{a}^l_\mathrm{t} \in \mathbb{R}^D $. This feature vector is taken as an input by the attention network for generating the annotation vector $z \in \mathbb{R}^D$. 
\subsubsection{LSTM}
We employ the following implementation of LSTM network:
%\begin{figure}
%\centering     %%% not \center
%\includegraphics[height=40mm, width=60mm]{lstm4.png}
%\label{f1}
%\caption{LSTM}
%\end{figure}
\begin{equation}
\begin{pmatrix}
i_\mathrm{t} \\ f_\mathrm{t} \\ o_\mathrm{t} \\ g_\mathrm{t}
\end{pmatrix}
=
\begin{pmatrix}
\sigma \\ \sigma \\ \sigma \\ \mathrm{tanh}
\end{pmatrix}
M
\begin{pmatrix}
h_\mathrm{t-1} \\ z_\mathrm{t}
\end{pmatrix} 
\end{equation}
\begin{equation}
c_\mathrm{t}=f_\mathrm{t} \odot c_\mathrm{t-1} + i_\mathrm{t} \odot g_\mathrm{t}
\end{equation}
\begin{equation}
h_\mathrm{t}=o_\mathrm{t} \odot \mathrm{tanh}(c_\mathrm{t})
\end{equation}
where $i_\mathrm{t}$, $f_\mathrm{t}$, $o_\mathrm{t}$, $c_\mathrm{t}$, and $h_\mathrm{t}$ correspond to the input, forget, output, memory and hidden state of the LSTM, respectively. Let $d$ be the dimensionality of all LSTM states and matrix $M:\mathbb{R}^a \rightarrow \mathbb{R}^b $, in equation 3, is an affine transformation of trainable parameters with dimension $a=d+D$ and $b=4d$. As shown in equation 3-5, the LSTM network takes the annotation vector $z_\mathrm{t} \in \mathbb{R}^D$, previous hidden state $h_\mathrm{t-1}$, and the previous memory state $c_\mathrm{t-1}$ as an input in order to produce the next hidden state $h_\mathrm{t}$. This hidden state $h_\mathrm{t}$ is given to the attention network $G$ and to the linear output layer for generating the annotation vector $z_\mathrm{t+1}$ at next time step and for providing the output Q-value for each of the legal actions, respectively.  
\subsubsection{Attention network} 
The attention network generates the dynamic representation, called annotation vector $z_\mathrm{t}$, of the corresponding parts of an input image at time $t$. The attention mechanism $\phi$, a multilayer perceptron, takes a D-dimensional L feature vectors $a_\mathrm{t}$ and a previous hidden state $h_\mathrm{t-1}$ of the LSTM network as an input for computing the positive weights $\beta^l_\mathrm{t}$ for each location $l$. The weights $\beta^l_\mathrm{t}$ are computed as follow:
\begin{equation}
\beta^l_\mathrm{t}=\frac{\mathrm{exp}(\alpha^l_\mathrm{t})}{\sum^L_{k=1}\mathrm{exp}(\alpha^k_\mathrm{t})} \text{; where }\alpha^l_\mathrm{t}=\phi(a^l_\mathrm{t},h_\mathrm{t-1})
\end{equation}
%\begin{equation}
%\beta^l_\mathrm{t}=\frac{\mathrm{exp}(\alpha^l_\mathrm{t})}{\sum^L_{k=1}\mathrm{exp}(\alpha^k_\mathrm{t})}
%\end{equation}
The annotation vector $z_\mathrm{t}$ is computed as $z_\mathrm{t}=\sum^L_{l=1} \beta^l_\mathrm{t} a^l_\mathrm{t}$. 
%using the weights $\beta^l_\mathrm{t}$ and the feature vectors $a^l_\mathrm{t}$ as shown below: 
%\begin{equation}
%z_\mathrm{t}=\sum^L_{l=1} \beta^l_\mathrm{t} a^l_\mathrm{t}
%\end{equation}
This annotation vector is used by LSTM for computing next hidden state. \\
There are two type of attention network \cite{xu2015show} in the literature: the soft and hard attention network. The attention network used in MDARQN is the soft attention network and unlike hard attention network, it fully differentiable and deterministic. 

Since each of the streams of the MDARQN model is fully differentiable, therefore, each network stream is trained by minimizing the general loss function (Equation 1) through a standard back-propagation method. Finally, output from the two streams are fused together for taking a greedy action as shown in the figure 2. For the fusion, the output Q-values from each Q-network stream are first normalized and then these normalized Q-values from each stream are averaged together to generate output Q-values of MDARQN. The greedy action is then taken by picking the action which has a highest Q-value from these fused Q-values. 
%Since the attention network is differentiable and deterministic, the whole MDARQN model is trained by minimizing the general loss function (Equation 1) through standard back-propagation. 
%For hard attention network these weights $\pi_i$ can be interpreted as the probability of selection location $i$ for focusing the networks attention while taking an action for the interaction. The feature vector $a^i_\mathrm{t}=\{a^1_\mathrm{t},\cdots,a^L_\mathrm{t}\}$ provides features extracted by convnets from different image locations $i$. For each location $i$. The attention network takes a L-dimensional feature vector $a_\mathrm{t}$ from the convnets together with  a previous hidden state $h_\mathrm{t-1}$ of the LSTM network for generating the annotation vector $z_\mathrm{t}$. 
\section{Implementation details}
This section outlines the implementation details of the proposed project. The MDARQN code was built on the baseline \cite{mnih2015human} \cite{sorokin2015deep} and is implemented in torch/lua\footnote{http://torch.ch/}. The robot side programming is done in python. The system used for training MDARQN has 3.40GHz$\times$8 Intel Core i7 processor with 32 GB RAM and GeForce GTX 980 GPU. The rest of the section explains various modules of the project.
\subsection{Robotic system}
A Pepper robot\footnote{//www.aldebaran.com/en/cool-robots/pepper/find-out-more-about-pepper} was used for the proposed project. Out of many built-in sensors of the Pepper, we only use a 2-D camera located on robot's forehead and an ASUS Xtion 3-D sensor located behind robot eyes for the grayscale and depth images, respectively. The 2-D camera and the 3-D sensor were operated at 10 fps with $320 \times 240$ resolution. In addition to visual sensors, we also equip Pepper's right hand with FSR touch sensor which detects if the handshake has happened or not and this handshake detection forms the basis for our reward function (as discussed later). For aesthetic reasons we also hide the touch sensor under the woolen gloves as can be seen in figure 1.   
\subsection{Robot actions with attention}
In order to ensure perceivable HRSI, we utilize the annotation vector $z$ given by attention network $G$ for attention steering of the robot. This attention steering is done as follow.

\textit{Attention steering for greedy actions:} 
The images used by the MDARQN has dimensions $198 \times 198$. We divide horizontal and vertical axis of the input image into five sub-regions based on the author's defined thresholds. The horizontal axis is divided into the left most, left, center, right, right most regions while the vertical axis is divided into top most, top, center, bottom and bottom most regions. The indicators $I^\mathrm{x} \in \{-2,-1,0,1,2\}$ and $I^\mathrm{y} \in \{-2,-1,0,1,2\}$ indicate these sub-regions of horizontal and vertical axis, respectively, starting from -2 which corresponds to the left/top most region. The robot attention mechanism uses annotation vector $z_\mathrm{t}$ to extract the pixel location on the input image, we call it attention mark, where the MDARQN pays the maximum attention. 
%It should be noted that the annotation vector is the softmax distribution over the input image and the maximum attention location corresponds to the pixel location where the probability of agent paying attention is high.
The attention mark and author's defined thresholds are then used to determine the indicator values $I=\{I^\mathrm{x},I^\mathrm{y}\}$. The indicator $I$ and robot's actual position indicator $I^\mathrm{a}=\{I^\mathrm{a}_\mathrm{x},I^\mathrm{a}_\mathrm{y}\}$ is then used to compute the next attention location. The value of robot's actual position indicator at the given time-step is computed relative to the actual location at previous time-step and it is calculated as follow:
\begin{equation}
I^\mathrm{a}_\mathrm{t}= 
\begin{dcases}
    0,\text{if } I+I^\mathrm{a}_\mathrm{t-1}>2 \text{ or } I+I^\mathrm{a}_\mathrm{t-1}<-2\\
    I+I^\mathrm{a}_\mathrm{t-1} ,\text{otherwise}
\end{dcases}
\end{equation}
The robot actual position is initialized to its central location i.e., $I^a_0=0$. The motion of the robot in a real world is determined by $\omega=\{\omega_\mathrm{x},\omega_\mathrm{y}\}$ where $\omega_\mathrm{x}$ controls the rotation of the robot body while $\omega_\mathrm{y}$ controls the robot's head projection. The $\omega$ is computed as follow:
\begin{equation}
\{\omega_\mathrm{x},\omega_\mathrm{y}\}=\{s\theta_1, s\theta_2\} \text{ where } s=I^\mathrm{a}_\mathrm{t}-I^\mathrm{a}_\mathrm{t-1} 
\end{equation}   
The value of $\theta_1$ and $\theta_2$ are $\pi/6$ and $\pi/9$, respectively. It should be noted that it is important to bind the robot motion to predefined regions. As in the proposed work we utilized embedded visual sensors in the robot which have limited field of view and are mobile due to the robot motion. Hence, restricted motion allows a safe localization of robot in public environments.

\textit{Attention steering for non-greedy actions:}
Since during non-greedy robot's behavior, the system randomly picks an action from the set of legal actions, therefore, it is necessary to equip the robot with another attention system that facilitates it's interaction with humans. This function instills the awareness into the robot and makes it sensitive to the stimulus coming from the real world. The stimuli used are the sound and the movement detection. In case robot senses any stimulus, it looks for human at the stimulus origin. If there is not any human, the robot returns to its previous orientation but otherwise it tracks the human with its head in order to engage them for an interaction.

After attending, the robot executes a chosen action. The rest of this section describes the implementation details of these four legal actions, i.e., waiting, looking towards humans, waving its hand and hand shaking with a human.

\textit{Wait:} For this action, during a greedy policy, the robot does nothing other than attending to the attention location. However, in case of non-greedy policy, the robot randomly moves its head within allowable range of head pitch and head yaw. 

\textit{Look towards human:} During this action, if there is human, the robot tracks the person with its head. If this action is being performed under a greedy policy then the robot tracks the human within a narrow field in order to avoid any desynchronization with the greedy attention mechanism. 

\textit{Wave hand:} During this, the robot waves its hand and says \textit{Hello}. 

\textit{Handshake:} For performing a handshake, the robot lifts its right hand up to a certain height and then waits for a few seconds. If FSR touch sensor detects the touch then the robot grabs the person's hand otherwise robot brings its hand down to the default position.
%\begin{figure}
%    \centering
%    \captionsetup{justification=centering}
%    \begin{subfigure}[b]{0.23\textwidth}
%       \fbox{ \includegraphics[height=1.3cm,width=3.4cm]{IMG_2609.jpg}}
%        \caption{Successful handshake.}
%    \end{subfigure}
%      %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
%      %(or a blank line to force the subfigure onto a new line)
%    \begin{subfigure}[b]{0.23\textwidth}
%       \fbox{ \includegraphics[height=1.3cm,width=3.4cm]{IMG_2544.jpg}}
%        \caption{Unsuccessful handshake.}
%    \end{subfigure}
%    \caption{Instances of successful and unsuccesful hand-shakes.}\label{unsuccess}
%\end{figure}
\subsection{Reward function} Handshake detection through touch sensor forms the baseline of our reward function. The robot gets the reward of 1 and -0.1 on the successful and unsuccessful handshake, respectively. Furthermore, the reward of value 0 is given on actions other than handshake. The handshake is successful if the human and robot actually shake each others hand while it is unsuccessful when robot attempts to do a handshake but the handshake does not happen. 

\subsection{Model Architecture}
This section provides the architecture details of MDARQN model. The MDARQN consist of two streams: the Y-channel and the Depth-channel stream for processing the grayscale and depth images, respectively. Since, the structure of both streams are identical, therefore, we only discuss one of the streams.

The convolutional neural network consists of four convolution layers each of which is followed by a non-linear rectifier function. The input dimension to the CNN is $1 \times 198 \times 198$. The convolution layer 1, 2, 3 and 4 convolves 16 filters of $9\times9$, 32 filters of $8\times8$, 64 filters of $7\times7$ and 256 filter of $6\times 6$, respectively. The stride of convolution 1, 2, 3 and 4 are 3, 2, 2 and 1 respectively. The CNN outputs 256 feature maps of dimension $7\times7$. The output feature maps from CNN are given to the attention network which takes 49 vectors each of size 256. To be consistent with attention network, the LSTM network also has 256 units. To generate Q-values for the four set of actions, the output of the LSTM is transformed to four units through a linear layer preceded by non-linear rectifier unit.

\subsection{ Training dataset, data augmentation and pre-processing}
We double the training dataset through two data augmentation techniques: 1) Random cropping of the input image of size $320 \times 240$ to the size suitable for the model i.e., $198 \times 198$; 2) Mirroring the input image and then cropping it randomly to the size $198 \times 198$. The total training data collected during 14 days of experiment comprise of 111,504 grayscale and depth frames. After data augmentation, the number of grayscale and depth frames grows to 223,008. To prepare an input for the MDARQN, the eight most recent depth and grayscale frames, of each time step, are stacked together to form an input for the Y-channel and the Depth-channel of the MDARQN, respectively.  
\subsection{Training procedure}
We present a training procedure which comprise of two phases, the \textit{data generation phase} and \textit{learning phase}. 
\subsubsection{Data generation phase}
In this phase the agent interacts with an environment for generating interaction experiences $e$. At time $t$, the environment provides an observation state $s_\mathrm{t}$, the agent after observing a state $s_\mathrm{t}$ takes an action $a_\mathrm{t}$ using $\epsilon$-greedy policy, the environment in return provides the scalar reward $r_\mathrm{t}$ and the next state $s_\mathrm{t+1}$. The interaction experience $e_\mathrm{t}=\{s_\mathrm{t},a_\mathrm{t},r_\mathrm{t},s_\mathrm{t+1}\}$ is then stored into a replay buffer $\mathcal{M}$ for experience replay during the learning phase. This cycle of generating data keeps on repeating until the terminal state $T$ is achieved. The replay buffer stores $N$ most recent interaction experiences.
\subsubsection{Learning phase}
During this phase, the agent feeds on the replay memory $\mathcal{M}$ for training the MDARQN $Q(s,a;\theta)$ by minimizing loss function (Equation 1). Like DQN training, we also maintain two MDARQN i.e., the target network and current network with old parameters $\theta^-$ and new $\theta$ parameters, respectively.

In the propose work, the MDARQN agent was trained for 14 days. Every day, the robot interacted with people for some time period $T$ in order to generate a data (\textit{data-generation phase}). After $T$ time period, the robot went to rest position and the learning phase began.
It should be noted that the proposed training method is different from the DQN training procedure \cite{mnih2015human}. In DQN training, after filling a memory buffer with $n$ experiences, the Q-network is trained on a minibatch after collecting each and every interaction experience $e$. This training of Q-network after every single interaction experience adds a delay between the agent's interaction with an environment at time $t$ and at time $t+1$. In \cite{mnih2015human}, the environment for the DQN is an Atari emulator which is somehow controllable. From the word controllable we mean that during the DQN training, the Atari environment halts and it waits for the DQN-agent to execute its next action. In our proposed work, the environment is real, uncontrollable and it requires the MDARQN agent to interact with people. Therefore any significant delay while robot is in the field for interaction with the people is unacceptable. Hence, we divided the training procedure into two phases. 
%It should be noted that the proposed training method is different from the DQN training procedure. In DQN training, after collecting each and every interaction experience $e$, the DQN trains the Q-network on a mini-batch sampled from replay buffer.
\subsection{Experiment details and hyper-parameters}
We conducted the experiment for 14 days. Every day the data-generation phase was executed for around 4 hours followed by the learning phase. The number of interaction steps the robot could perform during 4 hours data-generation phase depended on the internet speed\footnote{With upstream speed of 37 Mbps and downstream speed of 23 Mbps, the robot could execute 2010 interaction steps i.e., $T=2010$.} as we used the wireless media for a communication between Pepper and the computer system running MDARQN. For each interaction step, the robot provides eight most recent depth and grayscale frames i.e., $m=8$. The replay buffer stored up to 3750 most recent interaction experiences. During learning phase, a mini buffer of size 2000 samples was randomly sampled from the replay buffer $\mathcal{M}$. This mini buffer was then used for mini-batch training of the Q-network using RMSProp algorithm. This mini-batch training was repeated 10 times during the learning phase and the size of a mini-batch was 25 samples. As suggested in \cite{sorokin2015deep}, the initial LSTM hidden and memory state were zeroed for each new mini-batch. The target network parameters were updated every day after training and the learning rate was kept constant at 0.00025. The exploration parameter $\epsilon$ was annealed linearly from 1 to 0.1 over the 28000 interaction steps, however, during 14 days of experiment the robot could perform only 13938 interaction steps due to variations in internet speed at different locations.         
\subsection{Evaluation Procedure}
In order to evaluate the MDARQN decisions and the impact of attention model on the human-robot interaction, we carried out two kinds of evaluations: 
\subsubsection{Evaluating MDARQN decisions on a test dataset} Since for each given scenario there can be more than one feasible action, therefore, to evaluate either agent decision is right or wrong, we use the following evaluation method. The MDARQN decisions on a test dataset, not seen by the MDARQN during training, were evaluated by three volunteers. The test dataset has 4480 grayscale and depth frames. Each volunteer observes the sequence of eight grayscale frames depicting the scenario followed by the MDARQN decision. The volunteer then decides if the decision is right or wrong. If the decision is marked wrong by the majority of volunteer then the volunteers were asked to pick up the most suitable action for the depicted scenario.
\subsubsection{Evaluating the impact of attention mechanism} We placed the robot in public but this time, the robot interacted with people under our trained Q-networks' policy. The performance of the MDARQN was compared with MDQN through a ratio of number successful handshakes over total number of handshake attempts. The results of the evaluations are presented in the results section.  
%This measure indicate how successful the agent is, in engaging human for interaction.
\subsection{Source code and data availability}
In order to facilitate the implementation of the proposed MDARQN, we release the source code of our complete project together with the depth dataset collected during 14 days of experiment\footnote{https://sites.google.com/a/irl.sys.es.osaka-u.ac.jp/member/home/ahmed-qureshi/deephri}. Although the dataset used for training comprised of both grayscle and depth images but due to privacy concerns, only the depth dataset is made publicly available.
\begin{figure*}
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.24\textwidth}
       \fbox{ \includegraphics[height=1.9cm,width=3.9cm]{28.png}}
        \caption{\footnotesize{{\color{blue}W=0.27} L=0.24 H=0.25 S=0.24}}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
       \fbox{ \includegraphics[height=1.9cm,width=3.9cm]{49.png}}
        \caption{\footnotesize{{\color{blue}W=0.26} L=0.24 H=0.25 S=0.25}}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
       \fbox{ \includegraphics[height=1.9cm,width=3.9cm]{6.png}}
       \caption{\footnotesize{{\color{blue}W=0.26} L= 0.25 H=0.25 S=.24}}
    \end{subfigure}
      %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.24\textwidth}
       \fbox{ \includegraphics[height=1.9cm,width=3.9cm]{7.png}}
        \caption{\footnotesize{W=0.23 {\color{blue}L=0.27} H=0.25 S=0.24}}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
       \fbox{ \includegraphics[height=1.9cm,width=3.9cm]{10.png}}
        \caption{\footnotesize{W=0.22 L=0.25 {\color{blue}H=0.28} S=0.26}}
    \end{subfigure}
        \begin{subfigure}[b]{0.24\textwidth}
       \fbox{ \includegraphics[height=1.9cm,width=3.9cm]{23.png}}
        \caption{\footnotesize{W=0.23 {\color{blue}L=0.27} H=0.25 S=0.24}}
    \end{subfigure}
     %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.24\textwidth}
       \fbox{ \includegraphics[height=1.9cm,width=3.9cm]{30.png}}
        \caption{\footnotesize{W=0.22 L=0.23 H=0.26 {\color{blue}S=0.30}}}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
       \fbox{ \includegraphics[height=1.9cm,width=3.9cm]{46.png}}
        \caption{\footnotesize{W=0.23 L=0.24 H=0.25 {\color{blue}S=0.28}}}
    \end{subfigure}
        %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
            %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \caption{Successful cases of agents decision.}\label{success}
\end{figure*}
\begin{table}
\centering
\begin{tabular}{ | l | l | l | l |}
    \hline
    Trained Model & MDARQN(Aug) & MDARQN & MDQN \\ \hline \hline  
    Hand-shake ratio & 0.74 & - & 0.48\\ \hline
    Accuracy (\%) & 95.2 & 91.3 &  95.3   \\ \hline
    True positive rate (\%) & 90.5 & 82.7 & 90.7\\ \hline
    False positive rate (\%) & 3.15 & 5.77 & 3.09 \\ \hline
    %Misclassification rate (\%) & 4.7 & 8.6 & 4.6 \\ \hline 	
\end{tabular}
\caption{Performance measures of trained Q-networks.}
\label{St}
\end{table}	
\section{Results}
This section presents the results of the proposed neural Q-networks. Table 1 compares the performance of three models i.e., MDARQN(Aug), MDARQN and MDQN. The MDARQN(Aug) was trained on an augmented training dataset while MDARQN and MDQN were trained on an unaugmented training dataset. The description of nomenclature used in table 1 is as follow. The handshake ratio, as discussed earlier, measures how often the robot augmented with a certain Q-network can attract people for handshaking in an uncontrolled public environment. Accuracy is a measure of how often the Q-network's predictions were correct. True positive rate corresponds to the percentage of predicting positive targets as positive. False positive rate measures how often the negative examples were classified as positive. The first row of table 1 indicates that the handshake ratios for a robot augmented with MDAQRN(Aug) and MDQN are 0.74 and 0.48, respectively. Furthermore, it can be seen in the last three rows of table 1 that MDARQN(Aug) and MDQN demonstrate similar performance while MDARQN has relatively inferior performance on the test dataset. In addition, we have also noticed that the performance of individual streams of MDQN and MDARQN(Aug) were also similar with a true positive rate of around 70\%. This indicates that in order to provide similar performance to the neural network without attention, the attention driven neural networks require more training data.  

From now onward all results presented correspond to our proposed MDARQN(Aug). Figures 3 and 4 show the successful and unsuccessful cases of MDARQN(Aug) decisions, respectively, in the depicted scenarios. The actions: wait, look towards human, wave hand and handshake are abbreviated as W, L, H, and S respectively in these figures. In figure 3, in each sub-figure, the top two frames (starting from left) show the first and the last frame out of eight most recent frames for any situation while the bottom two images indicates the region of attention on these frames. An action with maximum Q-value is highlighted in blue to indicate the agent's decision for the particular scenario. The discussion on the MDARQN correct decisions is presented in discussion section. In figure 4, the action highlighted in red is the agent's decision while the action highlighted in green is the decision considered right by the evaluators. %Figures 6(a) and 6(b) show similar situation in which the wave hand action is considered the most appropriate one. In figure 6(c) there is no human but the agent pick handshake action which is incorrect. Finally, figure 6(d) shows that the agents picks wrong place for focusing its attention.
%Figure 5 indicates the situations in which the MDARQN(Aug) took the right decisions. In each sub-figure of figure 5, the top two frames (starting from left) show the first and the last frame out of eight most recent frames for any situation while the bottom two images indicates the region of attention on these frames. The action with maximum Q-value is highlighted in blue to indicate the agent's decision for the particular scenario. 
%In figure 5(a), 5(b) and 5(c)  there is no human in the scene, the women is taking a picture and a person is walking away from the robot, respectively, thus the agent decides to wait in these situations. In figure 5(d) a person is walking towards the robot while looking towards it, hence in an attempt to engage the person for interaction the agent picks the look towards human action. In figure 5(e), a group of people are interacting with each other while some of them are looking toward the robot hence the agent's decides to look towards them too. In figure 5(f), there is a bunch of people but unlike a situation in figure 5(e) no one is looking towards the robot, thus in order to gain attention the agent takes the wave hand action. Finally, figures 5(g)-5(h) presents the situations in which agent decides to handshake with people. 
%Figure 6 indicates unsuccessful cases of agent decisions. The action highlighted in red is the agent's decision while the action highlighted in green is the decision considered right by the evaluators. Figures 6(a) and 6(b) show similar situation in which the wave hand action is considered the most appropriate one. In figure 6(c) there is no human but the agent pick handshake action which is incorrect. Finally, figure 6(d) shows that the agents picks wrong place for focusing its attention.
\section{Discussion}
This section provides a brief discussion on the intention prediction ability of MDARQN, impact of attention steering and reward function definition on HRSI. 
%Before going any further, it should be noted that the action look towards human is a softer way of gaining human attention as compared to waving hand and saying \textit{hello}, thus for engaging human in interaction, this softer or harder way of gaining attention can be employed.
\subsection{Intentions depicting factors}
As discussed earlier, human intention prediction is crucial for HRSI and human intentions can be predicted from various intention depicting factors. Results in figure 3 indicate that our proposed model has learned to infer intention from those factors. In figure 3(b), an activity is in progress i.e., a person is taking a picture and the agent decides to wait. This action of MDARQN is also in accordance with human social norms as we humans usually do not intervene when someone is taking a picture. Figure 3(c) and 3(d), highlights the ability of our model to interpret human walking trajectory as in the former a person is walking away and agent waits while in the latter, the person is walking towards the robot and the agent decides to look towards the person. Furthermore, our model has also learned to determine the level of human engagement with the robot during an interaction.  The scenarios in figures 3(e)-3(h) are arranged in increasing order of human involvement with the robot during the interaction. The scene in figure 3(e) indicates least humans involvement because people are at distance and are not looking towards the robot, the agent takes \textit{wave hand} action to gain people's attention. The scene in figure 3(f) shows relatively higher people involvement so agent chooses \textit{look towards human} action which is a softer way of gaining human attention as compared to wave hand. In the scenarios in figures 3(g)-3(h), people are fully engaged so agent decided to handshake. This level of human engagement with a robot is indicated by person's body orientation and distance from the robot. Hence, the results indicate that the MDARQN has learned to predict human intentions from intention indicating factors. %Finally, comparing the agent decision in the situations shown in figures 6(e)-6(f) with figures 6(g)-6(h), it can be said that the agent has understanding of the implications of human head orientations and body language. In figures 6(g)-6(h) the people are looking towards robot and are standing in front of it so agent shakes hand while in figure 6(e) and 6(f), people do not show full engagement with a robot as they are not looking towards the robot so the agent picks look towards human action and wave hand action, respectively.
\subsection{Impact of attention steering on human-robot interaction}
The higher handshake ratio of MDARQN(Aug) as compared to MDQN indicates that people show more willingness to interact with a robot that exhibits it's attention and is responsive to human stimuli compared to a robot that is not. This result is in accordance with the findings of \cite{bruce2002role} and hence, attentioning is important for a successful HRSI. Despite higher handshake ratio of MDARQN(Aug) with respect to MDQN, the handshake ratio for both models is actually low. One of the reasons for this low ratio is robot's repeated attempts to perform a handshake with a person who is fully engaged (as can also be seen in the accompanying video) but we, humans, avoid multiple handshakes. Therefore, in our future plan, we hope to add memory to the model so that the robot can determine with whom it has already interacted. 
%Since the whole project is done within Japan, therefore, this low ratio is because of Japanese culture of avoiding physical contact. 
In addition to willingness, the attention is also important to determine with whom the robot is intending to interact out of many other people in the scene. As in figures 3(g) and 3(h), the attention network highlights the person on right side and the person at the center, respectively in order to perform handshake with them. It should be noted that this precise attentioning can not be possible with the attention steering method for non-greedy actions.
\begin{figure}
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.235\textwidth}
       \fbox{ \includegraphics[height=1.1cm,width=3.9cm]{e1.png}}
        \caption{\footnotesize{W=0.23 L=0.24 {\color{green}H=0.25} {\color{red}S=0.26}}}
    \end{subfigure}
      %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.235\textwidth}
       \fbox{ \includegraphics[height=1.1cm,width=3.9cm]{e2.png}}
        \caption{\footnotesize{{\color{green}W=0.24} L=0.23 H=0.24 {\color{red}S=0.29}}}
    \end{subfigure}
     %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.235\textwidth}
       \fbox{ \includegraphics[height=1.1cm,width=3.9cm]{e3.png}}
        \caption{\footnotesize{Attention error}}
    \end{subfigure}
    \begin{subfigure}[b]{0.235\textwidth}
       \fbox{ \includegraphics[height=1.1cm,width=3.9cm]{e4.png}}
        \caption{\footnotesize{W=0.24 {\color{red}L=0.27} {\color{green}H=0.25} S=0.24}}
    \end{subfigure}
    \caption{Unsuccessful cases of agents decision.}\label{unsuccess}
\end{figure}
\begin{figure}
\centering     %%% not \center
%\includegraphics[height =74mm, width=150mm]{MDQN.eps}
\includegraphics[height =2.3cm, width=3.5cm]{penalty_att.eps}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{MDQN}
\caption{Effect of reward function on the robot's behavior.}
\end{figure} 
\subsection{Reward function and robot's behavior}
Reward function definition determines the robot behavior; the results presented so far were based on the reward function discussed earlier. In this section, we evaluate the effect of different reward functions on the robot's behavior. High penalty on unsuccessful handshake inculcates rude behavior into the robot as robot become reluctant to handshake while low penalty e.g., 0 inculcates amiable behavior as robot repeatedly attempts to do a handshake. In order to test which robot behavior is acceptable, we trained five models and the penalties on unsuccessful handshake for these five models were 0, 0.1, 0.2, 0.5 and 1 while rest of the reward function definitions were kept same as discussed earlier. The performance of these models was evaluated on test dataset following the agent decision evaluation procedure (discussed earlier). The graph in figure 5 shows that model with penalty of 0.1 on unsuccessful handshake generates more socially acceptable decisions as compared to other models.        
\section{Conclusion}
In order to ensure successful HRSI, it is essential for a robot to interpret complex human behavior and respond to these behaviors in a perceivable way. We propose a Multimodal Deep Attention Q-Network (MDARQN) which was trained through a 14 days of hit and trial method based robot interaction with people in real unconstrained public environments. The results of training indicate that our proposed MDARQN enabled the robot to respond to complex human behavior by first interpreting them and then executing a responsive action with attention indication. The results also show i) that the robot has learned to infer intention from intention depicting factors such as human body language, walking trajectory or any ongoing activity; ii) that the attention indication adds perceivability to robot actions and thus people show more willingness for interaction with a robot; iii) the diverse interaction scenarios which were definitely hard to envision and yet the MDARQN learned to choose appropriate decisions in these diverse scenarios. \\
In our future plan, we plan to i) explore the impact of different fusion strategies on multimodal learning; ii) augment the proposed network with differentiable working memories in order to realize long-term HRI.
%In this paper, we present a multi-model deep Q-network (MDQN) using which the robot learns the social interaction skills through hit and trial. We believe that this kind of self-learning paradigm is the need for any social robot as it is very difficult to envision all the possible interaction sequences before hand. The results show that the robot has learned which action to choose at each time-step in the widely diverse scenarios. Furthermore, it is also shown that the MDQN-agent has learned to extract the features like walking trajectories, head orientation, activity in progress etc., automatically.
\bibliographystyle{IEEEtran}
\bibliography{references}
\nocite{*}
\end{document}