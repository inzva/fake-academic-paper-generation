\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
%%%%% NEW MATH DEFINITIONS %%%%%\usepackage{amsmath,amsfonts,bm}% Mark sections of captions for referring to divisions of figures\newcommand{\figleft}{{\em (Left)}}\newcommand{\figcenter}{{\em (Center)}}\newcommand{\figright}{{\em (Right)}}\newcommand{\figtop}{{\em (Top)}}\newcommand{\figbottom}{{\em (Bottom)}}\newcommand{\captiona}{{\em (a)}}\newcommand{\captionb}{{\em (b)}}\newcommand{\captionc}{{\em (c)}}\newcommand{\captiond}{{\em (d)}}% Highlight a newly defined term\newcommand{\newterm}[1]{{\bf #1}}% Figure reference, lower-case.\def\figref#1{figure~\ref{#1}}% Figure reference, capital. For start of sentence\def\Figref#1{Figure~\ref{#1}}\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}% Section reference, lower-case.\def\secref#1{section~\ref{#1}}% Section reference, capital.\def\Secref#1{Section~\ref{#1}}% Reference to two sections.\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}% Reference to three sections.\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}% Reference to an equation, lower-case.\def\eqref#1{equation~\ref{#1}}% Reference to an equation, upper case\def\Eqref#1{Equation~\ref{#1}}% A raw reference to an equation---avoid using if possible\def\plaineqref#1{\ref{#1}}% Reference to a chapter, lower-case.\def\chapref#1{chapter~\ref{#1}}% Reference to an equation, upper case.\def\Chapref#1{Chapter~\ref{#1}}% Reference to a range of chapters\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}% Reference to an algorithm, lower-case.\def\algref#1{algorithm~\ref{#1}}% Reference to an algorithm, upper case.\def\Algref#1{Algorithm~\ref{#1}}\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}% Reference to a part, lower case\def\partref#1{part~\ref{#1}}% Reference to a part, upper case\def\Partref#1{Part~\ref{#1}}\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}\def\ceil#1{\lceil #1 \rceil}\def\floor#1{\lfloor #1 \rfloor}\def\1{\bm{1}}\newcommand{\train}{\mathcal{D}}\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}\def\eps{{\epsilon}}% Random variables\def\reta{{\textnormal{$\eta$}}}\def\ra{{\textnormal{a}}}\def\rb{{\textnormal{b}}}\def\rc{{\textnormal{c}}}\def\rd{{\textnormal{d}}}\def\re{{\textnormal{e}}}\def\rf{{\textnormal{f}}}\def\rg{{\textnormal{g}}}\def\rh{{\textnormal{h}}}\def\ri{{\textnormal{i}}}\def\rj{{\textnormal{j}}}\def\rk{{\textnormal{k}}}\def\rl{{\textnormal{l}}}% rm is already a command, just don't name any random variables m\def\rn{{\textnormal{n}}}\def\ro{{\textnormal{o}}}\def\rp{{\textnormal{p}}}\def\rq{{\textnormal{q}}}\def\rr{{\textnormal{r}}}\def\rs{{\textnormal{s}}}\def\rt{{\textnormal{t}}}\def\ru{{\textnormal{u}}}\def\rv{{\textnormal{v}}}\def\rw{{\textnormal{w}}}\def\rx{{\textnormal{x}}}\def\ry{{\textnormal{y}}}\def\rz{{\textnormal{z}}}% Random vectors\def\rvepsilon{{\mathbf{\epsilon}}}\def\rvtheta{{\mathbf{\theta}}}\def\rva{{\mathbf{a}}}\def\rvb{{\mathbf{b}}}\def\rvc{{\mathbf{c}}}\def\rvd{{\mathbf{d}}}\def\rve{{\mathbf{e}}}\def\rvf{{\mathbf{f}}}\def\rvg{{\mathbf{g}}}\def\rvh{{\mathbf{h}}}\def\rvu{{\mathbf{i}}}\def\rvj{{\mathbf{j}}}\def\rvk{{\mathbf{k}}}\def\rvl{{\mathbf{l}}}\def\rvm{{\mathbf{m}}}\def\rvn{{\mathbf{n}}}\def\rvo{{\mathbf{o}}}\def\rvp{{\mathbf{p}}}\def\rvq{{\mathbf{q}}}\def\rvr{{\mathbf{r}}}\def\rvs{{\mathbf{s}}}\def\rvt{{\mathbf{t}}}\def\rvu{{\mathbf{u}}}\def\rvv{{\mathbf{v}}}\def\rvw{{\mathbf{w}}}\def\rvx{{\mathbf{x}}}\def\rvy{{\mathbf{y}}}\def\rvz{{\mathbf{z}}}% Elements of random vectors\def\erva{{\textnormal{a}}}\def\ervb{{\textnormal{b}}}\def\ervc{{\textnormal{c}}}\def\ervd{{\textnormal{d}}}\def\erve{{\textnormal{e}}}\def\ervf{{\textnormal{f}}}\def\ervg{{\textnormal{g}}}\def\ervh{{\textnormal{h}}}\def\ervi{{\textnormal{i}}}\def\ervj{{\textnormal{j}}}\def\ervk{{\textnormal{k}}}\def\ervl{{\textnormal{l}}}\def\ervm{{\textnormal{m}}}\def\ervn{{\textnormal{n}}}\def\ervo{{\textnormal{o}}}\def\ervp{{\textnormal{p}}}\def\ervq{{\textnormal{q}}}\def\ervr{{\textnormal{r}}}\def\ervs{{\textnormal{s}}}\def\ervt{{\textnormal{t}}}\def\ervu{{\textnormal{u}}}\def\ervv{{\textnormal{v}}}\def\ervw{{\textnormal{w}}}\def\ervx{{\textnormal{x}}}\def\ervy{{\textnormal{y}}}\def\ervz{{\textnormal{z}}}% Random matrices\def\rmA{{\mathbf{A}}}\def\rmB{{\mathbf{B}}}\def\rmC{{\mathbf{C}}}\def\rmD{{\mathbf{D}}}\def\rmE{{\mathbf{E}}}\def\rmF{{\mathbf{F}}}\def\rmG{{\mathbf{G}}}\def\rmH{{\mathbf{H}}}\def\rmI{{\mathbf{I}}}\def\rmJ{{\mathbf{J}}}\def\rmK{{\mathbf{K}}}\def\rmL{{\mathbf{L}}}\def\rmM{{\mathbf{M}}}\def\rmN{{\mathbf{N}}}\def\rmO{{\mathbf{O}}}\def\rmP{{\mathbf{P}}}\def\rmQ{{\mathbf{Q}}}\def\rmR{{\mathbf{R}}}\def\rmS{{\mathbf{S}}}\def\rmT{{\mathbf{T}}}\def\rmU{{\mathbf{U}}}\def\rmV{{\mathbf{V}}}\def\rmW{{\mathbf{W}}}\def\rmX{{\mathbf{X}}}\def\rmY{{\mathbf{Y}}}\def\rmZ{{\mathbf{Z}}}% Elements of random matrices\def\ermA{{\textnormal{A}}}\def\ermB{{\textnormal{B}}}\def\ermC{{\textnormal{C}}}\def\ermD{{\textnormal{D}}}\def\ermE{{\textnormal{E}}}\def\ermF{{\textnormal{F}}}\def\ermG{{\textnormal{G}}}\def\ermH{{\textnormal{H}}}\def\ermI{{\textnormal{I}}}\def\ermJ{{\textnormal{J}}}\def\ermK{{\textnormal{K}}}\def\ermL{{\textnormal{L}}}\def\ermM{{\textnormal{M}}}\def\ermN{{\textnormal{N}}}\def\ermO{{\textnormal{O}}}\def\ermP{{\textnormal{P}}}\def\ermQ{{\textnormal{Q}}}\def\ermR{{\textnormal{R}}}\def\ermS{{\textnormal{S}}}\def\ermT{{\textnormal{T}}}\def\ermU{{\textnormal{U}}}\def\ermV{{\textnormal{V}}}\def\ermW{{\textnormal{W}}}\def\ermX{{\textnormal{X}}}\def\ermY{{\textnormal{Y}}}\def\ermZ{{\textnormal{Z}}}% Vectors\def\vzero{{\bm{0}}}\def\vone{{\bm{1}}}\def\vmu{{\bm{\mu}}}\def\vtheta{{\bm{\theta}}}\def\va{{\bm{a}}}\def\vb{{\bm{b}}}\def\vc{{\bm{c}}}\def\vd{{\bm{d}}}\def\ve{{\bm{e}}}\def\vf{{\bm{f}}}\def\vg{{\bm{g}}}\def\vh{{\bm{h}}}\def\vi{{\bm{i}}}\def\vj{{\bm{j}}}\def\vk{{\bm{k}}}\def\vl{{\bm{l}}}\def\vm{{\bm{m}}}\def\vn{{\bm{n}}}\def\vo{{\bm{o}}}\def\vp{{\bm{p}}}\def\vq{{\bm{q}}}\def\vr{{\bm{r}}}\def\vs{{\bm{s}}}\def\vt{{\bm{t}}}\def\vu{{\bm{u}}}\def\vv{{\bm{v}}}\def\vw{{\bm{w}}}\def\vx{{\bm{x}}}\def\vy{{\bm{y}}}\def\vz{{\bm{z}}}% Elements of vectors\def\evalpha{{\alpha}}\def\evbeta{{\beta}}\def\evepsilon{{\epsilon}}\def\evlambda{{\lambda}}\def\evomega{{\omega}}\def\evmu{{\mu}}\def\evpsi{{\psi}}\def\evsigma{{\sigma}}\def\evtheta{{\theta}}\def\eva{{a}}\def\evb{{b}}\def\evc{{c}}\def\evd{{d}}\def\eve{{e}}\def\evf{{f}}\def\evg{{g}}\def\evh{{h}}\def\evi{{i}}\def\evj{{j}}\def\evk{{k}}\def\evl{{l}}\def\evm{{m}}\def\evn{{n}}\def\evo{{o}}\def\evp{{p}}\def\evq{{q}}\def\evr{{r}}\def\evs{{s}}\def\evt{{t}}\def\evu{{u}}\def\evv{{v}}\def\evw{{w}}\def\evx{{x}}\def\evy{{y}}\def\evz{{z}}% Matrix\def\mA{{\bm{A}}}\def\mB{{\bm{B}}}\def\mC{{\bm{C}}}\def\mD{{\bm{D}}}\def\mE{{\bm{E}}}\def\mF{{\bm{F}}}\def\mG{{\bm{G}}}\def\mH{{\bm{H}}}\def\mI{{\bm{I}}}\def\mJ{{\bm{J}}}\def\mK{{\bm{K}}}\def\mL{{\bm{L}}}\def\mM{{\bm{M}}}\def\mN{{\bm{N}}}\def\mO{{\bm{O}}}\def\mP{{\bm{P}}}\def\mQ{{\bm{Q}}}\def\mR{{\bm{R}}}\def\mS{{\bm{S}}}\def\mT{{\bm{T}}}\def\mU{{\bm{U}}}\def\mV{{\bm{V}}}\def\mW{{\bm{W}}}\def\mX{{\bm{X}}}\def\mY{{\bm{Y}}}\def\mZ{{\bm{Z}}}\def\mBeta{{\bm{\beta}}}\def\mPhi{{\bm{\Phi}}}\def\mLambda{{\bm{\Lambda}}}\def\mSigma{{\bm{\Sigma}}}% Tensor\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}\def\tA{{\tens{A}}}\def\tB{{\tens{B}}}\def\tC{{\tens{C}}}\def\tD{{\tens{D}}}\def\tE{{\tens{E}}}\def\tF{{\tens{F}}}\def\tG{{\tens{G}}}\def\tH{{\tens{H}}}\def\tI{{\tens{I}}}\def\tJ{{\tens{J}}}\def\tK{{\tens{K}}}\def\tL{{\tens{L}}}\def\tM{{\tens{M}}}\def\tN{{\tens{N}}}\def\tO{{\tens{O}}}\def\tP{{\tens{P}}}\def\tQ{{\tens{Q}}}\def\tR{{\tens{R}}}\def\tS{{\tens{S}}}\def\tT{{\tens{T}}}\def\tU{{\tens{U}}}\def\tV{{\tens{V}}}\def\tW{{\tens{W}}}\def\tX{{\tens{X}}}\def\tY{{\tens{Y}}}\def\tZ{{\tens{Z}}}% Graph\def\gA{{\mathcal{A}}}\def\gB{{\mathcal{B}}}\def\gC{{\mathcal{C}}}\def\gD{{\mathcal{D}}}\def\gE{{\mathcal{E}}}\def\gF{{\mathcal{F}}}\def\gG{{\mathcal{G}}}\def\gH{{\mathcal{H}}}\def\gI{{\mathcal{I}}}\def\gJ{{\mathcal{J}}}\def\gK{{\mathcal{K}}}\def\gL{{\mathcal{L}}}\def\gM{{\mathcal{M}}}\def\gN{{\mathcal{N}}}\def\gO{{\mathcal{O}}}\def\gP{{\mathcal{P}}}\def\gQ{{\mathcal{Q}}}\def\gR{{\mathcal{R}}}\def\gS{{\mathcal{S}}}\def\gT{{\mathcal{T}}}\def\gU{{\mathcal{U}}}\def\gV{{\mathcal{V}}}\def\gW{{\mathcal{W}}}\def\gX{{\mathcal{X}}}\def\gY{{\mathcal{Y}}}\def\gZ{{\mathcal{Z}}}% Sets\def\sA{{\mathbb{A}}}\def\sB{{\mathbb{B}}}\def\sC{{\mathbb{C}}}\def\sD{{\mathbb{D}}}% Don't use a set called E, because this would be the same as our symbol% for expectation.\def\sF{{\mathbb{F}}}\def\sG{{\mathbb{G}}}\def\sH{{\mathbb{H}}}\def\sI{{\mathbb{I}}}\def\sJ{{\mathbb{J}}}\def\sK{{\mathbb{K}}}\def\sL{{\mathbb{L}}}\def\sM{{\mathbb{M}}}\def\sN{{\mathbb{N}}}\def\sO{{\mathbb{O}}}\def\sP{{\mathbb{P}}}\def\sQ{{\mathbb{Q}}}\def\sR{{\mathbb{R}}}\def\sS{{\mathbb{S}}}\def\sT{{\mathbb{T}}}\def\sU{{\mathbb{U}}}\def\sV{{\mathbb{V}}}\def\sW{{\mathbb{W}}}\def\sX{{\mathbb{X}}}\def\sY{{\mathbb{Y}}}\def\sZ{{\mathbb{Z}}}% Entries of a matrix\def\emLambda{{\Lambda}}\def\emA{{A}}\def\emB{{B}}\def\emC{{C}}\def\emD{{D}}\def\emE{{E}}\def\emF{{F}}\def\emG{{G}}\def\emH{{H}}\def\emI{{I}}\def\emJ{{J}}\def\emK{{K}}\def\emL{{L}}\def\emM{{M}}\def\emN{{N}}\def\emO{{O}}\def\emP{{P}}\def\emQ{{Q}}\def\emR{{R}}\def\emS{{S}}\def\emT{{T}}\def\emU{{U}}\def\emV{{V}}\def\emW{{W}}\def\emX{{X}}\def\emY{{Y}}\def\emZ{{Z}}\def\emSigma{{\Sigma}}% entries of a tensor% Same font as tensor, without \bm wrapper\newcommand{\etens}[1]{\mathsfit{#1}}\def\etLambda{{\etens{\Lambda}}}\def\etA{{\etens{A}}}\def\etB{{\etens{B}}}\def\etC{{\etens{C}}}\def\etD{{\etens{D}}}\def\etE{{\etens{E}}}\def\etF{{\etens{F}}}\def\etG{{\etens{G}}}\def\etH{{\etens{H}}}\def\etI{{\etens{I}}}\def\etJ{{\etens{J}}}\def\etK{{\etens{K}}}\def\etL{{\etens{L}}}\def\etM{{\etens{M}}}\def\etN{{\etens{N}}}\def\etO{{\etens{O}}}\def\etP{{\etens{P}}}\def\etQ{{\etens{Q}}}\def\etR{{\etens{R}}}\def\etS{{\etens{S}}}\def\etT{{\etens{T}}}\def\etU{{\etens{U}}}\def\etV{{\etens{V}}}\def\etW{{\etens{W}}}\def\etX{{\etens{X}}}\def\etY{{\etens{Y}}}\def\etZ{{\etens{Z}}}% The true underlying data generating distribution\newcommand{\pdata}{p_{\rm{data}}}% The empirical distribution defined by the training set\newcommand{\ptrain}{\hat{p}_{\rm{data}}}\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}% The model distribution\newcommand{\pmodel}{p_{\rm{model}}}\newcommand{\Pmodel}{P_{\rm{model}}}\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}% Stochastic autoencoder distributions\newcommand{\pencode}{p_{\rm{encoder}}}\newcommand{\pdecode}{p_{\rm{decoder}}}\newcommand{\precons}{p_{\rm{reconstruct}}}\newcommand{\laplace}{\mathrm{Laplace}}% Laplace distribution\newcommand{\E}{\mathbb{E}}\newcommand{\Ls}{\mathcal{L}}\newcommand{\R}{\mathbb{R}}\newcommand{\emp}{\tilde{p}}\newcommand{\lr}{\alpha}\newcommand{\reg}{\lambda}\newcommand{\rect}{\mathrm{rectifier}}\newcommand{\softmax}{\mathrm{softmax}}\newcommand{\sigmoid}{\sigma}\newcommand{\softplus}{\zeta}\newcommand{\KL}{D_{\mathrm{KL}}}\newcommand{\Var}{\mathrm{Var}}\newcommand{\standarderror}{\mathrm{SE}}\newcommand{\Cov}{\mathrm{Cov}}% Wolfram Mathworld says $L^2$ is for function spaces and $\ell^2$ is for vectors% But then they seem to use $L^2$ for vectors throughout the site, and so does% wikipedia.\newcommand{\normlzero}{L^0}\newcommand{\normlone}{L^1}\newcommand{\normltwo}{L^2}\newcommand{\normlp}{L^p}\newcommand{\normmax}{L^\infty}\newcommand{\parents}{Pa}% See usage in notation.tex. Chosen to match Daphne's book.\DeclareMathOperator*{\argmax}{arg\,max}\DeclareMathOperator*{\argmin}{arg\,min}\DeclareMathOperator{\sign}{sign}\DeclareMathOperator{\Tr}{Tr}\let\ab\allowbreak

\usepackage{hyperref}
\usepackage{url}


%--------------------self-added---------------------------
\usepackage{comment}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{varwidth}
\usepackage{algpseudocode}
\usepackage{wrapfig}
\usepackage{graphicx}
%\graphicspath{{_fig/}}

%\DeclareMathOperator*{\argmax}{argmax}
%\DeclareMathOperator*{\argmin}{argmin}

%Table Line
\usepackage{tabularx, booktabs}
\newcolumntype{I}{!{\vrule width 3pt}}
\newlength\savedwidth
\newcommand\whline{\noalign{\global\savedwidth\arrayrulewidth
                            \global\arrayrulewidth 1pt}%
                   \hline
                   \noalign{\global\arrayrulewidth\savedwidth}}
\newlength\savewidth
\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
                            \global\arrayrulewidth 1.5pt}%
                   \hline
                   \noalign{\global\arrayrulewidth\savewidth}}
%Table Line
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmicforall}{\textbf{for each}}
\algtext*{EndWhile}
\algtext*{EndIf}
\algtext*{EndFor}
\algtext*{EndProcedure}
%-------------------------------------------------------------

%\title{Interpretable Convolutional Neural Network Redundancy Analysis \\by Visualization Oriented Filter Pruning}
\title{Interpretable Convolutional Filter Pruning}
%with Visualized Filter FuncationalityNeural Network Redundancy Analysis \\by Visualization Oriented Filter Pruning}


\author{
	Zhuwei Qin$^1$, Fuxun Yu$^2$, Chenchen Liu$^3$, Liang Zhao$^4$, Xiang Chen$^5$ \\
	$^{1,2,5}$Department of Electrical Computer Engineering, George Mason University, Fairfax, VA 22030 \\
	$^{4}$Department of Information Science and Technology, George Mason University, Fairfax, VA 22030 \\
	$^{3}$Department of Electrical Computer Engineering, Clarkson University, Potsdam, NY 13699 \\
	\texttt{zqin@gmu.edu$^1$, fyu2@gmu.edu$^2$, chliu@clarkson.edu$^3$, lzhao9@gmu.edu$^4$}\\
	\texttt{xchen26@gmu.edu$^5$}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\maketitle

\vspace{-3mm}
\begin{abstract}
The sophisticated structure of Convolutional Neural Network (CNN) allows for outstanding performance, but at the cost of intensive computation.
	As significant redundancies inevitably present in such a structure, many works have been proposed to prune the convolutional filters for computation cost reduction.
	Although extremely effective, most works are based only on quantitative characteristics of the convolutional filters, and highly overlook the qualitative interpretation of individual filter's specific functionality.
	In this work, we interpreted the functionality and redundancy of the convolutional filters from different perspectives, and proposed a functionality-oriented filter pruning method.
	With extensive experiment results, we proved the convolutional filters' qualitative significance regardless of magnitude, demonstrated significant neural network redundancy due to repetitive filter functions, and analyzed the filter functionality defection under inappropriate retraining process.
	Such an interpretable pruning approach not only offers outstanding computation cost optimization over previous filter pruning methods, but also interprets filter pruning process.

	%as well as the functionality
	% the model tuning's impact the heir functionality.
	%By analysis the filter we also proves that the traditional


	% its provides an interpretable analysis approach to examine the filters' actual
	% Compare to traditional filter pruning methods, the proposed one can  which utilizes the interpretable filter characteristics to facilitate CNN computation efficiency.
	% Experiments on VGG-16 show that, the visualization analysis can effectively identify and cluster over 93\% repetitive filters.
	% With optimal accuracy maintained, the proposed visualization-oriented filter pruning method could outperform state-of-the-art by 32\% computation cost optimization with better robustness and interpretability.
\end{abstract}
\section{Introduction}
The great success of Convolutional Neural Network (CNN) is benefited from its advanced algorithm and architecture, which utilize inter-connected multi-layer network structure to hierarchically abstract the data feature for recognition tasks (\cite{Kriz:2012:NIPS}).
	However, this complex mechanism offers CNN outstanding performance at the price of intensive computation cost.
	Therefore, many CNN optimization works have been proposed to alleviate this computation cost (\cite{han2015learning}, \cite{jaderberg2014speeding}, \textit{etc}).

While those network redundancy leveraging works are mostly based on quantitative evaluation, the qualitative analysis based on network interpretation is highly overlooked.
	%The network interpretation can be referred as explaining CNN to the human understanding in a figurative manner \XX{[]}.% One of the most effective interpretation approaches is CNN visualization, which is widely adopted to illustrate the network components' specific semantic meanings or human-cognitive graphic patterns \cite{Yosinski:2015:ICML:AM}.
	With the neural network interpretation, CNN can be well analyzed regarding its hierarchical structure and individual component's functionality, and therefore no longer a ``black-box''.
	However, most neural network interpretation works is merely considered as a post-analysis approach, and very few works have utilized it to analysis and guide the optimization directly (\cite{Yosinski:2015:ICML:AM}).

Considering the absence of the network interpretation analysis and optimization, in this work we utilized different CNN interpretation techniques to analysis the convolutional filter functionality and optimize the filter pruning method.
	%visualization to analysis and optimize the redundancy in the convolutional filters.%Based on the similarity analysis of the visualized filter activation patterns, we reveal that many filters in a convolutional layer share repetitive function (\textit{i.e.} feature extraction preference).%Such repetition indicates certain network redundancy that may defect the computation efficiency.%Therefore, we further proposed a hierarchical visualization-oriented filter pruning method, which utilizes the interpretable filter characteristics to reduce the filter redundancy and facilitate CNN computation efficiency in different network operation levels (\textit{e.g.} filter, cluster, layer, and class preference).
In this work, we have the following major contributions:

\vspace{-1mm}\begin{itemize}
	\item We utilized CNN visualization techniques and backward-propagation gradients analysis to interpret the convolutional filter functionality, and demonstrate significant network redundancy due to repetitive filter functions rather than insignificant magnitude;
	%based method to interpret the functionality of the convolutional filters and reveal the filter functionality based network redundancy.
	% \item We proposed a novel filter clustering scheme, which groups the filters in a convolutional layer based on their functionality similarity.
	\item We designed a hierarchical filter functionality-oriented pruning method to explore the interpretable neural network optimization;
	\item We observed the filter functionality transission with model tuning, and reveals the inappropriate pruning and retraining methods may significantly defect the filter functionality and cause unnecessary network reconstruction effort.
	% under analyzed the filter functionality defection under inappropriate retraining process.We renetwork redundancy reduction.
	% \item We examined the filter evolution during retraining phase to reveal the mechanism of filter function reconfiguration.
\end{itemize}\vspace{-1mm}

Experiment results with various CNN models and image dataset show that, the proposed filter interpretation approach can effectively analysis the convolutional filters' the functionality and similarity.
	The proposed functionality-oriented pruning methods also achieves outstanding performance compared to traditional filter pruning methods with better training efficiency and interpretability

% the proposed visualization analysis can effectively identify and cluster the functionality repetitive filters.% With optimal accuracy maintained, the proposed visualization-oriented filter pruning method could achieve competitive performance with better robustness and interpretability.
\section{Related work}\label{sec:prelim}\textbf{Convolutional Filter Pruning}
~~It is well known that the major computation cost in a CNN comes from the convolutional layers.
	Therefore, most CNN optimization works fall into convolutional filter pruning or compact CNN training with convolutional filter constraints:
	% less important filters of a well trained CNN or training compact CNN with sparse constrains.% This work focuses on interpreting the filter function of a well trained CNN and reducing redundant filters.\cite{Li:2016:pruning} ranked the convolutional filters with their absolute weight sum as the filter significance indication and pruned the insignificant filters for better computation efficiency, which is well-recognized and $\ell_1$ ranking based prunning;
%\cite{hu2016network} evaluated the filter significance calculating the occupation of zero valued weights for pruning guidance;
	\cite{he2017channel} also applied Lasso-regression in the filter pruning for batched processing.
	%similar work, but instead of removing the filters one by one, they have proposed to use to optimize a mask to prune filters.%
The filter significance is also evaluated with the filter feature maps:
	%is also Instead of pruning filters based on their weight significance, another approach prunes filters based on the quantitative evaluation on the feature maps.\cite{polyak2015channel} pruned the filters with small feature map values;
	\cite{lLuo2017Thinet} evaluate filter's significance by testing its impact on feature map reconstruction errors after being pruned.

Most of these works are based on quantitative analysis on the filter significance.
	However, such an approach is already questioned by some recent works: \cite{huang2018learning} proved that certain redundancy also exists in filters with large ``significance'';
	while \cite{ye2018rethinking} correspondingly demonstrated that filters with small values also significantly affect the neural network performance.
%lacking sufficient interpretation for the convolutional filter functionality.%%While, some works also proved that certain redundancy also exist in significant filters.%	trained a pruning agent that can automatically select which filter to be pruned.%	Their experiment showed that some pruned filter has a large absolute weight sum.%	\cite{ye2018rethinking} utilized the batch normalization parameter as the ``gate'' to select the redundant filters to prune.%
Therefore, rather than merely evaluating the convolutional filters in a quantitative approach, we analysis the filter functionality in a qualitative manner to guide the neural network optimization.

\textbf{Convolutional Filter Visualization}
~~As the convolutional filters are designed to capture certain input features, the semantics of the captured feature can effectively indicate the functionality of each filter.
	However, as the convolutional filter are embodied by matrices, the  functionality is hard to directly interpret, which significantly hinders the qualitative neural network optimization.
	%the semantics of those filters are impossible to be directly explained.%Hence, the poor network interpretability significantly hinders the further optimization on the network structure.

Recently, many CNN visualization techniques have been proposed to qualitatively analysis the CNN in perspectives of network structure and component functionality,
	%Many effective network visualization works have been proposed in the literature,
	such as Activation Maximization (AM) (\cite{Yosinski:2015:ICML:AM}), Network Inversion (\cite{mahendran2015:Network-Inversion}), Deconvolutional Neural Networks (\cite{Zeiler:2014:ECCV:DeconvolNet}), and Network Dissection based Visualization (\cite{Zhou:2016:CVPR:Network-Dissection}).
	Among those methods, AM offers the most efficient and effective interpretation for the convolutional filter functionality, which is defined by each filter's maximum activation pattern.
	%It aim to find an image that maximally activates a given filter from iterative synthesis process.%The final image will present a visualized graphic pattern, which can be considered as the feature function representation and activation preference.
In this work, we utilize the AM as our major visualization tool for convolutional filter analysis.

\section{Convolutional Filter Functionality Interpretation \\ \hspace{5.5mm} and Filter Redundancy Analysis}\label{sec:visual}\begin{figure}
	\begin{minipage}[c]{0.5\linewidth}
		\includegraphics[width=2.5in]{./1_pattern.pdf}
		\caption{The illustration of visualized patterns of the convolutional filters.}
		\label{fig:vis}
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.5\linewidth}
	\vspace{-2mm}
		\includegraphics[width=2.6in]{./2_correlation.pdf}
		\vspace{1mm}
		\caption{Filter similarity matrices.}
		\label{fig:sim}
	\end{minipage}
\end{figure}

Different from previous quantitative filter significance analysis, the convolutional filter functionality is qualitatively interpreted in this paper.
	% through the CNN visualization associated with backward-propagation gradients analysis.
	Based on the functionality interpretation, the neural network redundancy is further analyzed regarding the filter functionality.
	% among the convolutional filters.%based on the filter functionality in an interpretable approach.
	For preliminary demonstration, we adopt a VGG-16 model trained on CIFAR-10 dataset for intuitive result demonstration.

\subsection{Interpretable Convolutional Filter Functionality Analysis}% Different from previous works that mainly quantitatively analyze the convolutional filters, we utilize AM and backward-propagation gradients for qualitative functionality analysis from two perspectives:% in this work, we utilize the
To qualitatively interpret the convolutional filter functionality, both AM visualization and backward-propagation gradients analysis are utilized.
	%to qualitatively interpret the filter's functionality.%
AM visualization interprets a filter's feature extraction preference from the neural network inputs, while the backward-propagation gradients evaluate the filters' contribution to classification outputs.

In AM visualization, the feature extraction preference of a filter $F_i^l$ from layer $L_l$ is represented by a synthesized input image $X$ that causes the highest activation of $F_i^l$ (\textit{i.e.} the convolutional feature map value).
	Mathematically, the synthesis process of such an input can be formulated as:
\begin{equation}
	V(F^l_i)=\argmax_{X} {A^l_i(X),
	\hspace{0.6cm} X \leftarrow X} + \eta \cdot \frac{\partial A^l_i(X)}{ \partial X},
	\label{eq:am}
\end{equation}
where $A^l_i(X)$ is the activation of filter $F_i^l$ from an input image $X$, $\eta$ is the gradient ascent step size.
	With $X$ initialized as an input image of random noises, each pixel of this input is iteratively changed along the $\partial A^l_i(X)$/$\partial X$ increment direction to achieve the highest activation.
	%guided by the gradients of .
	Eventually, $X$ demonstrates a specific visualized pattern $V(F^l_i)$, which contains the filter's most sensitive input features with certain semantics, and represents the filter's functional preference for feature extraction.
	% activation preference for feature extraction.

Fig.~\ref{fig:vis} demonstrates several filters' visualized patterns from different layers.
	By interpreting the content of these patterns, it is clear to see that the filters in lower layers are more sensitive to fundamental colors and shapes.
	While with the layer depth increasing, the patterns tend to show specific objects and eventually recognizable class targets.

In the backward-propagation gradients analysis,
	%to evaluate the filters' contribution to the final neural network output.
	the gradients indicate the impact of each convolutional filter to individual classification targets:
\begin{equation}
	\gamma_{i,y_{j}} = \frac{1}{N}\sum_{n=1}^{N} \left \| \frac{\partial P(y_{j})}{\partial A_{i}^{l}(x_{n})} \right \|,
	\label{eq:contri}
\end{equation}
where $P(y_{j})$ is the output probability of a sample image $x_n$ to the class $y_{j}$, and $A_{i}^{l}(x_{n})$ is the activation of filter $F_i^l$ for each test image $x_n$.
	$\partial P(y_{j}) / \partial A_{i}(x_{n})$ is the backward-propagation gradient of class $y_j$ respective to the filter's activation, which indicates the dedicated contribution of filter $F_i^l$ to class $y_j$.
	With preliminary experiments, we also find that some filters' visualized patterns also constrain obvious class objects, which indicates large impact to certain classes $y_j$.
	Therefore, the connection between filters and dedicated class outputs can be also determined.

Associating these two analysis methods, namely the CNN visualization and backward-propagation gradients analysis, the filters' functionalities can be well interpreted in a qualitative manner.

\subsection{Function Similarity based Filter Redundancy Analysis}
Based on the above analysis, the convolutional filter functionality can be well interpreted regarding the input feature extraction preference and output contribution.
	These qualitative interpretations also guide certain neural network redundancy analysis, where filters with similar functionalities may indicate unnecessary structural repetition.
	%% analysis Therefore, the filters with similar functionalities may have Regarding the interpretable CNN visualization, the similarities in the visualized patterns indicate functional repetition of the convolutional filters, which could introduce considerable redundancy into the neural network.
	Such repetition can be well identified among the filters with similar visualized patterns, as shown in the Fig.~\ref{fig:vis}:
	%, we find many similar patterns inside each layer.
	as denoted by the red blocks, significant functionality similarities present among the convolutional filters in each layer.

To quantitatively determinate possible filter functionality repetition, a similarity degree between filter $F_k^{(c,l)}$ and $F_i^{(c,l)}$ is formulated regarding both AM visualized patterns and the back-propagation gradients:
% \begin{equation}% 	S_{ED}[V(F_{i}^{(c, l)}), V(F_{k}^{(c, l)})]=\|V(F_i^{(c,l)}) - V(F_k^{(c,l)}) \|^2.% 	\label{eq:sd}% \end{equation}\begin{equation}
	S_{D}[V(F_{i}^{(c, l)}), V(F_{k}^{(c, l)})]=\|V(F_i^{(c,l)}) - V(F_k^{(c,l)}) \|^2 \cdot \|\gamma_{i} - \gamma_{k} ) \|^2,
	\label{eq:sd}
\end{equation}
where $\|V(F_i^{(c,l)}) - V(F_k^{(c,l)}) \|^2$ calculates the Euclidean distance between two visualized patterns, which indicate feature extraction preference similarity
	\footnote{Although image similarity analysis tools like SSIM are more specialized, they cannot be applied into low layer filters with extremely small resolutions. Therefore, the Euclidean distance is adopted for its generality.}.
	And $\|\gamma_{i} - \gamma_{k} ) \|^2$ calculates the similarity between the two filter's contribution to specific output classes.
	To equally evaluate these two criterion, both vectores are normalized, and the summation is utilized as the filter similarity degree.
	% both vectors from 0 to 1.% We use the the whole training set to compute the filter contribution to obtain more accurate evaluation results.%could utilize the structural similarity (SSIM) (\cite{wang:2004:SSIM}), which measures the similarity between two visualized patterns.%(Although SSIM is a well-recognized image similarity analysis tool, it can not be applied into lower layer filters with extremely small resolution.)% because the visualized patterns in the lower layers only have 3x3 or 6x6 pixels.% Considering more interpretable filter functionality, the Euclidean distance and the backward-propagation gradient are chosen as our evaluation metrics:

According to the qualitative analysis of Eq.\ref{eq:sd}, a set of matrices filter similarity degrees are constructed as shown in Fig.\ref{fig:sim}.
	The matrix axes represent the filter index in each layer (\textit{e.g.} 512x512 for filter Conv5\_2), and each cross-point indicates the similarity degree $S_{D}$ between two filters.
	The magnitudes of the similarity degrees are denoted by a color range, where the minimum is denoted as red and the maximum is denoted as white.
	From Fig.\ref{fig:sim}, it can be observed that the filter similarity widely present in different layers.
	% with an average similarity degree above 0.7.
	Meanwhile, significant filter similarity exit in the shallowest and deepest layers of the neural network (\textit{e.g.} Conv1\_1 and Conv5\_2).
	Those high similarity degrees indicate considerable functionality repetition among the convolutional filters, which might be leveraged for effective neural network optimization.
	%sign lot of filter functional repetitions exit in each layer.%Leveraging the functional repetitions, we will further introduce our pruning method in the next section.% The axes demonstrates the similarity matrices based on the  for several convolutional layers with heat maps, where the minimum is denoted as red and the maximum is denoted as white.% The similarity of each pair of visualized patterns are demonstrated by the visualized matrix.% From Fig.\ref{fig:sim}, we can observe that the filter similarity widely present in different layers, especially in the two ends of the neural network (\textit{e.g.} Conv1\_1 and Conv5\_2).% The filter similarity indicates a lot of filter functional repetitions exit in each layer.% Leveraging the functional repetitions, we will further introduce our pruning method in the next section.
In later sections, the filter functionality evaluation approach is widely applied for network redundancy analysis and filter pruning guidance.
\section{Hierarchical Filter Functionality-Oriented Pruning}\label{sec:pruning}\begin{figure}[t]
	\centering
	\includegraphics[width=5.5in]{./pruning_visua.pdf}
	\caption*{\hspace{1mm}(a) Proposed functionality-oriented filter pruning. \hspace{10mm} (b) $\ell_1$ ranking based filter pruning.}
	\caption{Case study of the hierarchical filter functionality-oriented  and filter $\ell_1$ ranking based filter pruning on the Conv5\_1 of VGG-16.
		The convolutional filters are shown by their visualized patterns, and aligned according to the visualization based filter clustering.}
	\label{fig:pruning}
\end{figure}

Based on the convolutional filter functionality and redundancy analysis, we further propose a novel hierarchical filter pruning method oriented by the qualitative functionality interpretation.
% to explore interpretable and efficient neural network redundancy reduction.\subsection{Pruning Method Overview}
The proposed functionality-oriented filter pruning method is designed in a hierarchical manner to address the functionality redundancy in different neural network component levels.

The method has the following major stages:
	(1) Filter Clustering -- Based on the aforementioned filter similarity analysis with AM visualization and the backpropagation gradients, the filters with similar functionalities in each layer are clustered into multiple groups.
	The filters in the same cluster are considered to have repetitive functionalities;
	(2) Filter Level Pruning -- Inside each cluster, the filters' relative contribution to the output classes are qualitative evaluated by the backpropagation gradients analysis.
	The contribution index of each filter can be considered as the filter significance regarding its functionality;
	%considering their impact to the classification output;% (3) Cluster Level Pruning -- Given a pruning rate for a layer, the relative pruning rate for each cluster is calculated to guide the filter pruning in each cluster;
	(3) Cluster Level Pruning -- Inside each layer, the relative cluster pruning rate is based on the cluster volume size to guide the filter pruning in each cluster;
	% (4) Layer Level Pruning --  Given a pruning rate for the whole neural network model, the relative pruning rate for each layer is calculated to guide the pruning in each layer and therefore each cluster.
	(4) Layer Level Pruning --  Guided by the layer sensitive to pruning, the relative pruning ratio for each layer is calculated to guide the pruning in each layer and therefore each cluster.
% by its impact to In each layer, filters are pruned from each cluster with different pruning rate in parallel according the filter contribution.% (3) Considering the retraining operation may change the interpretation of each filter and potentially disturb the filter clustering, we retrain the pruned model only once after pruning the whole neural network.

Fig.~\ref{fig:pruning}(a) illustrate an intuitive example for the filter clustering in the layer Conv5\_1, where each row represents one cluster in the layer, and the horizontal axis indicates the filter's contribution index.
	%The filters in the last row will not be pruned since these filters demonstrate unique visualized patterns.
	Given a pruning rate for the model, the relative pruning rate for each layer and cluster can be determined, and the filters with least classification contribution will be pruned first (marked by green crosses).
	% Our method supposed to pruning filters from the bottom of each cluster, marked by the red cross sign.%
Meanwhile, we also demonstrate $\ell_1$ ranking based filter pruning in the Fig.~\ref{fig:pruning}(b).
	Considering $\ell_1$ ranking's quantitative filter significance evaluation, the pruned filters (marked by red crosses) have no interpretable selection patterns and no functionality correlations.
	%seem randomly selected in have no obvious functionality to prune se which the filters with small $\ell_1$-norm will be first pruned.%As shown in the Fig.~\ref{fig:pruning}(b) the $\ell_1$-norm based method demonstrates different filter selection, marked by the blue cross sign.
	Moreover, comparing to $\ell_1$ ranking based filter pruning, the proposed functionality-oriented pruning requires extremely small effort for model retraining, which will be further explained in later sections.

In the next section, the details for each processing stage will be presented.
%filter functionality-oriented pruning scheme in detail.\subsection{Hierarchical Filter Pruning Scheme}\begin{algorithm}[t]
	\caption{Visualization based k-means Filter Clustering}
	\begin{algorithmic}[1]
	\Require CNN, Number of layers L, and Number of filters in each layer $I_{l}$
	\Ensure  L Clusters
				\\ \textbf{for each}{$Layer~L_{l}$} \textbf{do} $List_{l}$=[];
					\\ \hspace{4.5mm} \textbf{for each}{$Filter~F_{i}^{l}$} \textbf{do} Generate the pattern: $V(F_i^l)$=$X_{i}^{l}$; $List_{l}$.append($V(F_i^l)$);
		\\ \textbf{for} c in range (1, $I_{l}/2$) \textbf{do} Grid search all possible cluster numbers: $C_{l}$=kmeans.cluster($List_{l}$, c);
			\\ \hspace{4.5mm} \textbf{for} $C_{l}^{c}$ in $C_{l}$ \textbf{do} Merge the single filter clusters: $C_{locked}^{l}$ = [];
				\\ \hspace{9mm} \textbf{if}{Len($C_{c}^{l}$)== 1} \textbf{then} $C_{locked}^{l}$.append($C_{c}^{l}$); k=k-1.
		\State Select the maximal c as the optimal cluster number: c=Max(c)
		\Return $C_{c}^{l}$
	\end{algorithmic}
	\label{alg:cluster}
\end{algorithm}\textbf{Filter Clustering}
~~$S_D$ derived from Eq.~3 demonstrates the functionality similarity of any two convolutional filter.
	While to evaluate all the filters' similarity in a comprehensive neural network model, we applies \textit{k}-means algorithm in each layer to cluster the filters.
	%the Based on the similarity analysis, we cluster the filters by performing k-means algorithm in each layer.

As described in Alg.~\ref{alg:cluster}, we first utilize AM to obtain the visualized pattern of each convolutional filter.
	Then, we apply the \textit{k}-means algorithm to $S_D$ values of all the convolutional filters in each individual layer.
	To determine proper cluster amount in each layer (i.e. \textit{k}), a grid search is conducted from one to half of the total filter number.
	It is worth noting that there are inevitably certain amount of filters with extremely minimal similarity with others, which are also group together and won't be considered for pruning due to their possible instinct functionalities.
	%The clusters with only one filter are grouped in a locked cluster, which is considered to have a unique function and won't participate in the filter pruning.%Therefore, the filter clustering process groups the filter with the highest functionality similarity as much as possible.

Each of the convolutional filter clusters can be considered as an interpretable functionality node.
	However, they can be also considered as neural network redundancy units that extract repetitive features from the inputs, and possibly defect the neural network computation efficiency.
	% In the next section, taking the filter clusters as our central analysis targets, we further explore the interpretable neural network redundancy reduction through filter pruning methodology.\textbf{Filter Level Pruning}
~~Ideally, the filters with the exactly same functionality can be substituted to each other, and each cluster can reserve only one filter and prune out the rest.
	However, due to the complex mechanism of neural networks, the filters with similar functionalities may have distinct contribution to the classification results.
	Therefore a relative filter contribution evaluation inside the cluster is necessary to determine the pruning significance for each filter.
%Therefore, the in each cluster can be substituted to each and  can be prunedInside each cluster, we assume the filters are sensitive to the similar input patterns and have similar functionality.

To quantitatively identify the filters' contribution variation, the backward-propagation gradients is utilized to rank the filters in each cluster.
	Given a layer $l$ with $I_{l}$ filters, the neural network output could be formulated by any layer $l$'s output feature maps in the following format:
	\begin{equation}
		Z(F, A^{l}) = F^L (... ~\alpha(F^{l+1} * A^{l}+b^{l+1}) ...) + b^L,
		\label{eq:1}
	\end{equation}
	where $F$ is the filter weights and bias in every layer.
	$A^{l}$ is the output feature maps (activation maps) of layer $l$.
	Due to the high complexity of $F$ and multiple layers' combination, an approximate function $Z$ is needed.
	Here we use $Z$'s first-order Taylor expansion for approximating:
	\begin{equation}
		Z(A^l+\Delta) = Z(A^l) + \frac{\partial Z(A^l)}{\partial A^l} \cdot \Delta.
		\label{eq:1}
	\end{equation}
	In our pruning method, when filter $i$ in the $l-1$ layer is pruned, the filter's output feature map is corresponding set to zero, i.e. changing the $i^th$ dimension of $A^l$ to zero.
	Therefore, the influence on $Z$ can be qualitatively evaluated as:
	\begin{equation}
		\begin{aligned}
			\frac{\partial Z(A_i^l)}{\partial A_i^l} \cdot \Delta, ~ where ~ \Delta = A_i^l \rightarrow 0.
			\label{eq:1}
		\end{aligned}
	\end{equation}

	Before pruning, each filter $F^{(c,l)}_i$ in the cluster $C_c^l$ of layer $l$ is firstly ranked by the contribution index, $I(F^{(c,l)}_i)$, which is calculated by examining the backward-propagation gradients.
	Specifically,
	\begin{equation}
		\medmuskip=-2mu
		I(F^{(c,l)}_i) = \frac{1}{N}\sum_{n=1}^{N} \left \| \frac{\partial Z(F, A^{l})}{\partial A_{i}^{l}(x_{n})} \right \|,
		\label{eq:contri}
		\vspace{-0.5mm}
	\end{equation}
	where the $Z(F, A^{l})$ is the neural network output loss of a sample image $n$, and the $A_i^l(x_{n})$ is the feature map of filter $i$ for each test image $n$.

	As shown in Fig.~\ref{fig:pruning}, the filters in each cluster are ranked with contribution index.
	When a specific filter pruning amount is assigned to the cluster, filters with small contribution will be firstly pruned.

\textbf{Cluster Level Pruning}
~~As filter clusters are supposed to bear certain inner redundancy, they can perform filter pruning independently.
	Hence a parallel pruning operation can be well executed in each cluster as shown in Fig.~\ref{fig:pruning}.
	%which could significantly improve the neural network pruning speed.
However, the significant variance of filter distribution also presents in each cluster.
	In each layer, a few clusters may have significantly larger volumes, while there are also a certain amount of filters that can't be well clustered.
	Therefore, a dedicated cluster level scheme is necessary to balance the pruning rates between clusters.
	Considering the cluster volume, we define an adaptive pruning rate of $R_{clt}^{(c,l)}= length(C_c^l)$.
	The $length(C_c^l)$ calculates the cluster volume size, since larger cluster volume demonstrates more filter redundancy.
	And a larger $R_{clt}^{(c,l)}$ value will lead significantly more aggressive cluster level pruning.
	Therefore, we can see different filter pruning amounts between clusters in the Fig.~\ref{fig:pruning}.

\textbf{Layer Level Pruning}% ~~In many state-of-the-art convolutional filter pruning methods, the layer level pruning is iteratively tested with retraining operation layer by layer due to unpredictable neural network accuracy defection.
	As the functionalities of each layer are relatively independent, all the layers can be pruned in a parallel manner with minor accuracy defection.
	%However, with the evenly parallel pruning based on the filter functional repetition, the filter functionality-oriented pruning expected to demonstrate a more minor damage to the filter functionality.
	In our pruning method, we derive a hyper-parameter $Pr$ based on each layers' accuracy sensitive to the pruning.
	Specifically, we prune filters in each layer independently and evaluate the pruned model's accuracy on the test set without retraining.
	$Pr$ is the neural network accuracy drop by pruning certain amount filters in each layer.
	Since each layer has different accuracy sensitivity to pruning, we empirically determine the same accuracy drop $Pr$ for all layers.
	By setting the hyper-parameter $Pr$, we can find a certain amount of filters $r$ to prune for each layer, corresponding amount of filters can be well assigned to each cluster as $r\cdot R_{clt}^{(c,l)}$.
	And in each cluster, the filters to be pruned can be well identified by the contribution index of $I(F_k^{(c,l)})$.

	Leveraging the convolutional filters' qualitative functionality analysis, our proposed method is expected to leverage the neural network interpretability for more accurate redundant filter allocation, faster pruning speed, and optimal computation efficiency.
	%In the next section, we will further explore interpretable redundancy reduction and the impact of the retraining operation with experimental analysis.\begin{comment}
\paragraph{Cluster Level Pruning}
As each cluster is supposed to bear certain inner redundancy, it can perform filter pruning independently.
	Hence a parallel pruning operation can be well executed in each cluster as shown in Fig.~\ref{fig:pruning}, which could significantly improve the network pruning speed.

However, significant variance of filter distribution also present in each cluster.
	In each layer, a few clusters may have significant larger repetition volumes, while there are also certain amount of clusters that can't be well clustered.
	Therefore, dedicated cluster level scheme is necessary to balance the pruning rates between clusters.
	Considering the cluster volume and filter contribution distribution, we define an adaptive pruning rate of $R_{clt}^{(c,l)}$:
\begin{equation}
	R_{clt}^{(c,l)}=\alpha\cdot length(C_c^l) \times \beta\cdot \frac{1}{\frac{1}{C}\sum_{n=1}^{C} I(F^{(c,l)}_i)},
	\label{eq:cluster}
\end{equation}

where $\frac{1}{C}\sum_{n=1}^{C} I(F^{(c,l)}_i)$ calculates the average contribution of filters in cluster $C^l_c$, and $length(C_c^l)$ calculates the cluster volume size.

In Eq.~\ref{eq:cluster}, large average filter contribution indicates more important filter function, while larger cluster volume demonstrates more filter redundancy.
	Taking these two factors into consideration, we set empirical factors (\textit{i.e.} $\alpha$ and $\beta$) for the clusters in each layers.
	And a larger $R_{clt}^{(c,l)}$ value will lead significantly more aggressive cluster level pruning.
	Therefore, we can see distinct pruning rates between clusters in Fig.~\ref{fig:pruning}.
\end{comment}
\section{Experiments}

In this section, we will evaluate the functionality-oriented filter pruning method and testify the interpretable filter functionality that the method derived from.

\subsection{Experiment Setup}

The visualization analysis and filter pruning method are implemented in Caffe environment (\cite{jia2014caffe}).
The evaluation is performed on our designed ConvNet and VGG-16 on CIFAR-10, VGG-16 and ResNet-32 on ImageNet.

In evaluations on CIFAR-10, a data argumentation procedure is processed firstly through horizontal flip and random crop, and a 4 pixel padded training data set is generated.
	The whole training data is utilized in calculating the contribution index in Eq.\ref{eq:contri} for more accurate estimation.
	Our designed ConvNet is designed based on the classic AlexNet model, in which the convolutional filter has the size of 3x3 and the number as the same as the original model
	\footnote{The detailed neural network structure of the ConvNet and the VGG-16 are demonstrated in the Appendix.}.

In evaluations on ImageNet, 10 classes images out of 1000 classes, namely ImageNet-10 is utilized in this work.
	And each class contains 1300 training images and 50 validation images.
	The pre-trained VGG-16 and ResNet-32 is fine-tuned on ImageNet-10 with a batch size of 128 and a learning rate of $1e^{-3}$, achieving 98.6\% and 97.1\% testing accuracy respectively.
As the same, the train data set is fully utilized to calculate the contribution index.

The ResNets-32 for ImageNet have three stages of residual blocks for feature maps with sizes of 56x56, 28x28, and 14x14.
	The three stages contain 3, 4, and 2 residual blocks respectively, and each residual block has three convolutional layers.
	The first convolutional layer of ResNets-32 is numbered as 1 and the layer number is defined to increase from shallower to deeper layers.
	Note that the projection layer located in the junction of two adjacent stages is neither numbered nor pruned as was done in other previous works.
	In each residual block, only the first two convolutional layers are pruned to keep the input and output feature maps to be identical.
	Hence, only filters from layer 2, 3, 5,... ,9 in stage 1, layer 11, 12,... 18 in stage 2, and layer 23, 24, 26, 27 in stage 3 are pruned.

In the experiment, the well trained neural network models are pruned by both proposed methods and $\ell_1$ are compared from the perspectives of layer-wise pruning, model-wise pruning, filter training behavior, as well as the overall performance.
The retraining process is executed on 20 epochs (i.e. 1/8 original training epochs) with a constant learning rate of 0.001.

%The visualization analysis and filter pruning method are implemented in Caffe (\cite{jia2014caffe}).%We evaluate our designed ConvNet and the VGG-16 on the CIFAR-10 and another two CNN models, namely VGG-16 and ResNet-32 on a subset of the ImageNet.%For CIFAR-10, we generate 4 pixel padded training data, which horizontal flip and random crop are performed for data argumentation.%We use the total 50000 train data set to calculate the contribution index to obtain more accurate estimation.%Our ConvNet is designed based on the classic AlexNet model, which all the convolutional filters' size to 3x3 while keeping the number of filters in each layer as the same as the original model.%The detailed neural network structure is demonstrated in the Table~\ref{tab:ConvNet} of the Appendix.%The VGG-16 on the CIFAR-10 is the same as~\cite{Li:2016:pruning}, which is showed in the Table~\ref{tab:VGG} of the Appendix.%For ImageNet, we subtract 10 classes images out of 1000 classes as our evaluation target, which are named ImageNet-10. %Each class contains 1300 training images and 50 validation images.%We first fine-tuning the pre-trained VGG-16 and ResNet-32 on the of ImageNet-10.%We use a batch size of 128 and learning rate $1e^{-3}$, which can achieve 98.6\% and 97.1\% testing accuracy for the VGG-16 and the ResNet-32 respectively.%Same as the CIFAR-10, we use the total 13000 train data set to calculate the contribution index.%Additionally, the ResNets-32 for the ImageNet have three stage of residual blocks for feature maps with sizes of 56x56, 28x28, and 14x14.%The three stages contain 3, 4 and 2 residual blocks respectively, which each residual block has three convolutional layers.%We number the first convolutional layers of ResNets-32 as 1 and from shallower to deeper layers, the layer number increase.%When it comes to the junction of two adjacent stages, we did not number the projection layer since we will not prune these layers.%For each residual block, we only prune the first two convolutional layers to keep the input and out put feature maps identical.%Namely, only filters from layer 2, 3, 5,... ,9 in stage 1, layer 11, 12,... 18 in stage 2, and layer 23, 24, 26, 27 in stage 3 are pruned. %Every neural network pruning starts from a well trained model and we compare our proposed method with filter $\ell_1$ and activation based method from the perspectives of layer-wise pruning, model-wise pruning, filter training behavior, as well as the overall performance.%For retraining, we use a constant learning rate 0.001 and only retrain 20 epochs, which is one-eighth of the original training epochs.\subsection{Layer-wise Pruning Analysis}\begin{figure}[t]
  \centering
  \includegraphics[width=5.5in]{./layer_sen.pdf}
  \vspace{-6mm}
  \caption{Sensitivity of each individual layer to pruning methods.}
  \label{fig:layer}
  \vspace{-2mm}
\end{figure}

We demonstrate the effectiveness of our proposed method by comparing the accuracy defection sensitivity of individual layers to the proposed pruning method and $\ell_1$ (\cite{Li:2016:pruning}) based pruning.

%In this section, we analyze the sensitivity to pruning for each individual layer.%To demonstrate the effectiveness of our proposed method, we compared with the $\ell_1$ (\cite{Li:2016:pruning}) based method.
The first row of Fig.~\ref{fig:layer} shows the layer-wise network accuracy degradation under different pruning ratio according to our method, while the the accuracy degradation caused by the $\ell_1$ based method is demonstrated in the second row.

%Fig.~\ref{fig:layer} shows the layer-wise models accuracy drop under different pruning ratio.%The first row demonstrates our method and the second row demonstrates $\ell_1$ based method.%We can observe that:
The following phenomenon is observed:
(1) The proposed pruning has a slower accuracy degradation rate compared with the $\ell_1$ ranking based pruning method in majority of layers, especially in the scenario of VGG-16 on the CIFAR-10;
%Most of the layers of our proposed method perform a slower accuracy drop rate than $\ell_1$ based method, especially for the VGG-16 on the CIFAR-10, the effective of our method is more obvious.
(2) In the proposed pruning, the accuracy degradation is slight and degradation rate is small before more than 50\% repetitive convolutional filters are pruned;
%Before half of the repetitive convolutional filters are pruned, our method performs an very slow accuracy drop rate.
(3) The proposed method can slow down the accuracy degradation of some representative layers of ResNet-32 with the increasing of pruning rate.
The reason is the ResNet contains much less filters compared to other neural network model.
In addition, the deeper layers of ResNet are more sensitive to pruning than the shallower layers, which is different with the other network models.

%We selectively show some representative layers of ResNet-32. %Our method demonstrates comparable effectiveness than the $\ell_1$ based method.%The reason is that the layers in the ResNet contain much less filters.%And the deeper layers of ResNet are more sensitive to pruning than layers in the earlies stages of the neural network.

As such, our proposed method demonstrates significant pruning stability and robustness in the layer-wise pruning.
It also means the functionality redundant filters can be more accurately identified and pruned through the proposed interpretive approach.

\subsection{Model-wise Pruning Analysis}\begin{figure}[b]
  \centering
  \includegraphics[width=5.5in]{./model_sen.pdf}
  \caption{Model-wise pruning analysis.}
  \label{fig:model}
  \vspace{-2mm}
\end{figure}

In this section, we further analyze the sensitivity to pruning of different neural network models.
The performance of the proposed method is compared with both the $\ell_1$ and the activation based methods.
%Both the $\ell_1$ and the activation based methods are compared with our proposed method.
In the activation based filter pruning method, filters with small output activation are removed~(\cite{polyak2015channel}).

Fig.~\ref{fig:model} shows the model-wise pruning results of different network models under different pruning rate in the three pruning methods that are named as \emph{Cluster}, \emph{L1}, \emph{Activation} respectively.
Here, all convolutional layers in a network model are pruned simultaneously.
The results depict that
(1) The proposed method outperforms the $\ell_1$ and activation based methods on all models, especially when the neural networks pruning rate is between 20\% and 40\%;
(2) The proposed method has comparable performance with the $\ell_1$ based method when the pruning rate is smaller than 10\%.

The above results demonstrate that our proposed method can be applied on full model compression with less accuracy degradation.
The overall performance is discussed in the next section.
%Therefore, our proposed method can be well applied on the whole model compression.%We will discuss the overall performance in the next section. \subsection{Overall Performance Evaluation}\begin{table}[t]
\caption{CIFAR-10 Pruning Configuration}
\centering
\begin{tabular}{lllllll}
\toprule
         &    Pruning    & FLOPs    & FLOPs          & prune    & retrain        \\
network  &    Rate       & (x$10^{8}$)  & reduction      & accuracy & accuracy       \\ \midrule
ConvNet  &           0   &  8.53      &                &          &  90.05\%*        \\
prune A  &        40.2\% &  5.34      &  \textbf{37\%}     & 87.88\%  &  \textbf{90.04\%}\\
A-L1     &        40.2\% &  5.34      &  37\%          & 83.29\%  &  89.42\%         \\
prune B  &        64.5\% &  2.1       &  \textbf{63\%}     & 65.61\%  &  \textbf{88.57\%}\\
B-L1    &        64.5\% &  2.1       &  63\%          & 53.72\%  &  87.88\%         \\ \bottomrule
VGG-16   &           0   &  3.31      &                &          &  90.2\%*         \\
prune A &        42.9\% &  1.85        &  \textbf{41\%}     &  88.1\%  &  \textbf{90.3\%} \\
A-L1     &        42.9\% &  1.85      &   41\%         &  82.4\%  &  89.82  \%       \\
prune A  &        66.6\% &  1.03        &  \textbf{68\%}     &  72.1\%  &  \textbf{89.9\%} \\
B-L1    &        66.6\% &  1.03      &   68\%         &  60.2\%  &  89.73   \%      \\ \bottomrule
\end{tabular}
\label{tab:CIFAR-10}
\caption*{\hspace{-65mm}\footnotesize{*Baseline accuracy}}
\vspace{-6mm}
\end{table}\begin{table}[b]
\caption{ImageNet-10 Pruning Configuration}
\centering
\begin{tabular}{lllllll}
\toprule
         &      Pruning  & FLOPs      & FLOPs          & prune    & retrain        \\
network  &      Rate     & (x$10^{10}$) & reduction      & accuracy & accuracy       \\ \midrule
VGG-16   &           0   &  1.54      &                &          &  98.6\%*         \\
prune A  &         28.3\%&  0.84      &  \textbf{45\%}     & 51.9\%   &  \textbf{96.8\%} \\
A-L1     &         28.3\%&  0.84      &  45\%          & 40.7\%   &  95.7\%          \\
prune B  &         40.9\%&  0.57      &  \textbf{63\%}     & 25.1\%   &  \textbf{94.3\%} \\
B-L1     &         40.9\%&  0.57      &  63\%          & 21.2\%   &  93.2\%          \\ \bottomrule
ResNet-32&           0   &  2.31      &                &          &  97.12\%*        \\
prune A  &         21.5\%&  1.73      &  \textbf{25\%}     &  94.75\% &  \textbf{97.50\%}\\
A-L1     &         21.5\%&  1.73      &   25\%         &  92.87\% &  96.25\%         \\
prune B  &         30.2\%&  1.55      &  \textbf{33\%}     &  91.25\% &  \textbf{96.25\%}\\
B-L1     &         30.2\%&  1.55      &   33\%         &  90.37\% &  94.75   \%      \\ \bottomrule
\end{tabular}
\label{tab:imageNet-10}
\caption*{\hspace{-65mm}\footnotesize{*Baseline accuracy}}
\vspace{-6mm}
\end{table}

In this section, the overall pruning performance of our proposed method and the $\ell_1$ based method is evaluated and compared, which are depicted as \emph{prune} and \emph{L1} respectively in Table~\ref{tab:CIFAR-10} and Table~\ref{tab:imageNet-10}.
In the evaluation, a retraining procedure is performed to further recover the accuracy.
Table~\ref{tab:CIFAR-10} shows the performance in FLOPs reduction, accuracy after pruning, and accuracy after retraining of ConvNet and VGG on the CIFAR-10 dataset, and Table~\ref{tab:imageNet-10} shows the results of  VGG and ResNet-32 on the ImageNet-10 dataset.
In the evaluations,  two pruning configuration is utilized.
In \emph{prune A} and \emph{A-L1}, a small amount of filters are pruned and the original accuracy can be completely recovered by retraining.
\emph{prune B} and \emph{B-L1} indicates the scenarios that filters are pruned aggressively to maximize the FLOPs reduction with optimal accuracy.
The pruning rate is the percentage of the pruned over the total filters in a neural network model, and the comparison is executed on the same pruning rate.

%For equal comparison, we pruning same amount of filters between our proposed method and $\ell_1$ based method.%The pruning configuration is described as follows: in prune A, less filters are pruned, which the original accuracy can be recovered by retraining; in prune B, we prunes more aggressively to maximize the FLOPs reduction while maintain the optimal accuracy.%The pruning ratio means the total number of pruned filters over the total number of filters, which is determined by setting each layer's accuracy drop.%We retrain the models with same configuration and compare the final results in terms of FLOPs reduction, prune accuracy and retrain accuracy.   %Table~\ref{tab:CIFAR-10} shows the final pruned result of the ConvNet and VGG on the CIFAR-10 dataset.

Table~\ref{tab:CIFAR-10} and Table~\ref{tab:imageNet-10} demonstrates that our proposed method introduce less accuracy degradation than the previous $\ell_1$ based method.
In addition, the accuracy caused by the proposed pruning can be easier recovered through retraining with better accuracy.
%(1) Our method demonstrates less accuracy drop than the $\ell_1$ based method.%(2) The retraining accuracy of our methods is higher than the $\ell_1$ based method.
For example, in the evaluations of VGG on the CIFAR-10 dataset, the proposed method can achieve 41\% FLOPs reduction with retraining accuracy even higher than the original value, however, the accuracy loss caused in the $\ell_1$ based pruning can not be well recovered.
As is shown in Table~\ref{tab:imageNet-10}, large accuracy loss occurs in the VGG on the ImageNet-10 dataset in the two pruning methods.
And the proposed method induces less accuracy degradation and higher retraining accuracy compared with the $\ell_1$ based pruning.
Table~\ref{tab:imageNet-10} also shows that it is hard to recover the accuracy drop through retraining in the pruning of the ResNet on the ImageNet-10 dataset, and hence acceptable pruning rate in this scenario is relatively small.

%In the Table~\ref{tab:imageNet-10}, we shows the final pruned result of the VGG and ResNet-32 on the ImageNet-10 dataset.%we can observe that:%(1) For the VGG on the ImageNet-10 dataset, the models demonstrate a very large accuracy drop.%However, compared with the $\ell_1$ based pruning method, our pruned model demonstrates less accuracy drop.%And higher retraining accuracy is achieved by retraining.%(2) For the ResNet on the ImageNet-10 dataset, the final pruning ratio is relative small since it is hard to recovery the accuracy drop by retraining.\subsection{Network Retraining Analysis}
In most state-of-the-art works for neural network compression, the retraining is a necessary operation to compensate the inaccurately pruned filters and enhance the network performance.
In this section, we evaluate the retraining operation quantitatively and qualitatively.

In Fig.~\ref{fig:Retrain}, we first quantitatively explore the retraining process in terms of the model accuracy recovery.
  As shown in Fig.~\ref{fig:Retrain}, the soiled lines represent the pruned model accuracy recovery based on our proposed method whereas the dot lines represent the $\ell_1$ based method.
  We can observed that:
  (1) The models pruned by our method always demonstrate more quickly accuracy recovery.
  (2) To recover to the same accuracy, our method requires less retraining iterations.

Meanwhile, we also qualitatively explore the retraining process by the AM. Here we mainly focus on the functionality transition during network retraining process.
  As shown in Fig.~\ref{fig:Retrain_pattern}, we randomly select one filter that has not been pruned by both our method and $\ell_1$ based method and its original function analysis pattern is showed in the first column.
  Then we use the same visualization methods to visualize the pattern during different retraining iterations, e.g. every $100$ iterations.
  The AM patterns show that, regardless the retraining iterations, the remained filter function of our proposed pruning method remains unchanged.
  However, the $\ell_1$ based method changes the original filter functionality during retraining process. This means that the retraining process of the $\ell_1$ based method rebuilds the filter functionality, which therefore needs more retraining iterations to restore the model accuracy. The reasons behind that are $\ell_1$ based method may partially destruct the original neural network's functionality composition, which then need more iterations to reconstruct and balance the functionality composition\footnote{More selected filters are demonstrated in the Appendix.}.

  That's also the reason why our pruning method causes significantly less accuracy drop and requires less retraining efforts since our pruning method induces less influence to the neural network functionality composition.
  Therefore, with more interpretable and accurate redundant filter functionality identification and pruning in our method, the costly retraining process will become less necessary, as our retraining analysis results demonstrate.

\begin{figure}[t]
  \centering
  \includegraphics[width=5.5in]{./Retrain.pdf}
  \vspace{-6mm}
  \caption*{\hspace{2mm}(a) CIFAR-10\hspace{50mm} (b) ImageNet-10}
  \caption{Pruned model accuracy recovery by retraining.}
  \label{fig:Retrain}
  \vspace{-2mm}
\end{figure}\begin{comment}
\begin{table}[t]
\caption{VGG on Cifar10 Pruning Configuration}
\centering
\begin{tabular}{llllllll}
\toprule
                 &        &         & base   & prune A & A-$\ell_1$ & prune B & B-$\ell_1$  \\
layer            & output & cluster & filter & filter  & filter     & filter  & filter    \\ \midrule
Conv1\_1         & 32x32  & 15      & 64     & 19      & 19         & 36      & 36      \\
Conv2\_1         & 16x16  & 20      & 128    & 8       & 8          & 38      & 38      \\
Conv3\_1         &  8x8   & 26      & 256    & 21      & 21         & 64      & 64      \\
Conv4\_1         &  4x4   & 27      & 512    & 146     & 146        & 329     & 329     \\
Conv5\_1         &  2x2   & 46      & 512    & 398     & 398      & 483     & 483     \\ \hline
p                &        &         &        & 0.5     & -        & 1       & -     \\
FLOPs (x$10^{8}$)&  3.13  &         &  -     & 1.85    & 1.85       & 1.03    & 1.03    \\
FLOPs reduction  &        &         &  -     & \textbf{41\%}    & 41\%      & \textbf{68\%}        & 68\%     \\
prune accuracy   &        &         &  -     & 88.1\%  & 82.4\%     & 72.1\%  & 60.2\%      \\
retrain accuracy &        &         & 90.2\%*& \textbf{90.3\%}  & 89.82\%     & \textbf{89.92\%}   & 89.73\%  \\ \bottomrule
\end{tabular}
\caption*{\hspace{-105mm}\footnotesize{*Baseline accuracy}}
\label{tab:VGG}
\end{table}


\begin{table}[t]
\caption{VGG on Cifar10 Pruning Configuration}
\centering
\begin{tabular}{llllllll}
\toprule
                 &        &         & base   & prune A & A-$\ell_1$ & prune B & B-$\ell_1$  \\
layer            & output & cluster & filter & filter  & filter     & filter  & filter    \\ \midrule
Conv1\_1         & 32x32  & 15      & 64     & 19      & 19         & 36      & 36      \\
Conv1\_2         & 32x32  & 17      & 64     & 3       & 3        & 19      & 19      \\
Conv2\_1         & 16x16  & 20      & 128    & 8       & 8          & 38      & 38      \\
Conv2\_2         & 16x16  & 31      & 128    & 17      & 17         & 26      & 26      \\
Conv3\_1         &  8x8   & 26      & 256    & 21      & 21         & 64      & 64      \\
Conv3\_2         &  8x8   & 24      & 256    & 18      & 18         & 64      & 64      \\
Conv3\_3         &  8x8   & 14      & 256    & 36      & 36         & 77      & 77      \\
Conv4\_1         &  4x4   & 27      & 512    & 146     & 146        & 329     & 329     \\
Conv4\_2         &  4x4   & 28      & 512    & 313     & 313      & 424     & 424     \\
Conv4\_3         &  4x4   & 38      & 512    & 223     & 223      & 438     & 438     \\
Conv5\_1         &  2x2   & 46      & 512    & 398     & 398      & 483     & 483     \\
Conv5\_2         &  2x2   & 61      & 512    & 395     & 395      & 432     & 432     \\
Conv5\_3         &  2x2   & 40      & 512    & 218     & 218        & 384     & 384     \\ \hline
p                &        &         &        & 0.5     & -        & 1       & -     \\
FLOPs (x$10^{8}$)&  3.13  &         &  -     & 1.85    & 1.85       & 1.03    & 1.03    \\
FLOPs reduction  &        &         &  -     & \textbf{41\%}    & 41\%      & \textbf{68\%}        & 68\%     \\
prune accuracy   &        &         &  -     & 88.1\%  & 82.4\%     & 72.1\%  & 60.2\%      \\
retrain accuracy &        &         & 90.2\%*& \textbf{90.3\%}  & 89.82\%     & \textbf{89.92\%}   & 89.73\%  \\ \bottomrule
\end{tabular}
\caption*{\hspace{-105mm}\footnotesize{*Baseline accuracy}}
\label{tab:VGG}
\end{table}


\begin{table}[t]
\caption{VGG on ImageNet Pruning Configuration}
\centering
\begin{tabular}{lllllll}
\toprule
        &        &     & FLOPs    & FLOPs          & prune    & retrain        \\
network & clusters & p   & (x$10^{10}$) & reduction      & accuracy & accuracy       \\ \midrule
VGG     &          & -   &  1.54      &                &          &  98.6\%*         \\
prune A &          & 0.5 &  0.84      &  \textbf{45\%}     & 51.9\%   &  \textbf{96.8\%} \\
A-L1    &          & -   &  0.84      &  45\%          & 40.7\%   &  95.7\%          \\
prune B &          & 1.5 &  0.57      &  \textbf{63\%}     & 25.1\%   &  \textbf{94.3\%} \\
A-L1    &          & -   &  0.57      &  63\%          & 21.2\%   &  93.2\%          \\ \bottomrule
\end{tabular}
\caption*{\hspace{-80mm}\footnotesize{*Baseline accuracy}}
\end{table}


\begin{table}[t]
\caption{ResNet on ImageNet Pruning Configuration}
\centering
\begin{tabular}{lllllll}
\toprule
         &        &     & FLOPs     & FLOPs          & prune    & retrain        \\
network  & clusters & p   & (x$10^{10}$)& reduction      & accuracy & accuracy       \\ \midrule
ResNet-32&          & -   &  2.31     &                &          &  97.12\%*        \\
prune A  &          & 0.2 &  1.73       &  \textbf{25\%}     &  94.75\% &  \textbf{97.50\%}\\
A-L1     &          & -   &  1.73     &   25\%         &  92.87\% &  96.25\%         \\
prune B  &          & 0.3 &  1.55       &  \textbf{33\%}     &  91.25\% &  \textbf{96.25\%}\\
A-L1     &          & -   &  1.55     &   33\%         &  90.37\% &  94.75   \%      \\ \bottomrule
\end{tabular}
\caption*{\hspace{-80mm}\footnotesize{*Baseline accuracy}}
\end{table}


\paragraph{Visualization based Filter Clustering}

In the Table~\ref{tab:1}, we first show the filter cluster distribution based on our visualization oriented clustering method.
Then, the last three columns show the pruned models of $\ell_1$ based filter pruning [1] and two clustering based parallel pruning.

  For 13 convolutional layers of VGG-16, the filters in each layer are grouped into 14$\sim$61 clusters.
  With the increasing of layer depth, the cluster amount also increases: in layer Conv5-2, the cluster is as much as 61.
  This is because the image complexity increment for the visualized graphic patterns, which minimize the similarity between two patterns.
  Therefore, the number of filter clusters also increases to adapt to meticulous filter similarities.
  Meanwhile, our proposed method clustered as much filter as possible.
  The minimum cluster ratio in all the layers remains above 85\%.
  In overall, about 93\% filters are well clustered, indicating an efficient filter redundancy analysis capability.


\paragraph{Hierarchical Filter Pruning}

Based on the filter clustering, a hierarchical filter pruning method is proposed to different levels of the neural network.
With the help of the adaptive pruning rate, each filter cluster can be pruned in an even rate, preventing overwhelmed pruning on individual clusters.
  In the Table~\ref{tab:1}, we can see that the $\ell_1$ based state-of-the-art only prunes small amount of filters, and the filters in several middle layers remained untouched.
  In order to compare with the $\ell_1$ based pruning directly, we pruning similar amount filters in each layer, which are showed by Parallel Pruning 1.
  The Parallel Pruning 2 shows our final pruning configurations.
  Compare to state-of-the-art, our proposed clustering method significantly boost the pruning ratio, especially in the middle layers, where each filter undertakes more computation cost.
  Our clustering method increased the pruned filter number up to 2082, which is $\sim$1.3$\times$ to the state-of-the-art.
  % Although, the accuracy drop is 4.55\% that is larger than the $\ell_1$ based Filter Pruning, we can eventually compensate the performance by retraining.
  % Therefore, the visualization based filter clustering method not only illustrate the functional repetition of the convolutional filters, but also enhance the filter pruning rate with optimal accuracy maintained.


  In Fig.~\ref{fig:fp}a, the effectiveness of the hierarchical pruning scheme is demonstrated.
  We can see that, with the proposed pruning method before half of the repetitive convolutional filters are pruned, all the clusters perform an very slow accuracy dropping rate.
  Meanwhile, as shown in Fig.~\ref{fig:fp}b, the $\ell_1$ based pruning scheme demonstrated fast accuracy dropping rate.
  Therefore, the visualization-oriented filter pruning demonstrated significant pruning stability and robustness.
  In other words, through the interpretable approach, the redundant filter can be more accurately identified and pruned.

\paragraph{Per Class Pruning}
As aforementioned, leveraging higher layer filters' class level preference, we also proposed the per class pruning scheme, which can effectively reconfigure the network to class specific.
  In Fig.~\ref{fig:cc}, we use the CNN to screen cat from random test images. According to our class label on the filter clusters, we prune all the cat-unrelated filter clusters in higher layer and examine the classification results.
\begin{wrapfigure}{r}{2.5in}
  %\leftaligned
  \captionsetup{justification=centering}
  %\rule{3cm}{7cm}
  \vspace{-4mm}
  \includegraphics[width=2.5in]{./_fig/6_class.pdf}
  \vspace{-5mm}
  \caption{Per-class filter pruning}
  \label{fig:cc}
  \vspace{-5mm}
\end{wrapfigure}
  Interestingly, with more cat-unrelated filter clusters pruned, the recognition results for cat can be further improved by almost 5\%, while other classes results are just normally decreasing.
In other words, with the per class pruning scheme, the CNN can be configured into certain class-specific detector with outstanding accuracy.
However, it is important to note that, such an adaptation scheme is built on the assumption that the CNN is used for explicit data with specific class or the under test data demonstrate certain class consistency that can be estimated for dedicated adaptation.
Fortunately, this assumption can be extensively founded with particular CNN utilization scenarios, especially on mobile systems.

\subsection{Filter Retraining Analysis}
In most state-of-the-art works for neural network compression, the retraining is an inevitable operation to compensate the inaccurately pruned filters and enhance the network performance.
  In Fig.~\ref{fig:retrain}, we explore the effectiveness and mechanism of the retraining operation through the experimental analysis on a single convolutional layer's filter distribution.

As shown in Fig.~\ref{fig:retrain}b, the $\ell_1$ ranking based pruning significantly biases the filter distribution to the higher $\ell_1$ ranking range, resulting in considerable classification accuracy drop of 3.1\%.
  Based on this pruning result, the dedicated layer-level retraining operation is applied to reconfigure all the filters, and eventually compensate the performance by 3.2\%.
  From the filter distribution change, we can see that, the compensation is actually achieved by reconstructing the pruned filters with smaller $\ell_1$ rankings (\textit{i. e.} weight significance).
  And the filter density in the higher $\ell_1$ ranking range is correspondingly reduced, indicating certain redundancy reduction.

Meanwhile, with our proposed visualization oriented filter pruning applied as shown in Fig.~\ref{fig:retrain}c, the significance of the retraining operation is relatively weakened.
  Considering the retraining operation may change the interpretation of each filter and potentially disturb the filter clustering, we apply it only once after pruning the whole network in our proposed method.
  The experiment results show that, regardless the retraining operation, the layer's filter distribution is remained uniformed across the whole  $\ell_1$ ranking range, maintaining sufficient $\ell_1$ ranking variety.
  Therefore, the visualization oriented filter pruning only cause accuracy drop of 1.9\%, which is much less compared to the $\ell_1$ ranking based pruning with the same pruning rate.
  However, the retraining also become less effective for performance compensation.

Based on the experimental analysis of filter retraining, we can see that, the network performance is not only supported by the filters with higher weight significance but also the smaller ones.
  The idea pruning process would reduce the filter redundancy for each feature representation, while maintaining certain functional integrity.
  Meanwhile the retraining process can effectively reconstruct the smaller filters and sparse the filter distribution to compensate the performance.
  However, with more interpretable and accurate redundant filter identification and pruning, the costy retraining process will become less necessary.
\begin{figure}[t]
  \centering
  \includegraphics[width=5.5in]{./_fig/7_retrain.pdf}
  \vspace{-10mm}
    \caption{Filter retraining analysis based on $\ell_1$ ranking and Euclidean Distance of the visualized filter activation:
    (a) Original convolutional filter distribution in Conv5-1 layer. Filters demonstrate uniform distribution in all range of $\ell_1$ ranking.
    (b) Filter distribution after $\ell_1$ pruning with 50\% filters pruned (denoted in blue), and dedicated retraining operation for each layer (denoted in orange).
    (c) Filter distribution after the visualization-oriented pruning with the same pruning rate (denoted in blue), and one comprehensive retraining operation for the whole network (denoted in green).}
  \label{fig:retrain}
  \vspace{-5mm}
\end{figure}

\subsection{Comparison with state-of-the-art Methods}
The performance comparison between our proposed visualization-oriented pruning method and state-of-the-art method [1] is shown in Table 2.
    From Table~\ref{tab:2}, we can see that, our proposed method significantly outperforms state-of-the-art.
    Given the same saved FLOP ratio, our proposed method (Parallel Pruning 1) has less accuracy drop.
    Meanwhile, when the retraining operation is applied to pruning, our proposed method case (Parallel Pruning 2) can significantly enhance the computation cost optimization to 45\%, which improved the state-of-the-art by 32\%.
    Moreover, when per class pruning scheme is utilized, give 58\% computation cost optimization, the accuracy is even higher than the baseline.
    Overall, the visualization-oriented filter pruning methods dramatically outperforms state-of-the-art.

\begin{table}[b]
  \caption{Comparison with state-of-the-art Methods}
  \label{sample-table}
  \centering
  \begin{tabular}{llll}
    \toprule
    Method     & Saved FLOP (\%) &  Accuracy Drop (\%) &  Retrain Drop (\%) \\
    \midrule
    $\ell_1$           &    34          &      3.1           &     0     \\
    Parallel Pruning 1     &    34          &      1.9           &     -0.1  \\
    Parallel Pruning 2     &    45          &      4.55          &     0.1   \\
    Cat                &    58          &      -4.6          &           \\
    \bottomrule
  \end{tabular}
  \label{tab:2}
\end{table}


\begin{table}[b]
\caption{VGG Pruning Configuration}
\centering
\begin{tabular}{llllllll}
\toprule
                 &        &         & base   & prune A & A-$\ell_1$ & prune B & B-$\ell_1$  \\
layer            & output & cluster & filter & filter  & filter     & filter  & filter    \\ \midrule
Conv1\_1         & 32x32  & 15      & 64     & 34      & 34         & 26      &       \\
Conv1\_2         & 32x32  & 17      & 64     & 0       & 0        & 11      &       \\
Conv2\_1         & 16x16  & 20      & 128    & 0       & 0          & 44      &       \\
Conv2\_2         & 16x16  & 31      & 128    & 0       & 0        & 6       &       \\
Conv3\_1         &  8x8   & 26      & 256    & 0       & 0          & 25      &       \\
Conv3\_2         &  8x8   & 24      & 256    & 0       & 0          & 33      &       \\
Conv3\_3         &  8x8   & 14      & 256    & 0       & 0          & 36      &       \\
Conv4\_1         &  4x4   & 27      & 512    & 256     & 256        & 172     &       \\
Conv4\_2         &  4x4   & 28      & 512    & 258     & 256      & 204     &       \\
Conv4\_3         &  4x4   & 38      & 512    & 256     & 256      & 432     &       \\
Conv5\_1         &  2x2   & 46      & 512    & 261     & 256      & 435     &       \\
Conv5\_2         &  2x2   & 61      & 512    & 264     & 256      & 405     &       \\
Conv5\_3         &  2x2   & 40      & 512    & 264     & 256        & 273     &       \\ \hline
p                &        &         &        &         &          &         &       \\
FLOPs (x$10^{6}$)& 313    &         &  -     &         &          &         &       \\
FLOPs reduction  &        &         &  -     & 34\%    & 34\%       & 49.3\%  &       \\
accuracy drop    &        &         &  -     & 1.8     & 3.1      & 4.55    &       \\
retrain accuracy &        &         & 90.2\% & 90.3\%  & 90.2\%     & 90.1\%   &      \\ \bottomrule
\end{tabular}
\label{tab:VGG}
\end{table}

\begin{table}[]
\begin{tabular}{p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}}
\hline
\multicolumn{6}{c}{CIFAR-10}                                 &  & \multicolumn{7}{c}{ImageNet-10}                                         \\ \cline{1-6} \cline{8-14}
Network   & prune  & FLOPs & FLOPs     & prune    & retrain  &  & Network   & prune & FLOPs & FLOPs     & prune     & prune    & retrain  \\
          & ration & ()    & reduction & accuracy & accuracy &  &           &       &       & reduction & reduction & accuracy & accuracy \\ \cline{1-6} \cline{8-14}
ConvNet   &        &       &           &          &          &  & VGG       &       &       &           &           &          &          \\
Prune A   &        &       &           &          &          &  &           &       &       &           &           &          &          \\
L1\_Based &        &       &           &          &          &  &           &       &       &           &           &          &          \\
Prune B   &        &       &           &          &          &  &           &       &       &           &           &          &          \\
L1\_Based &        &       &           &          &          &  &           &       &       &           &           &          &          \\ \cline{1-6} \cline{8-14}
VGG       &        &       &           &          &          &  & ResNet-32 &       &       &           &           &          &          \\
Prune A   &        &       &           &          &          &  &           &       &       &           &           &          &          \\
L1\_Based &        &       &           &          &          &  &           &       &       &           &           &          &          \\
Prune B   &        &       &           &          &          &  &           &       &       &           &           &          &          \\
L1\_Based &        &       &           &          &          &  &           &       &       &           &           &          &          \\ \hline
\end{tabular}
\end{table}

\begin{table}[t]
\caption{Convnet on Cifar10 Pruning Configuration}
\centering
\begin{tabular}{llllllllll}
\toprule
                 &        & base    & cluster& prune A & A-$\ell_1$  & prune B & B-$\ell_1$ \\
layer            & output & filter  & distribution& filter  & filter      & filter  & filter    \\ \midrule
Conv1            & 32x32  & 96      &  15    & 42      & 42          & 64      & 54     \\
Conv2            & 32x32  & 256     &  40    & 75      & 75        & 153     & 112      \\
Conv3            & 16x16  & 384     &  53    & 54      & 54        & 167     & 85     \\
Conv4            & 16x16  & 384     &  36    & 49      & 49        & 166     & 112    \\
Conv5            & 8x8    & 256     &  24    & 85      & 85        & 149     & 112      \\ \hline
Pr               & -      & -       &  -     & 0.5     & -           & 2.5     & -        \\
FLOPs (x$10^{8}$)& 8.53   & -       &  -     & 5.34    & 5.34        & 2.1     & 2.1    \\
FLOPs reduction  & -      & -       &  -     & \textbf{37\%}    & 37\%        & \textbf{75\%}    & 75\%     \\
prune accuracy   & -      & -       &  -     & 87.88\% & 83.29\%     & 65.61\% & 53.72\%  \\
retrain accuracy & -      & 90.05\%* &  -     & \textbf{90.04\%} & 89.42\%     & \textbf{88.57\%} & 87.88\%    \\ \bottomrule
\end{tabular}
\caption*{\hspace{-112mm}\footnotesize{*Baseline accuracy}}
\label{tab:Convnet}
\end{table}


\begin{table}[t]
\caption{VGG on Cifar10 Pruning Configuration}
\centering
\begin{tabular}{lllllll}
\toprule
         &             & FLOPs    & FLOPs          & prune    & retrain        \\
network  &           Pr  & (x$10^{8}$)  & reduction      & accuracy & accuracy       \\ \midrule
VGG-16   &           -   &  3.31      &                &          &  90.2\%*         \\
prune A  &           0.5 &  1.85        &  \textbf{41\%}     &  88.1\%  &  \textbf{90.3\%} \\
A-L1     &           -   &  1.85      &   41\%         &  82.4\%  &  96.25\%         \\
prune B  &           1   &  1.03        &  \textbf{68\%}     &  72.1\%  &  \textbf{89.9\%} \\
A-L1     &           -   &  1.03      &   68\%         &  60.2\%  &  89.73   \%      \\ \bottomrule
\end{tabular}
\caption*{\hspace{-65mm}\footnotesize{*Baseline accuracy}}
\end{table}


\begin{table}[]
\begin{tabular}{p{0.4in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.01in}p{0.4in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}}
\hline
\multicolumn{5}{c}{CIFAR-10}                         &  & \multicolumn{6}{c}{ImageNet-10}                                 \\ \cline{1-5} \cline{7-12}
Network   & prune  & FLOPs     & prune    & retrain  &  & Network   & prune & FLOPs     & prune     & prune    & retrain  \\
          & ratio  & reduction & accuracy & accuracy &  &           & ratio & reduction & reduction & accuracy & accuracy \\ \cline{1-5} \cline{7-12}
ConvNet   &        &           &          &          &  & VGG       &       &           &           &          &          \\
Prune A   &        &           &          &          &  &           &       &           &           &          &          \\
L1\_Based &        &           &          &          &  &           &       &           &           &          &          \\
Prune B   &        &           &          &          &  &           &       &           &           &          &          \\
L1\_Based &        &           &          &          &  &           &       &           &           &          &          \\ \cline{1-5} \cline{7-12}
VGG       &        &           &          &          &  & ResNet-32 &       &           &           &          &          \\
Prune A   &        &           &          &          &  &           &       &           &           &          &          \\
L1\_Based &        &           &          &          &  &           &       &           &           &          &          \\
Prune B   &        &           &          &          &  &           &       &           &           &          &          \\
L1\_Based &        &           &          &          &  &           &       &           &           &          &          \\ \hline
\end{tabular}
\end{table}

\end{comment}
\section{Conclusion}

In this work, through activation maximization based visualization and gradient based filter functionality analysis, we firstly show that convolutional neural network filter redundancy exists in the form of functionality repetition. In other words, the functional repetitive filters could be effectively pruned from neural network to provide computation redundancy. Based on such motivation, in this paper, we propose an interpretable filter pruning method by first clustering filters with same functions together and then reducing the repetitive filters with smallest significance and contribution. Extensive experiments on CIFAR and ImageNet demonstrate the superior performance of our interpretable pruning method over state-of-the-art method, e.g. $\ell_1$ ranking based pruning. By further analyzing the functionality changing of remaining filters in the retraining process, we show that $\ell_1$ ranking based pruning actually partially destructs original neural network's functionality composition. By contrast, our method shows consistent neuron functionality during retraining process, denoting less harm to original network composition. This reveals the underlying reasons of our methods' better performance than L1 method.
\begin{figure}[hb]
  \centering
  \includegraphics[width=5.5in]{./Retrain_visua_wide.pdf}
  \caption{Filter functionality transition during retraining.}
  \label{fig:Retrain_pattern}
  \vspace{-2mm}
\end{figure}

\bibliography{iclr2019_conference}
\bibliographystyle{iclr2019_conference}

\newpage
\section{Appendix}\subsection{Network configuration}
Table~\ref{tab:ConvNet_app} and Table~\ref{tab:VGG_app} show the detailed network structure and pruning configuration of our ConvNet and the VGG-16 on CIFAR10 respectively.
The cluster distribution is the total number of clusters in each convolutional layers.

\begin{table}[h]
\caption{ConvNet on CIFAR10 Pruning Configuration}
\centering
\begin{tabular}{llllllll}
\toprule
                 &        & cluster & base   & prune A & A-$\ell_1$ & prune B & B-$\ell_1$	\\
layer            & output & distribution & filter & filter  & filter     & filter  & filter		\\ \midrule
Pr               &        &         &        & 0.5     & -       	& 2.5     & -			\\
Conv1\_1         & 32x32  & 15      & 64     & 19      & 19         & 36      & 36			\\
Conv2\_1         & 16x16  & 20      & 128    & 8       & 8        	& 38      & 38			\\
Conv3\_1         &  8x8   & 26      & 256    & 21      & 21        	& 64      & 64			\\
Conv4\_1         &  4x4   & 27      & 512    & 146     & 146       	& 329     & 329			\\
Conv5\_1         &  2x2   & 46      & 512    & 398     & 398     	& 483     & 483			\\ \bottomrule
\end{tabular}
\label{tab:ConvNet_app}
\end{table}\vspace{-2mm}\begin{table}[h]
\caption{VGG-16 on CIFAR10 Pruning Configuration}
\centering
\begin{tabular}{llllllll}
\toprule
                 &        &  cluster& base   & prune A & A-$\ell_1$ & prune B & B-$\ell_1$	\\
layer            & output & distribution& filter & filter  & filter     & filter  & filter		\\ \midrule
Pr               &        &         &        & 0.5     & -       	& 1       & -			\\
Conv1\_1         & 32x32  & 15      & 64     & 19      & 19         & 36      & 36			\\
Conv1\_2         & 32x32  & 17      & 64     & 3       & 3       	& 19      & 19			\\
Conv2\_1         & 16x16  & 20      & 128    & 8       & 8        	& 38      & 38			\\
Conv2\_2         & 16x16  & 31      & 128    & 17      & 17       	& 26      & 26			\\
Conv3\_1         &  8x8   & 26      & 256    & 21      & 21        	& 64      & 64			\\
Conv3\_2         &  8x8   & 24      & 256    & 18      & 18        	& 64      & 64			\\
Conv3\_3         &  8x8   & 14      & 256    & 36      & 36        	& 77      & 77 			\\
Conv4\_1         &  4x4   & 27      & 512    & 146     & 146       	& 329     & 329			\\
Conv4\_2         &  4x4   & 28      & 512    & 313     & 313     	& 424     & 424			\\
Conv4\_3         &  4x4   & 38      & 512    & 223     & 223     	& 438     & 438			\\
Conv5\_1         &  2x2   & 46      & 512    & 398     & 398     	& 483     & 483			\\
Conv5\_2         &  2x2   & 61      & 512    & 395     & 395     	& 432     & 432			\\
Conv5\_3         &  2x2   & 40      & 512    & 218     & 218      	& 384     & 384			\\ \bottomrule
\end{tabular}
\label{tab:VGG_app}
\end{table}\subsection{Filter Functionality Transition During Retraining}\begin{figure}[h]
  \centering
  \includegraphics[width=5.5in]{./Retrain_visua.pdf}
  \caption{Filter functionality transition during retraining.}
  \label{fig:Retrain_visua_app}
  \vspace{-2mm}
\end{figure}

\end{document}
