
%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color}
\usepackage{colortbl}
% \usepackage{subfigure}
\usepackage{subfig}
\usepackage{flushend}

\definecolor{cwblue1}{rgb}{0.27,0.427,0.623}
\definecolor{cwblue2}{rgb}{0.286,0.454,0.658}
\definecolor{cwblue3}{rgb}{0.733,0.811,0.905}

\newcommand{\cw}[1]{\textbf{\textcolor{blue}{Christian: #1}}}
\newcommand{\ck}[1]{\textbf{\textcolor{red}{Christopher: #1}}}
\newcommand{\bm}[1]{\textbf{\textcolor{green}{Bastien: #1}}}

\newcommand{\para}[1]{\textbf{#1} ---}

%%%%%%%%% NOTATIONS %%%%%%%%%

% * Outputs, intermediate representations...
\def\score{\rho} % Originally s
\def\hij{\h(i,j)} % {\h_{i,j}} %  ?

% * Fixed hyper-parameters
\def\DeltaX{{\Delta_{\mathrm{x}}}}
\def\deltaX{{\delta_{\mathrm{x}}}}
\def\DeltaY{{\Delta_{\mathrm{y}}}}
\def\deltaY{{\delta_{\mathrm{y}}}}
\def\WIDTH{W} % JL: Previously M
\def\HEIGHT{H} % JL: Previously N
\def\numOutputsPerBlock{N} % JL: Previously K
%\def\numOutputs{N'} % JL: Previously N/K

% * Free parameters
%\def\W{{\mathbf{W}}}
\def\w{{\mathbf{w}}}
\def\u{{\mathbf{u}}}
\def\v{{\mathbf{v}}}
\def\b{{\mathbf{b}}}
\def\h{{\mathbf{h}}}
%\def\c{{\mathbf{c}}}
\def\U{{\mathbf{U}}}
\def\sigmoid{\sigma}
\def\transposed{^\top}%\def\transposed{^{\scriptscriptstyle\mathrm{T}}}%{^{\scriptstyle\mathbf{'}}}

\def\NA{} % previously {/}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Full-Page Text Recognition: \\ Learning Where to Start and When to Stop}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
%\author{\IEEEauthorblockN{Bastien Moysset}
%\IEEEauthorblockA{A2IA SA, PARIS,  FRANCE \\
%INSA-LYON, LIRIS, UMR5205, F-69621\\
%bm@a2ia.com}
%\and
%\IEEEauthorblockN{Christopher Kermorvant}
%\IEEEauthorblockA{TEKLIA SAS, PARIS, FRANCE}
%\and
%\IEEEauthorblockN{Christian Wolf}
%\IEEEauthorblockA{UNIVERSIT\'E DE LYON, CNRS, FRANCE \\
%INSA-LYON, LIRIS, UMR5205, F-69621}
%}

%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


\author{
\IEEEauthorblockN{Bastien Moysset\IEEEauthorrefmark{1}\IEEEauthorrefmark{4}, Christopher Kermorvant\IEEEauthorrefmark{2}, Christian Wolf\IEEEauthorrefmark{3}\IEEEauthorrefmark{4}}
%
%
\IEEEauthorblockA{\IEEEauthorrefmark{1}A2iA SA, Paris,  France}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Teklia SAS, Paris,  France}
\IEEEauthorblockA{\IEEEauthorrefmark{3}Universit\'e de Lyon, CNRS, France }
\IEEEauthorblockA{\IEEEauthorrefmark{4}INSA-Lyon, LIRIS, UMR5205, F-69621}
%
%\IEEEauthorblockN{\\Anonymous ICDAR submission}
%\IEEEauthorblockA{\\Paper ID 9}
%
}


% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}

Text line detection and localization is a crucial step for full page document analysis, but still suffers from heterogeneity of real life documents. In this paper, we present a new approach for full page text recognition. Localization of the text lines is based on regressions with Fully Convolutional Neural Networks and Multidimensional Long Short-Term Memory as contextual layers.

In order to increase the efficiency of this localization method, only the position of the left side of the text lines are predicted. The text recognizer is then in charge of predicting the end of the text to recognize. This method has shown good results for full page text recognition on the highly heterogeneous Maurdor dataset.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart

Most applications in document analysis require text recognition at page level, where only the raw image is available and no preliminary hand-made annotation can be used. Traditionally, this problem has mainly been addressed by separating the process into two distinct steps; namely the text line detection task, which is frequently proceeded by additional paragraph and word detection steps, and the text recognition task. In this work we propose a method, which couples these two steps tighter by unloading some of the burden of the difficult localization step to the recognition task. In particular, the localization step detects the starts of the text lines only. The problem of finding where to stop the recognition is solved by the recognizer itself.


\subsection{Related work}
Numerous algorithms have been proposed for the text line localization. Some are used in a bottom-up approach by grouping sub-components like connected components or black pixels into lines. RLSA \cite{wong1982document} uses morphological opening on black pixels to merge the components that belong to the same text line. Similarly, Shi et al. \cite{Shi2009a} resort to horizontal ellipsoidal steerable filters to blur the image and merge the components of the text line. In \cite{WolfICPR2002V}, gradients are accumulated and filtered. Louloudis et al. \cite{Louloudis2009b} employ a Hough algorithm on the connected component centers while Ryu et al. \cite{ryu2014language} cluster parts of the connected components according to heuristic based successive splits and merges.

Other methods follow a top-down approach and split the pages into smaller parts. The XY-cut algorithm \cite{nagy1984hierarchical} looks for vertical and horizontal white spaces to successively split the pages in paragraphs, lines and words. Similarly, projection profile algorithms like Ouwayed et al. \cite{ouwayed2012general} are aimed at finding the horizontal whiter parts of a paragraph. This technique is extended to non-horizontal texts by methods like Nicolaou et al. \cite{Nicolaou2009} that dynamically finds a path between the text lines or by Tseng et al. \cite{tseng1999recognition} that use a Viterbi algorithm to minimize this path.

Techniques like the ones proposed by Mehri et al. \cite{mehri2013pixel} or Chen et al. \cite{chen2015page} classify pixels into text or non-text but need post-processing techniques to constitute text lines.

These techniques usually work well on the homogeneous datasets they have been tuned for but need heavy engineering to perform well on heterogeneous datasets like the Maurdor dataset \cite{Brunessaux2014}. For this reason, Machine learning has proven to be efficient, in particular deep convolutional networks. Early work from Delakis et al. \cite{delakis2008text} classifies scene text image parts as text and non-text with a Convolutional Neural Network on a sliding window. In \cite{moysset2015paragraph}, paragraph images are split vertically using a recurrent neural network and CTC alignment. More recently, methods inspired from image object detection techniques like MultiBox \cite{erhan2014scalable}, YOLO \cite{YOLOCVPR2016} or Single-Shot Detector (SSD) \cite{LiuErhanSSD2016} have arisen. Moysset et al. \cite{moysset2016learning} proposed a MultiBox based approach for direct text line bounding boxes detection. Similarly, Gupta et al. \cite{gupta2016synthetic} and Liao et al. \cite{liao2016textboxes} use respectively YOLO based and SSD based approach for scene text detection. Moysset et al. \cite{moysset2016points} also propose the separate detection of bottom-left and top-right corners of line bounding boxes.

The text recognition part is usually made with variations of Hidden Markov Models \cite{bertolami2008hidden} or 2D Long Short Term Memory (2D-LSTM) \cite{Graves2DLSTM2009} neural networks.

Finally, Bluche et al. \cite{BlucheNIPS2016} use a hard attention mechanism to directly perform full page text recognition without prior localization. The iterative algorithm finds the next attention point based on the sequence of seen glimpses modeled through the hidden state of a recurrent network.

\subsection{Method overview}
In this work, we address full page text recognition in two steps. First, a neural network detects where to start to recognize a text line, and a second network performs the text recognition and decides when to stop the process. More precisely, the former network detects the left sides of each text lines by predicting the value of the object position coordinates as a regression problem. This detection neural network system is detailed in Part \ref{sec:objectLocalization} and the left-side strategy is explained in Part \ref{sec:triplets}. The latter network recognizes the text and predicts the end of the text of the line  as described in Part \ref{sec:recognizer}. The experimental setup is described in Part \ref{sec:experimental} and results are shown and analyzed in Part \ref{sec:results}.

\section{Object localization with deep networks}
\label{sec:objectLocalization}

\subsection{Network description}

\begin{table}
\begin{center}
\caption{Network architecture/hyper-parameters. The input and feature map sizes are an illustrative example. The number of parametres does \emph{NOT} depend on the size of the input image.}
\label{tab:architecture}
\begin{tabular}{lcccr}
Layer & Filter  & Stride & Size of the  & Number of  \\ %& feature map size??
      & size    &        & feature maps & parameters \\
\arrayrulecolor{cwblue1} \toprule  
Input & / & / & 1$\times$(598$\times$838) & \NA \\
C1  & 4$\times$4 & 3$\times$3 & 12$\times$(199$\times$279) & 204 \\
LSTM1 & / & / & " " & 8880 \\
C2 & 4$\times$3 & 3$\times$2 & 16$\times$(66$\times$139) & 2320 \\
LSTM2 & / & / & " " & 15680 \\
C3 & 6$\times$3 & 4$\times$2 & 24$\times$(16$\times$69) & 6936 \\
LSTM3 & / & / & " " & 35040 \\
C4 & 4$\times$3 & 3$\times$2 & 30$\times$(5$\times$34) & 8670 \\
LSTM4 & / & / & " " & 54600 \\
C5 & 3$\times$2 & 2$\times$1 & 36$\times$(2$\times$33) & 6516 \\
Output & 1$\times$1 & 1$\times$1 & 4$\times$20$\times$(2$\times$33) & 2960 \\
\end{tabular}
\end{center}
\end{table}

In the lines of \cite{erhan2014scalable}, we employ a neural network as a regressor to predict the positions of objects in images. The network predicts a given number $N$ of object candidates. Each of these object candidates is indexed by a linear index $n$ and defined by $K$ coordinates $l_n{=}\{ l^{k}_n \}, \ k{=}{1\dots4K}$ corresponding to the position of the object in the document and a confidence score $c_n$. As the number of objects in an image is variable, at test time, only the objects with a confidence score over a threshold are kept.

In order to cope with the small amount of training data available for   document analysis tasks and to detect a large number of objects corresponding to our text lines, we adopted the method described in \cite{moysset2016learning}. We do not use a fully connected layer at the end of the network that has as inputs features conveying information about the whole page image and, as outputs, all the object candidates of the page. Instead, our method is fully convolutional, which allows the network to share parameters over the different regressors. More precisely, we use a $1{\times}1$ convolution to predict the objects locally and, consequently, to highly reduce the number of parameters in the network. 

Layers constituted of Two-Dimensional Long-Short-Term-Memory cells (2D-LSTM) \cite{Graves2DLSTM2009} are interleaved between the convolutional layers in order to recover the context information lost by the local nature of the detection. 

The architecture is similar to the one in \cite{moysset2016learning}. It is described in Table \ref{tab:architecture} and illustrated in Figure \ref{fig:archi}.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{teaser-sdl.png}
  \caption{Sketch of the Convolutional Recurrent Neural Network that locally predicts the object positions (we do not show the correct numbers of layers and units).}
  \label{fig:archi}
\end{figure}

\subsection{Training}
We used the same training process as the one described in \cite{erhan2014scalable}. The cost function is a weighted sum between a confidence cost and the Euclidean distance between the two object positions (predicted and ground-truth):
\begin{equation}
\label{eq:globalCost}
\begin{split}
Cost & = \sum_{n=0}^N \sum_{m=0}^M X_{nm} \left( \alpha \left\|l_{n}-t_{m}\right\|^2 - \log(c_{n})\right) \\
 & - (1 - X_{nm}) \log(1-c_{n})
\end{split}
\end{equation}
Here, the $N$ object candidates have position coordinates $l_{n}$ and confidence $c_{n}$ while the $M$ reference objects have position coordinates $t_{m}$. $\alpha$ is a parameter weighting localisation and confidence costs.  As the output of the network (as well as the ground-truth information) is structured, a matching between the two of them is necessary in order to calculate the loss in equation \ref{eq:globalCost}. This matching is modelled through variable $X{=}\{X_{nm}\}$, a binary matrix. In particular, $X_{nm}{=}1$ if network output $n$ has been matched to ground truth object $m$ in the given image. Equation \ref{eq:globalCost} needs to be minimized under constraints enforcing one-to-one matches, which can be solved efficiently through the Hungarian algorithm \cite{munkres1957algorithms}.

We could not confirm the claims reported in \cite{erhan2014scalable} who apply this matching process for object detection in natural images. In particular, no improvement was found when using anchor positions associated to  objects which were mined through k-means clustering. On the other hand, we found it useful to employ different weights $\alpha$ for the two different uses of equation \ref{eq:globalCost}. A higher value for $\alpha$ was used during matching (solving for $X$) than for backpropagation (learning of network parameters). This favours the use of all outputs during training --- details are given in section \ref{sec:results}.

\section{Localization and recognition}
% In this section we describe the proposed method for localization and recognition, which is summarized in Figure \ref{fig:triplets}.

\begin{figure}[!t]
  \centering
  \small{(a)} \includegraphics[width=2.5in]{TripletsPipe_ImageWithTriplets.pdf} \\
  \small{(b)} \includegraphics[width=2.5in]{TripletsPipe_ImageWithEndOfLine.pdf} \\
  \small{(c)} \includegraphics[width=2.5in]{TripletsPipe_Recognition.pdf}
  \caption{Description of the triplet-based localization techniques. (a) Detection of left-side triplet objects. (b) Extraction of corresponding text lines. (c) Recognition of text lines with End-of-line characters (EOL).}
  \label{fig:triplets}
\end{figure}

\subsection{Line detection with left-side triplets}
\label{sec:triplets}
The first step detects the left-side of each lines through the network described in Section \ref{sec:objectLocalization}. The model predicts three position values, i.e. $K{=}3$: 2 coordinates for the lower left corner plus the text height. Additionally, a prediction confidence score is output.

We also compare this method with two competing strategies: 

i) point localization \cite{moysset2016points}, $K{=}2$, where only the x and y coordinates of lower left points are detected, 

ii) and full box localization \cite{moysset2016learning}, where $K{=}4$ and the x and y coordinates of the bottom-left corners of the text line bounding boxes are predicted with the width and the height of the text lines. 

We also found that expanding the text box by a 10 pixel margin 
% in every direction 
improves the full-page text recognition rates. 

% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=\linewidth]{Figs/teaser-sdl.png}
%   \caption{Sketch of the Convolutional Recurrent Neural Network that locally predicts the object positions (we do not show the correct numbers of layers and units).}
%   \label{fig:triplets}
% \end{figure}

\subsection{End-of-line detection integrated with recognition}
\label{sec:recognizer}
Detecting only the left side of the text lines and extending it toward the right part of the image as illustrated in Figure \ref{fig:triplets} b) means that for documents with complex layouts, some text from other text lines can be present in the image to be recognized.

For this reason, a 2D-LSTM based recognizer similar to the one described in \cite{pham2014dropout} is trained with the Connectionist Temporal Classification \cite{Graves06connectionisttemporal} (CTC) alignment procedure to recognize the text present in, and only in, the designed text line. We found that the results were slightly improved by adding a End-of-line (EOL) label at the end of the text labels. 
This  means that the network will learn, through the CTC training, to align the text labels with the frames corresponding to these image characters, as usual. But it will also learn to predict when the line is over, mark it with the EOL label, and learn  not to predict anything else on the right side of the image. The context conveyed by the LSTM recurrent layers is responsible for this learning ability.

Two different recognition networks are trained, respectively for French and English. They are trained to recognized both printed and handwritten simultaneously. 

\section{Experimental Setup}
\label{sec:experimental}
\subsection{Datasets}
We evaluate our method on the Maurdor dataset \cite{Brunessaux2014}, which is composed of 8773 heterogeneous documents in French, English or Arabic, both printed and handwritten (Train: 6592, Validation: 1110, Evaluation: 1071). Because the annotation is given at paragraph level, we used the technique described in \cite{bluche2014automatic} to check the quality of line candidates with a constrained text recognition in order to obtain annotation at line level. All these lines are used to train the text recognizers described in section \ref{sec:recognizer} and, on the test set, for the recognition experiments in Table \ref{tab:reco}.

For training the text line detection systems, only the 5308 pages for which we are confident enough that all the lines are detected are kept (Train: 3995, Validation: 697, Evaluation: 616). This subset is also used for the precision experiments shown in Tables \ref{tab:precisenessPoint} and \ref{tab:precisenessTriplet}.

Finally, for the end-to-end evaluation results shown in Table \ref{tab:fullPage}, we kept all the original documents of the test set in only one language, in order to avoid the language identification problem. We obtain 507 pages in French and 265 pages in English.

\subsection{Metrics}
Three metrics were used for evaluation : 

\begin{enumerate}
	\item \textbf{F-Measure} metrics is used in Tables \ref{tab:precisenessPoint} and \ref{tab:precisenessTriplet} in order to measure precision the  of detected objects being in the neighbourhood of reference objects. A detected object $l$ is considered as correct if it is the closest hypothesis from the reference object $t$ and if $||l^k - t^k|| < T$ for all $k \in [0,K]$ where K is the number of coordinates per object set to 2 for Table \ref{tab:precisenessPoint} (points) and to 3 in Table \ref{tab:precisenessTriplet} (triplets) and $T$ is the size of the acceptance zone given as a proportion of the page width. 
	\item \textbf{Word Error Rate} (WER) metrics is the word level Levenshtein distance \cite{levenshtein1966binary} between recognized and reference sequences of text.
	\item \textbf{Bag of Word} (BOW) metrics is given at page level as a F-Measure of words recognized or not in the page. As explained in \cite{pletschacher2015europeana}, it is a proper metric to compute recognition rate at page level because it does not need any alignment or ordering of the text lines that can be ambiguous for unconstrained documents.
	
\end{enumerate} 



\subsection{Hyper-parameters}
We trained with the RmsProp optimizer \cite{tieleman2012lecture} with an initial learning rate of $10^{-3}$ and dropout after each convolutional layer. The $\alpha$ parameter is set to $\alpha{=}1000$ for matching (solving for X) and to $\alpha{=}100$ for gradient computation.


\section{Experimental results}
\label{sec:results}
\subsection{Precision of the object localizations}
Similarly to what is described in \cite{moysset2016points}, we observed some instability in the position of the predicted objects when trying to detect boxes. Our intuition is that precisely detecting objects which ends outside of the convolutional receptive field of the outputs is difficult. 

Characters may have a size of 1 or 2 mm in standard printed pages, corresponding to 0.005 and 0.01 as a proportion of the page width. Interlines may have similar sizes. Therefore, it is important that the position prediction is close enough in order not to harm the text recognition process.

The method described in \cite{moysset2016points} was dealing with this problem by detecting separately the bottom-left and top-right points and posteriorly pairing them. We observed that the precision was not harmed by the detection of triplets of coordinates (left, top, bottom). 

In Table \ref{tab:precisenessPoint}, we show the F-measure of the detection of left-bottom points for several acceptance zone sizes. The results emphasize that detecting full text boxes reduces precision. Meanwhile, the precision of bottom-left point prediction is equivalent when the network is trained to detect triplets and not points.

Table \ref{tab:precisenessTriplet} shows the same experiment with a 3D acceptance zone defined on the triplet positions, showing the same improved results for the triplet detection for small acceptance zones. 



\begin{table}
\begin{center}
\caption{Comparison of the F-Measure scores for the detection of bottom-left points with respect to the acceptance zone size for networks trained to detect points, triplets or boxes. Acceptance zones are given as proportion of page width.}
\label{tab:precisenessPoint}
\begin{tabular}{lccccc}
Network & 0.003 & 0.01  & 0.03 &  0.1  \\ % & 0.001
\arrayrulecolor{cwblue1} \toprule
Box network (\cite{moysset2016learning}, $K{=}4)$ & 6.8\% & 45.0\% & 82.8\% & 89.9\% \\ % & 0.72\% 
Point network (\cite{moysset2016points}, $K{=}2$) & 10.7\% & 57.4\% & 85.7\% & 91.7\% \\ % & 1.24\% 
Triplet network (Ours, $K{=}3$) & 11.2\% & 58.4\% & 87.0\% & 92.6\% \\ % & 1.33\% 
\end{tabular}
\end{center}
\end{table}

\begin{table}
\begin{center}
\caption{Comparison of the F-Measure scores for the detection of left-side triplets with respect to the acceptance zone size for networks trained to detect triplets or boxes. Acceptance zones are given as proportion of page width.}
\label{tab:precisenessTriplet}
\begin{tabular}{lccccc}
Network & 0.003 & 0.01  & 0.03 &  0.1  \\ % & 0.001 
\arrayrulecolor{cwblue1} \toprule
Box network (\cite{moysset2016learning}, $K{=}4$)  & 3.4\% & 24.6\% & 71.4\% & 89.7\% \\ % & 0.45\% 
Triplet network (Ours, $K{=}3$) & 4.2\% & 47.2\% & 84.8\% & 92.4\% \\ % & 0.15\% 
\end{tabular}
\end{center}
\end{table}

\subsection{Detection of the line end with the text recognizer}
%The drawback of the triplet based method compared to the box method is that the coordinate corresponding to the right of the text line bounding box is not predicted. In order to deal with this limitation, the text recognizer is in charge of learning when the line ends. 
In Table \ref{tab:reco} we compared two text line recognizers trained respectively on the reference text line images and on text line images defined only by the left sides coordinates of the text line and extended toward the right end of the page. These two recognizers are evaluated in both cases with the WER metric.

While the network trained on reference boxes is obviously not working well on extended test images, we see that the network trained on extended lines works on both tasks nearly as well as the network trained on reference boxes.This confirms that we can rely on the text recognizer to ignore the part of the line that does not belong to the text line.

\begin{table}
\begin{center}
\caption{Text recognition Word Error Rates (WER) for networks trained/evaluated on reference boxes or box defined only by their left sides. }
\label{tab:reco}
\begin{tabular}{c|cc}
 & Evaluated on & Evaluated with \\
 & reference boxes & left-sides only \\
\arrayrulecolor{cwblue1} \toprule
Trained on reference boxes & 9.0\% & 46.7\% \\
Trained with left-sides only & 10.6\% & 9.8\% \\
\end{tabular}
\end{center}
\end{table}

\subsection{Full page text recognition}
Finally, we compared our method with baselines and concurrent approaches for  full page recognition. The evaluation was carried out using the BOW metric and is shown on Table \ref{tab:fullPage}.
We show that the proposed methods yield good results on both the French and English subsets, consistently overpassing the document analysis baselines based on image processing, the object localisation baseline and the concurrent box detection and paired point detection systems.
Some illustrations of the left-side triplets detection alongside with the final full-page text recognition are given in Figure 3 and emphasize the ability of the system to give good results on various types of documents.

\begin{table}
\begin{center}
\caption{Comparison of full-page recognition systems with the Bag of Words (BOW) metric on the French and English documents of the Maurdor dataset. }
\label{tab:fullPage}
\begin{tabular}{c|cc}
System & French dataset & English dataset \\
\arrayrulecolor{cwblue1} \toprule
Shi et al. \cite{Shi2009a} & 48.6\% & 30.4\% \\
Nicolaou et al. \cite{Nicolaou2009} & 65.3 \% & 50.0 \% \\
\arrayrulecolor{cwblue1} \toprule
Erhan et al. & 65.3 \% & 50.0 \% \\
Multibox \cite{erhan2014scalable} & 27.2\% & 14.8\% \\
Multibox \cite{erhan2014scalable} (optimized) & 32.4\% & 36.2\% \\
\arrayrulecolor{cwblue1} \toprule
Box network \cite{moysset2016learning}  & 71.2\% & 71.1\% \\
Points network \cite{moysset2016points} & 71.7\% & 72.3\% \\
\arrayrulecolor{cwblue1} \toprule
Triplet network (proposed) & 79.9\% & 79.1\% \\
\end{tabular}
\end{center}
\end{table}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}




% \begin{figure*}[p]
%   \label{fig:IlusResultsFullPage} 
%   \caption{Illustration of full-page recognition results.}
%   \begin{minipage}[b]{0.49\linewidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figs/SDLTriplets_46_Recroped.pdf}
%     \label{fig:imgIlus1}
%     %\caption{Sketch of the Convolutional Recurrent Neural Network that locally predicts the object positions (we do not show the correct numbers of layers and units).}
%     \begin{small}
%     \begin{itemize}
%       \item Invitation pour mon∞
%       \item la Famille Rochefleur :∞
%       \item Nous avons le plaisir de vous inviter \`{a} l'ouverture∞
%       \item  exceptionnelle du salon du mariage qui se tiendra \`{a} chargey∞
%       \item le verte-end du 6 et 7 Octobre 2007 de 20h \`{a} 19h.∞
%       \item Vous aurez ainsi le plaisir d'assister \`{a} un∞
%       \item d\'{e}fil\'{e} de robes de mari\'{e}es, toutes plus oublimes les unes.∞
%       \item que les autres ; de rencontrer des animateurs de∞
%       \item sin\'{e}es, des d\'{e}coracteurs, des joailliers, des traiteuns et :∞
%       \item bien plus encore !∞
%       \item Gr\^{a}ce \`{a} cette invitation vous pourrez b\'{e}n\'{e}ficier de :∞
%       \item de prenmations exceptionnelles et vous pourrez∞
%       \item \'{e}galement profiter d'un d\'{e}licieux buffet, mis \`{a} disposition!∞
%       \item dans le sait bout d'\'{e}galer vos papilles!∞
%       \item Une Tombola sera \'{e}galement organis\'{e}e vous donnant.∞
%       \item ainsi l'opportunit\'{e} de gagner de nombreux cadeaux.∞
%       \item En plus d'un s\'{e}jour pour deux personnes, tout frais∞
%       \item pass\'{e}s \`{a} l'Ile haunit\'{e}.∞
%       \item En esp\'{e}rant votre pr\'{e}sence au salon, je vous∞
%       \item demanderai de remplir le talon r\'{e}ponse ci-joint dans∞
%       \item les meilleurs d\'{e}lais !∞
%       \item Cordialement,∞
%       \item l'organisatrice de∞
%       \item LEU******∞
%       \item PREVOST Myriam∞
%       \item *******∞
%     \end{itemize}
%     \end{small}
%   \end{minipage}
%   \begin{minipage}[b]{0.49\linewidth}
%     \centering
%     \includegraphics[width=0.7\textwidth]{Figs/SDLTriplets_UK_51_Recroped.pdf}
%     %\caption{Sketch of the Convolutional Recurrent Neural Network that locally predicts the object positions (we do not show the correct numbers of layers and units).}
%     \label{fig:imgIlus3}

%     \begin{small}
%     \begin{itemize}
%       \item DY∞
%       \item ****4481991991∞
%       \item An opportunity to buy food and local∞
%       \item products direct from local farmers,∞
%       \item 1cts direct from local farmers,∞
%       \item Only products which have been produced, grown,∞
%       \item raised, baked or caught locally are at the markets.∞
%       \item formally these are from within 30 miles of the market∞
%       \item (unless there is no business supplying a particular∞
%       \item product within 30 miles).∞
%       \item Usually the producer's family are stallholders. They are∞
%       \item pleased to answer questions about their products.∞
%       \item All staall holders must comply with relevant∞
%       \item food safety and hygiene regulations.∞
%       \item No genetically modified organisms can be∞
%       \item knowingly sold. Many staall holders will be registered∞
%       \item organic producers.∞
%       \item To Stockport∞
%       \item Dual∞
%       \item Carriageway∞
%       \item to BUXTON∞
%       \item A6∞
%       \item supermarket∞
%       \item Mailway∞
%       \item ∞
%       \item To Disien-∞
%       \item Whalay∞
%       \item Bridge∞
%       \item To Burton∞
%       \item A5004∞
%       \item UNITTING∞
%       \item CHURCH∞
%     \end{itemize}
%     \end{small}
%   \end{minipage}
%     \begin{minipage}[b]{0.49\linewidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figs/SDLTriplets_UK_142_Recroped.pdf}
%     %\caption{Sketch of the Convolutional Recurrent Neural Network that locally predicts the object positions (we do not show the correct numbers of layers and units).}
%     \label{fig:imgIlus2}
%     \begin{small}
%     \begin{itemize}
%       \item tonage∞
%       \item Room∞
%       \item offi***∞
%       \item Storage∞
%       \item Room∞
%       \item Faithroom∞
%       \item Merrill∞
%       \item Mainstake∞
%       \item Saloon∞
%     \end{itemize}
%     \end{small}
%   \end{minipage}
%   \begin{minipage}[b]{0.49\linewidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figs/SDLTriplets_343_Recroped.pdf}
%     %\caption{Sketch of the Convolutional Recurrent Neural Network that locally predicts the object positions (we do not show the correct numbers of layers and units).}
%     \label{fig:imgIlus4}
%     \begin{small}
%     \begin{itemize}
%       \item CR\'{e}publique Fran\c{c}aise∞
%       \item 'affirmationles : l'orgrientture∞
%       \item \'{e}ditionsuis d\'{e}livr\'{e} la Verdure∞
%       \item  commune \'{e}tant l'une du \`{a} cet entam\'{e}ravis fran\c{c}aises ayant∞
%       \item courir le fils par la croissance nuit∞
%       \item 1000∞
%       \item Cela a tout verandacarton de prie de l'orgrieurture Biologique :∞
%       \item 5 3 3501 site et dont le chiffre d'affaire a augment\'{e} de plus de H4 \% pendant∞
%       \item Le 2 nuit caireprises titrelistes sont les soci\'{e}t\'{e}s fran\c{c}aises Gulfprixd'inter comptait∞
%       \item en deux ans et leur nombre de salari\'{e}s a plus visiter la libre en moyenne tripl\'{e}∞
%       \item Motril fait Fraternit\'{e}∞
%       \item Lieu-fleur∞
%     \end{itemize}
%     \end{small}
%   \end{minipage}
% \end{figure*}


\section{Conclusion}
We described a full page recognition system for heterogeneous unconstrained documents that is able to detect and recognize text in different languages. The use of a neural network localisation process helps to be robust to the intra-dataset variations. In order to simplify the process and to gain both in precision and in preciseness, we focus on predicting the starting point (left) of the text line bounding boxes and leave the prediction of the end point (right)  to a 2D-LSTM based text recognizer. We report excellent results on the Maurdor dataset and show that our method outperform both image-based and concurrent learning-based methods.

\clearpage

\begin{figure*}[th!] \centering
  \label{fig:IlusResultsFullPage} 
  \caption{Illustration of full-page recognition results.}
  \subfloat[Input+Localization results]{ 
    \includegraphics[width=7cm]{SDLTriplets_46_Recroped2.pdf}
  }
  \subfloat[Input+Localization results]{
    \includegraphics[width=5cm]{SDLTriplets_UK_51_Recroped2.pdf}
  }
  \\
  \subfloat[Recognized text]{
   \label{fig:imgIlus1}
  \begin{minipage}[b]{8cm}
    \begin{small}
    \begin{itemize}
      \item Invitation pour mon∞
      \item la Famille Rochefleur :∞
      \item Nous avons le plaisir de vous inviter \`{a} l'ouverture∞
      \item  exceptionnelle du salon du mariage qui se tiendra \`{a} chargey∞
      \item le verte-end du 6 et 7 Octobre 2007 de 20h \`{a} 19h.∞
      \item Vous aurez ainsi le plaisir d'assister \`{a} un∞
      \item d\'{e}fil\'{e} de robes de mari\'{e}es, toutes plus oublimes les unes.∞
      \item que les autres ; de rencontrer des animateurs de∞
      \item sin\'{e}es, des d\'{e}coracteurs, des joailliers, des traiteuns et :∞
      \item bien plus encore !∞
      \item Gr\^{a}ce \`{a} cette invitation vous pourrez b\'{e}n\'{e}ficier de :∞
      \item de prenmations exceptionnelles et vous pourrez∞
      \item \'{e}galement profiter d'un d\'{e}licieux buffet, mis \`{a} disposition!∞
      \item dans le sait bout d'\'{e}galer vos papilles!∞
      \item Une Tombola sera \'{e}galement organis\'{e}e vous donnant.∞
      \item ainsi l'opportunit\'{e} de gagner de nombreux cadeaux.∞
      \item En plus d'un s\'{e}jour pour deux personnes, tout frais∞
      \item pass\'{e}s \`{a} l'Ile haunit\'{e}.∞
      \item En esp\'{e}rant votre pr\'{e}sence au salon, je vous∞
      \item demanderai de remplir le talon r\'{e}ponse ci-joint dans∞
      \item les meilleurs d\'{e}lais !∞
      \item Cordialement,∞
      \item l'organisatrice de∞
      \item LEU******∞
      \item PREVOST Myriam∞
      \item *******∞
    \end{itemize}
    \end{small}
  \end{minipage}
  }
  \subfloat[Recognized text]{   
  \label{fig:imgIlus3}
  \begin{minipage}[b]{8cm}
    \centering
    \begin{small}
    \begin{itemize}
      \item DY∞
      \item ****4481991991∞
      \item An opportunity to buy food and local∞
      \item products direct from local farmers,∞
      \item 1cts direct from local farmers,∞
      \item Only products which have been produced, grown,∞
      \item raised, baked or caught locally are at the markets.∞
      \item formally these are from within 30 miles of the market∞
      \item (unless there is no business supplying a particular∞
      \item product within 30 miles).∞
      \item Usually the producer's family are stallholders. They are∞
      \item pleased to answer questions about their products.∞
      \item All staall holders must comply with relevant∞
      \item food safety and hygiene regulations.∞
      \item No genetically modified organisms can be∞
      \item knowingly sold. Many staall holders will be registered∞
      \item organic producers.∞
      \item To Stockport∞
      \item Dual∞
      \item Carriageway∞
      \item to BUXTON∞
      \item A6∞
      \item supermarket∞
      \item Mailway∞
      \item ∞
      \item To Disien-∞
      \item Whalay∞
      \item Bridge∞
      \item To Burton∞
      \item A5004∞
      \item UNITTING∞
      \item CHURCH∞
    \end{itemize}
    \end{small}
  \end{minipage}
  }
\end{figure*}

\clearpage

\begin{figure}[th!]
  \centering{\footnotesize{(e) Input+Localization results}}
  \\
  \includegraphics[width=6cm]{SDLTriplets_UK_142_Recroped2.pdf}
  \\
   \centering{\footnotesize{(f) Recognized text}}
   \label{fig:imgIlus1}
    \begin{small}
    \begin{itemize}
       \item tonage∞
       \item Room∞
       \item offi***∞
       \item Storage∞
       \item Room∞
       \item Faithroom∞
       \item Merrill∞
       \item Mainstake∞
       \item Saloon∞
    \end{itemize}
    \end{small}
\end{figure}

% conference papers do not normally have an appendix


% use section* for acknowledgment
%\section*{Acknowledgment}




% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\bibliographystyle{splncs03}
\bibliography{tripletNN}

% that's all folks
\end{document}


