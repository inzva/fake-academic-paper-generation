\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools,nccmath}
\usepackage{physics}
\usepackage{multirow}
\usepackage{subfig}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{5688} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{A Psychovisual Analysis on Deep CNN Features for Perceptual Metrics and A Novel Psychovisual Loss}

\author{Taimoor Tariq and Munchurl Kim\\
Korea Advanced Institute of Science and Technology (KAIST)\\
{\tt\small $\{$taimoor.tariq, mkimee$\}$@kaist.ac.kr}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\\
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{figure*}[t!]
\includegraphics[width=\textwidth]{block_dia.jpg}
\caption{Experimental Setup. The network is stimulated by gratings of varying spatial frequency. The responses of different feature maps are recorded as activation vs spatial frequency curves.}
\label{fig:Spike_Sorting}
\end{figure*}

\begin{abstract}
   The efficacy of Deep Convolutional Neural Network (CNN) features as perceptual quality features has been demonstrated by researchers. Nevertheless, any thorough analysis in the context of human visual perception on 'why deep CNN features perform well as perceptual features?', 'Which layers are better?', 'Which feature maps are better?' and most importantly, 'Why are some better?' has not been studied. In this paper, we address these issues and provide an analysis for deep CNN features in terms of Human Visual System (HVS) characteristics. We characterize the frequency tuning of feature maps in a trained deep CNN (e.g., VGG-16) by applying grating stimuli of different spatial frequencies as input. We observe that feature maps behave as spatial frequency-selective filters whose characteristics vary with depth. We analyze the frequency sensitivity of deep features in relation to the human contrast sensitivity function and design a novel Visual Frequency Sensitivity Score (VFSS) to explain and quantify how good different deep features are as perceptual quality features. Based on our analysis, we propose a weighting mechanism to discriminate between feature maps on the basis of their perceptual properties and use this weighting to improve the VGG perceptual loss. The results will demonstrate that the proposed psychovisual loss improves the perception-distortion trade-off when used for CNN based image restoration.
\end{abstract}
%%%%%%%%% BODY TEXT
\section{Introduction}
    Image quality assessment is divided into two major subfields: (i) No-reference Image quality assessment (NR-IQA) \cite{3}; and (ii) Full-reference Image quality assessment (FR-IQA) \cite{4}. NR-IQA refers to the problem of quantifying the quality of an image without any reference for comparison. FR-IQA, on the other hand, is the problem of assessing the quality of an image relative to a given reference image. In this paper, we focus on FR-IQA. The PSNR (Peak Signal to Noise Ratio) and SSIM (Structural Similarity Index) \cite{6}, being fair metrics of distortion between two images, are not a satisfactory metrics to measure perceptive visual quality. The difference and trade-off between distortion and quality has been well explained in \cite{5}. The human visual perception process is fairly complex and still not understood completely. The problem of FR-IQA, on an extremely low level, could be expressed as how differently two images activate different regions in our visual cortex. Considering the highly non-linear nature of CNNs and their arguable similarity to the human visual cortex, it is only natural to assume that deep features perform very well as FR perceptual metrics.

    The perceptual loss proposed by Johnson et al. \cite{7} was one of the first to demonstrate how effective the deep CNN features can be as perceptual metrics, especially when used in loss functions for image restoration. After this proposal, the perceptual loss began to be popularly adopted in many image restoration problems such as super-resolution, style transfer, denoising etc. \cite{9},\cite{10},\cite{11}. However even now, little is understood about the efficacy of the deep feature manifold as a perceptual metric, especially in the context of human visual perception. There are important questions such as 'which feature maps are better perceptual features?', 'which layers are better?', 'which and most importantly, 'why some are better than others?'. Zhang et al. \cite{12} and Blau et al. \cite{5} further demonstrated how effective the deep features really are, but a deeper analysis addressing the raised issues is still missing. This paper addresses these important questions in the context of human visual perception.

   The spatial frequency theory \cite{13},\cite{14} in neuro-science states that the neurons in the human visual cortex are tuned to specific spatial frequencies or bands of frequencies. This is commonly explained by the contrast sensitivity function of HVS which has different sensitivities to different spatial frequencies of stimuli. An experimental technique, that has been used by neuro-scientists for decades, is used to verify the spatial frequency theory and to find the contrast sensitivity function of HVS \cite{15}. The technique is to stimulate the human visual cortex by showing observers different gratings of different characteristics such as spatial frequency, eccentricity and orientation. The responses to the stimuli are recorded in the form of functional magnetic resonance imaging (fMRI) or electroencephalography (EEG) potentials. This technique has been instrumental to understand the functional characteristics of the human brain and to verify proposed theories in neuro-science \cite{16},\cite{17}. An interesting question arises: Considering the similarities between CNNs and the visual cortex, (i) could the spatial frequency theory be applied to neural networks as well? (ii) are different feature maps tuned to different spatial frequencies? and (iii) if yes, what implications can this have?

    In order to explore these issues, we stimulate the VGG-16 \cite{30} neural network with concentric sinusoidal gratings of different spatial frequencies, and record the responses of feature maps in the form of mean activations of feature maps versus the spatial frequency. We establish a hypothesis that feature maps that are more sensitive to  spatial frequencies that the visual cortex is most sensitive to, are better perceptual quality features. We empirically verify our hypothesis by proposing a metric called Visual Frequency Sensitivity Score (VFSS) and utilizing it on various experiments.

   To the best of our knowledge, this is the first analysis of its type. Considering the highly non-linear nature of neural networks, it is unlikely that a tractable mathematical analysis of frequency response such as DFT and DCT could be applied. The grating based methods might be a good alternative to quantify the frequency responses of neural network feature maps.

The main contributions of our work are summarized as follows:
\begin{enumerate}
\item Taking inspiration from neuro-scientific experiments, we devise a novel technique to quantify the spatial frequency tuning characteristics of deep CNN features.
\item We present the first analysis and quantification to determine which feature maps are better perceptual quality features in the context of frequency sensitivity in human visual perception.
\item We propose a Visual Feature Sensitivity Score (VFSS), that can be used in feature selection and improvement of techniques that utilize deep features as full-reference perceptual metrics.
\item We apply our analysis to propose a weighted improvement of the VGG perceptual loss which we call the psychovisual loss. The proposed loss results in a better perception-distortion trade-off compared to the perceptual loss when used for image restoration.
\end{enumerate}
\section{A Psychovisual Approach}

\subsection{Spatial Frequency Theory}
The spatial theory frequency theorizes that the visual cortex operates on a code of spatial frequency, and specific regions or neurons in the visual cortex are tuned to specific spatial frequencies \cite{14}. The theory is contrary to earlier theories of Hubel and Wisel which used straight edges and lines to characterize perception in the visual cortex \cite{18}. The theory is supported by psycho-visual experiments which employ the Fourier theory. According to the Fourier theory, a light distribution in a retinal image can be expressed as a linear combination of basis harmonic components. Grating stimuli have therefore been used in psycho-visual experiments to study the characteristics of the visual cortex.

   Experiments such as the ones described in \cite{14}, \cite{17} and \cite{19} show that the neurons in various regions of the visual cortex behave as spatial frequency filters. This behavior gives a great insight into the function of different regions of the cortex, supporting the claim that a major function of the primary visual cortex is to split images into harmonic components for further processing.
\subsection{Methods}
Our experimental method is inspired by the grating stimulus experiments used by neuro-scientists to study characteristics of the visual cortex. We generate concentric sinusoidal gratings of a fixed contrast and varying spatial frequencies (cycles per degree), use them to stimulate a trained VGG-16 \cite{30} network and record the responses of the feature maps in the form of mean activation versus spatial frequency. Fig. 1 illustrates the overall scheme of measuring the spatial frequency responses of feature maps in various convolution layers of the trained VGG-16 network. The reason we are using a concentric pattern is to eliminate the factor of orientation sensitivity from our analysis. Some concentric grating stimulus patterns are shown as input to the trained VGG-16 network in Fig. 1.

    Some prominent response patterns are observed for feature maps in various convolution layers. Fig.2 shows some representative spatial frequency responses of the feature maps in the trained VGG-16 network for concentric stimuli of various spatial frequencies..

\begin{figure}[t!]
\includegraphics[width=0.5\textwidth]{act_figs1.jpg}
\caption{Spatial frequency responses of the feature maps in several internal convolution layers of the trained VGG-16 network. The x-axis in each subfigure indicates the spatial frequency in cycles per degree, and each y-axis is the mean activation of a feature map in a convolution layer.}
\label{fig:Spike_Sorting}
\end{figure}

\section{Deep Features as Perceptual Metrics}
\subsection{Motivation}
The main motivation behind using deep CNN features as perceptual metrics is that: (i) instead of a distance measure between two images being a good FR metric, computing distance after non-linear transformation of images into a high dimensional manifold, might result in a better perceptual quality measure. The high dimensional manifold in this case is the manifold of CNN features. The general form for the perceptual loss \cite{7} is given by Eq. (1)
\begin{equation}
\mathit{l_p}=\frac{1}{M\mathbf{x} W\mathbf{x} H}\sum\limits_{m=1}^M\Vert\Phi^k_m(I\textsubscript{out})-\Phi^k_m(I\textsubscript{GT})\Vert_2^2
\end{equation}
Where '$\Phi^k_m$' is the '$\textit{m}^{th}$' feature map in the '$\textit{k}^{th}$' layer with '$\textit{M}$' number of feature maps with dimensions '$\textit{H$\mathbf{x}$W}$' . This approach and its variants have proven to be remarkably effective as perceptual features in FR-IQA methods \cite{25}, image restoration \cite{10} and style transfer \cite{11} problems.

However, little is known about the internal characteristics of deep features as perceptual features. In the next sections, we will demonstrate a method to discriminate between different feature maps in a layer on the basis of their efficacy as perceptual quality features. Prior knowledge  about the efficacy of different feature maps will allow us to propose a weighted extension of the perceptual loss which proves to be very effective.

\begin{figure}[t!]
\includegraphics[width=0.5\textwidth]{VFSS_exp.png}
\caption{Two different feature maps may have different sensitivities to important visual frequencies.}
\label{fig:Spike_Sorting}
\end{figure}

\subsection{Visual Frequency Sensitivity}
In this section, we will use the results of the grating experiment to introduce the concept of visual frequency sensitivity. Let us consider the FR-IQA problem. Noise or distortions result in alterations in the frequency spectrum of an image. When important visual frequency components are altered, the resulting distortions are more perceptible to the observers. The more sensitive an FR-IQA metric is to these perceptual distortions, the better it performs an objective quality measure. The frequencies where the HVS is highly sensitive are termed as Sensitive Visual Frequencies (SVFs). The CSF is a measure of the visual system's sensitivity over different spatial frequencies.

    Considering the presented analysis on the spatial frequency selective behavior of deep feature maps. Our hypothesis is that the feature maps that are more sensitive to SVFs, can be better perceptual features, which can more appropriately be used for perceptual metrics. Consider Fig.3. Suppose some reference image, and a distorted version of the reference are separately input to a CNN. The distortion will effect important visual frequencies in the image spectrum, and cause the CNN feature maps to activate differently for both images. Hypothetically, a feature map that is more sensitive to sensitive visual frequencies should serve better as an indicator of perceptual differences. In Fig.3, it can be seen that different feature maps may have different sensitivity to different spatial frequencies.

  We model this behavior by proposing a metric we call the Visual Frequency Sensitivity Score (VFSS). The VFSS of a feature map is defined as
\begin{equation}
VFSS(k,m)=\sum\nolimits_{f}CSF(f).\abs\Big{\frac{\partial a_m^k}{\partial f}}
\end{equation}
where '\textit{k}' is the index for the convolution layer, '\textit{m}' is the feature map index in each convolution layer, '\textit{CSF}' is the contrast sensitivity function (CSF), '\textit{a}' is the mean activation of the feature map and '\textit{f}' is the spatial frequency in cycles per degree.
The VFSS quantifies the average sensitivity of a feature map to weighted by the CSF over different spatial frequencies. The feature maps having higher VFSS values should serve more importantly as perceptual features according to our hypothesis, because they can be more sensitive to visually perceivable distortions in input images. Therefore, the VFSS can be used:
\begin{itemize}
 \item To determine which feature maps are better perceptual quality features;
 \item To calculate a layer wise Mean Visual Frequency Sensitivity Score (MVFSS) to determine which layers have more feature maps that are better perceptual quality features;
 \item To acquire prior knowledge about potential efficacy of deep feature maps as perceptual quality features that can benefit applications such as FR-IQA, Image restoration, deep image compression and accelerating SR and IQA CNN's.
\end{itemize}

\subsection{A Novel Psychovisual Loss for Image Restoration}
As hypothetically, the VFSS is a measure of how good a feature map is as a perceptual quality feature, the feature maps weighted by the VFSS should improve perceptual properties of the VGG perceptual loss \cite{7}. Therefore, we propose a psychovisual loss by extending Eq. (1) to  Eq. (3):
\begin{equation}
\mathit{l_{pv}}=\frac{1}{M\mathbf{x} W\mathbf{x} H}\sum\limits_{m=1}^M w_m^k\Vert\Phi_m^k(I\textsubscript{out})-\Phi_m^k(I\textsubscript{GT})\Vert_2^2
\end{equation}
where
\begin{equation}
   w_m^k=\frac{VFSS(k,m)}{{\sum\limits_{t=1}^M VFSS(k,t)}}
\end{equation}
It should be noted that $w^k_m$ is defined as a normalized VFSS. The psychoviusal loss in Eq. (3) indicates the distance between a ground-truth image and a restored one in the feature space projected by the trained network (e.g., VGG-19) where the projections are weighted by the normalized VFSS in Eq. (4). The experimental results will demonstrate that the psychovisual loss in Eq. (3) helps achieve a better perception-distortion trade-off compared to the perceptual loss when used for image restoration.
\section{Experiments}
\subsection{Overview}
In order to verify the effectiveness and applicability of the proposed approach based on our hypothesis, we perform extensive experiments. For this, we first verify our hypothesis by performing objective quality assessment (OQA) experiments based on our proposed methods and comparing them with the results of subjective quality assessments (SQA). This experiment will indicate how well the Objective metrics are aligned with human judgment of quality. Secondly, we perform an image restoration experiment (super-resolution) with our proposed psychovisual loss to demonstrate that we achieve a better perception-distortion trade-off.

The first experiment includes two OQA experiments in comparison with the results of human SQA. For this, we use the image quality dataset, called LIVE dataset, with subjective quality scores from the LIVE Lab \cite{4}. A more detailed description about of the dataset can be found in \cite{4}. The Gaussian blur distorted images in the LIVE dataset have been used in our experiments. The first OQA experiment is to verify our hypothesis by comparing the performances of Eq. (1) and Eq. (3) with respect to the results of SQA.

In order to further reinforce the validity of our hypothesis, we design another OQA experiment. We take two smaller subsets from the set of feature maps in layers of a pretrained VGG-16. The first subset consists of 15\% of the feature maps with the highest VFSS scores in a layer. The second subset consists of 15\% of the feature maps with the lowest non-zero VFSS scores in a layer. Incorporating the subsets into Eq.(3), for the first subset with the Top-15\% VFSS feature maps, we use
\begin{equation}
  w_m^k=\begin{cases}
    1, & \text{if m $\in Top-15\%$}.\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation}
For the second subset with the Bottom-15\% VFSS feature maps, we use Eq.(3) with
\begin{equation}
  w_m^k=\begin{cases}
    1, & \text{if m $\in Bottom-15\%$}.\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation}
If our hypothesis is correct, Eq.(3) should serve as a much better objective metric with weights in Eq. (5) compared to weights in Eq. (6).

For an image restoration experiment, we compare our proposed psychovisual loss with the VGG perceptual loss \cite{7} in the context of super-resolution performance. We have used the VDSR \cite{26} network for super-resolution (SR) of x4 upscaling with the DIV2K dataset \cite{27} for our experiments. We demonstrate that training with the psychovisual loss results in a better perception-distortion trade-off compared to training with the classical VGG perceptual loss \cite{7}.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}

\begin{table}[]
\centering
\caption{Our proposed Psychovisual loss is more correlated with human subjective assessment of perceptual quality compared to the perceptual loss.}
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Layer}                                                                        & \textbf{Loss} & \textbf{RMSE} & \textbf{LCC} & \textbf{SROCC} \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}ReLU1\_1\\     (64)\end{tabular}}} & \textbf{$l_{p}$}             & 11.4039       & 0.6989       & 0.7144         \\ \cline{2-5}
                                                                                      & \textbf{$l_{pv}$}            & 11.3369       & 0.7032       & 0.7164         \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}ReLU2\_2\\    (128)\end{tabular}}} & \textbf{$l_{p}$}             & 10.3392       & 0.7612       & 0.7766         \\ \cline{2-5}
                                                                                      & \textbf{$l_{pv}$}            & 9.7808        & 0.7897       & 0.8112         \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}ReLU3\_2\\    (256)\end{tabular}}} & \textbf{$l_{p}$}             & 9.4553        & 0.8052       & 0.8288         \\ \cline{2-5}
                                                                                      & \textbf{$l_{pv}$}             & 9.3989        & 0.8078       & 0.8305         \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}ReLU4\_2\\    (512)\end{tabular}}} & \textbf{$l_{p}$}             & 9.4181        & 0.8069       & 0.8301         \\ \cline{2-5}
                                                                                      & \textbf{$l_{pv}$}             & 9.1521        & 0.8189       & 0.8404         \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}ReLU5\_2\\    (512)\end{tabular}}} & \textbf{$l_{p}$}             & 10.0056       & 0.7786       & 0.7979         \\ \cline{2-5}
                                                                                      & \textbf{$l_{pv}$}             & 9.9556        & 0.7811       & 0.8005         \\ \hline
\end{tabular}
\end{table}

\begin{table}[t!]
\caption{Our VFSS quantification of how good different deep features are as perceptual features is correct as the Top 15\% highest VFSS scored features maps perform much better compared to the 15\% lowest VFSS scored feature maps.}

\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Layer}                                                                        & \textbf{Loss} & \textbf{RMSE} & \textbf{LCC} & \textbf{SROCC} \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}ReLU1\_1\\     (64)\end{tabular}}} & Bottom-15\%           & 11.3535       & 0.7021       & 0.7118         \\ \cline{2-5}
                                                                                      & Top-15\%           & 11.3381       & 0.7031       & 0.7179         \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}ReLU2\_2\\    (128)\end{tabular}}} & Bottom-15\%           & 11.3508       & 0.7023       & 0.7159         \\ \cline{2-5}
                                                                                      & Top-15\%           & 9.9606        & 0.7809       & 0.8010         \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}ReLU3\_2\\    (256)\end{tabular}}} & Bottom-15\%           & 10.046        & 0.7765       & 0.7951         \\ \cline{2-5}
                                                                                      & Top-15\%           & 9.7041        & 0.7935       & 0.8195         \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}ReLU3\_3\\    (256)\end{tabular}}} & Bottom-15\%           & 10.3312       & 0.7617       & 0.7852         \\ \cline{2-5}
                                                                                      & Top-15\%           & 9.1932        & 0.8170       & 0.8451         \\ \hline
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}ReLU4\_2\\    (512)\end{tabular}}} & Bottom-15\%           & 10.5636       & 0.7490       & 0.7541         \\ \cline{2-5}
                                                                                      & Top-15\%           & 9.6460        & 0.7692       & 0.8188         \\ \hline
\end{tabular}
\end{table}

\begin{table*}[!hbt]
\caption{The $MVFSS_{Top-15\%}$ of each layer under investigation.}
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{Layer} & \textbf{ReLU1\_1} & \textbf{ReLU2\_2} & \textbf{ReLU3\_2} & \textbf{ReLU3\_3} & \textbf{ReLU4\_2} \\ \hline
\textbf{$MVFSS_{Top-15\%}$} & 0.33              & 28.53             & 198.65            & 270.24            & 175.23            \\ \hline
\end{tabular}
\end{table*}

\subsection{Results and Discussions}
\subsubsection{Objective Quality Experiments}
\begin{table*}[!hbt]
\centering
\caption{MVFSS measures for five different layers of the trained VGG-16 network}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Layer          & \textbf{ReLU1\_1} & \textbf{ReLU2\_2} & \textbf{ReLU3\_2} & \textbf{ReLU4\_2} & \textbf{ReLU5\_2} \\ \hline
\textbf{MVFSS} & 0.068             & 8.8097            & 64.5575           & 35.6725           & 3.8894            \\ \hline
\end{tabular}
\end{table*}


Tables 1 and 2 compare the Root-Mean-Square-Error (RMSE),  LCC (Linear Correlation Coefficient) and SROCC (Spearman Rank Order Correlation Coefficient) performances for the Eq. (1) and Eq. (3) on the Gaussian blur distortion dataset, for different layers of the trained VGG-16 network with the Subjective Quality Scores in the LIVE dataset. The log of the error scores in Eq. (1) and Eq. (3) were fitted with subjective scores using a third degree polynomial. The RMSE, LCC and SROCC were calculated after fitting as explained in \cite{4}. As observed in Tables 1 and 2, the psychovisual loss ($l_{pv}$) performs better as an OQA metric compared to the classical VGG perceptual loss ($l_{p}$) for all layers under investigation. This result demonstrates the efficacy of $l_{pv}$ as an FR Objective Quality Metric and shows that it is more aligned with human perceptual quality judgment. It also reinforces our hypothesis regarding the importance of visual frequency sensitivity in the performance of feature maps as perceptual features.

Whether we can use the VFSS to explain the OQA performance of a convolution layer against other layers is a different problem compared to the cases of the performance comparison among different feature maps within the same layer, which are a much simpler problem. All the feature maps in a layer can be thought as having the same complexity level because they reside in a same transformed manifold. However, two feature maps in different layers are difficult to be compared directly as perceptual features because they reside in different transformed manifolds that are constructed through different degrees of non-linear transforms. That is, when we compare different layers, due to the non-linear nature of neural networks, a variety of complex factors comes into play. So, it is difficult to understand the internal behaviors of neural networks.

In order to study whether the VFSS can be used to explain the efficacy of different layers in comparison to one another, we propose the Mean Visual Frequency Sensitivity Score of different layers (MVFSS) to measure the mean visual frequency sensitivity of each layer which can then be used to determine which layers have effective feature maps for perceptual metrics. The MVFSS of the \textit{k-th} layer is calculated as
\begin{equation}
   MVFSS(k)=\frac{1}{M}\sum\nolimits_{m}VFSS(k,m)
\end{equation}
where \textit{M} is the total number of feature maps in the \textit{k-th} layer.

Table 4 shows the MVFSS values measured for five different layers of the trained VGG-16 network. It is evident that the \textit{ReLU3\_2} layer has the highest MVFSS value followed by the \textit{ReLU4\_2} layer. It is noted that these two layers have significantly higher MVFSS values than the other three layers. Considering our analysis, these two layers should deliver features that perform very well as perceptual metrics. The results in Table 1 show that the \textit{ReLU3\_2} and \textit{ReLU4\_2} layers show the best OQA performances compared to the other three layers. The \textit{ReLU1\_1} layer in Table 4 has the lowest MVFSS value and it also shows the worst OQA performance shown in Table 1 .Furthermore, the MVFSS drops significantly from the \textit{ReLU4\_2} layer to the \textit{ReLU5\_2} layer and such performance drop can also be observed in Table 1. Although the change in MVFSS performance is not completely analyzed layer-to-layer, a definite correlation between the MVFSS and OQA performances over different layers is evident. This experimental evidence supports our hypothesis that feature maps, that are more sensitive to spatial frequencies which the human visual system is most sensitive to, are better for perceptual metrics.

Table. 2 shows results of the OQA experiment on the Gaussian Blur distorted images from the LIVE subjective quality data-set. It can be seen that the Top-15\% VFSS scored feature maps perform much better than the Bottom-15\% VFSS scored feature maps for all layers under investigation. This results shows that that the Top-15\% feature maps deliver features whose discrimination between images is more correlated with human quality judgment as opposed to the Bottom-15\%. These results further reinforce our hypothesis and demonstrate the effectiveness of our approach for the selection of deep features in applications that utilize them as perceptual metrics.

To investigate which layers have a better concentration of high VFSS scored feature maps and whether this concentration can be correlated to the performance of layers as perceptual metrics, we calculate the Mean VFSS scores for the Top-15\% VFSS scored feature maps in each layer. For example, in the layer \textit{ReLU3\_3} which has 256 feature maps, the $MVFSS_{Top-15\%}$ will the average VFSS for the 39 highest VFSS scored feature maps. Table. 3 shows that the performance of the Top-15\% feature maps in different layers in Table. 2 is very well correlated with the $MVFSS_{Top-15\%}$ of the layers. The \textit{ReLU3\_3} has the highest $MVFSS_{Top-15\%}$ score in Table. 3 and it can be seen to have the best performing Top-15\% feature maps in Table. 2. In terms of the SROCC and LCC, the ordering of performance of each layer in Table. 2 is correlated exactly with the $MVFSS_{Top-15\%}$ ordering of the layers. This is exactly in accordance with our hypothesis that layers with feature maps sensitive to important visual frequencies have better perceptual features.

The previous OQA experiments verify that the visual frequency sensitivity of feature maps is an important characteristic that determines the efficacy of deep features as perceptual quality features. To our best knowledge, this is the first analysis of deep CNN features as perceptual features in the context of human visual perception. Results with additional networks and distortion types are provided in the supplementary material.

\begin{figure}[t!]
\includegraphics[width=0.4\textwidth]{pd_tradeoff_fig.jpg}
\caption{The psychovisual loss (Eq. 9) improves the perception-distortion trade-off compared to the perceptual loss (Eq. 8) for x4 SR with the VDSR on the DIV2K dataset.}
\end{figure}

\subsubsection{Image Restoration (Super-Resolution) Experiments}
We have used the VDSR \cite{26} network and the DIV2K dataset \cite{27} for a x4 Super-Resolution (SR) problem in our experiments. We select the \textit{ReLU4\_2} layer of the VGG-16 for the perceptual loss as it is commonly used. In general for SR, as explained in \cite{7}, the perceptual loss \cite{7} is used in combination with pixel-wise losses such as \textit{l}\textsubscript{1} or \textit{l}\textsubscript{2} loss. Eq. (8) gives the general expression for the perceptual loss.
\begin{equation}
   L_{p} = \alpha\cdot\mathit{l}\textsubscript{1} + (1-\alpha)\cdot\mathit{l}\textsubscript{p}
\end{equation}
Using the VFSS-based weighting in Eq. (4), we extend Eq. (1) to our proposed psychovisual loss in Eq. (9):
\begin{equation}
  L_{pv} = \beta\cdot\mathit{l}\textsubscript{1} + c\cdot(1-\beta)\cdot\mathit{l}\textsubscript{pv}
\end{equation}
The coefficient 'c' has been introduced so that the proposed psychovisual loss has a comparable magnitude the to perceptual loss i.e $l_{p}\approx c\cdot l_{pv}$. This is helpful for implementation and comparison and does not effect the performance. We have empirically chosen $c=509$

It should be reminded that the traditional metrics such as PSNR and SSIM are known to be not highly correlated with perceptual quality and the distortion measured by PSNR and SSIM for a degraded image indicates the net deviation from its reference image. Many different images of different perceptual qualities may have a same distortion for a given reference. This is why objective distortion measures such as PSNR and SSIM do not necessarily account for perceptual quality. To quantify no-reference perceptual quality, a combination of the metrics such as NIQE\cite{28} and NRQM\cite{31} has been used as a perceptual indicator (PI) in recent works \cite{29}. The work of Blau et.al \cite{5} demonstrates that distortion and perceptual quality are in a trade-off relation and this trade-off is the correct measure for quantifying the efficacy of image restoration algorithms as explained in \cite{29}. Fig. 4 demonstrates the our proposed psychovisual loss in Eq.(9) provides a better perception-distortion trade-off compared to the perceptual loss in Eq.(8) when parameters are varied.

Fig.5 shows the x4 SR reconstructions by two VDSR networks: one was trained by the loss function in Eq. (8) and the other by the loss function in Eq. (9). Consider Fig.5-(b) and Fig.5-(d) in which the restored images have roughly the same distortion (SSIM)  with the ground truth in Fig.5-(a) but the psychovisual loss improves the perceptual quality (lower PI). Furthermore, even at significantly lower distortion, the perceptual loss based restored image in Fig.5-(c) fails to achieve as good perceptual quality as the psychovisual loss based restored image in  Fig.5-(b). Similar results can be deduced from Fig.5-(h) to -(k). This clearly demonstrates that the proposed psychovisual loss in Eq. (9) has the ability to deliver a better perception-distortion trade-off compared to the perceptual loss in Eq.(8).

\begin{figure*}[hbt!]
\centering
\subfloat[Ground Truth]{\includegraphics[width=0.25\textwidth]{gtn_crop1.png}\label{fig:f7}}
\hfill
\subfloat[][\textit{L}\textsubscript{pv}\centering($\beta$=0.88)\linebreak(SSIM:0.938,
PI:4.5984)]{\includegraphics[width=0.25\textwidth]{wtn_crop1.png}}
\hfill
\subfloat[][{\textit{L}\textsubscript{p}($\alpha$=0.6)\linebreak(SSIM:0.932, PI:4.6480)}]{\includegraphics[width=0.25\textwidth]{uwt_point6l1_crop1.png}\label{fig:f7}}
\hfill
\subfloat[][\textit{L}\textsubscript{p}($\alpha$=0.9)\linebreak (SSIM:0.939, PI:4.8849)]{\includegraphics[width=0.25\textwidth]{uwt_point9l1_crop1.png}\label{fig:f6}}
\hfill
\subfloat[][\textit{L}\textsubscript{p}($\alpha$=0.6)]{\includegraphics[width=0.33\textwidth]{hm_uw_p6l1_crop1.png}\label{fig:f7}}
\hfill
\subfloat[][\textit{L}\textsubscript{pv}($\beta$=0.88)]{\includegraphics[width=0.33\textwidth]{hm_wt_crop1.png}\label{fig:f6}}
\hfill
\subfloat[][\textit{L}\textsubscript{p}($\alpha$=0.9)]{\includegraphics[width=0.33\textwidth]{hm_uw_p9l1_crop1.png}\label{fig:f6}}
\hfill
\subfloat[][Ground Truth]{\includegraphics[width=0.25\textwidth]{gtn_crop2.png}\label{fig:f7}}
\hfill
\subfloat[][\textit{L}\textsubscript{pv}\centering($\beta$=0.88)\linebreak(SSIM:0.869, PI:4.1013) ]{\includegraphics[width=0.25\textwidth]{wtn_crop2.png}\label{fig:f6}}
\hfill
\subfloat[][\textit{L}\textsubscript{p}($\alpha$=0.6)\linebreak(SSIM:0.858, PI:4.1620)]{\includegraphics[width=0.25\textwidth]{uwt_point6l1_crop2.png}\label{fig:f7}}
\hfill
\subfloat[][\textit{L}\textsubscript{p}($\alpha$=0.9)\linebreak(SSIM:0.868, PI:4.4024)]{\includegraphics[width=0.25\textwidth]{uwt_point9l1_crop2.png}\label{fig:f6}}
\hfill
\subfloat[][\textit{L}\textsubscript{p}($\alpha$=0.6)]{\includegraphics[width=0.33\textwidth]{hm_uw_p6l1_crop2.png}\label{fig:f7}}
\hfill
\subfloat[][\textit{L}\textsubscript{pv}($\beta$=0.88)]{\includegraphics[width=0.33\textwidth]{hm_wt_crop2.png}\label{fig:f6}}
\hfill
\subfloat[][\textit{L}\textsubscript{p}($\alpha$=0.9)]{\includegraphics[width=0.33\textwidth]{hm_uw_p9l1_crop2.png}\label{fig:f6}}
\hfill
\caption{x4 SR experiment results on the VDSR show that the psychovisual loss ($L_{pv}$) in Eq. (9) delivers a better perception-distortion trade-off compared to the perceptual loss ($L_{p}$) in Eq. (8) .}
\end{figure*}

\subsubsection{Potential Applications}
Our proposed approach is the first method to quantify how good a deep feature map is as a perceptual feature. It can be used to select layers and feature maps of Deep CNNs for use in perceptual metrics/losses and other applications. The perceptual loss is extensively used for many applications and we have clearly demonstrated that our proposed psychovisual loss delivers a better perception-distortion trade-off and can be used for image restoration, style transfer and FR-IQA applications. Furthermore, deep CNN features are of great importance to the problem of deep image compression and prior knowledge about the potential efficacy of feature maps may help in better quantization techniques to improve the quality of compressed images. Prior knowledge of perceptual efficacy of deep features may also aid in feature compression or using a small number of feature maps for accelerating CNN performance.

\section{Conclusions}
We characterize the spatial frequency selectivity of deep CNN feature maps using an experimental technique inspired by neuro-science. We propose and verify a hypothesis that the feature maps that show more sensitivity to spatial frequencies which the human visual system is most sensitive to, perform better as perceptual features for perceptual metrics. We quantify this spatial frequency sensitivity of a feature map using a novel metric, called the Visual Frequency Sensitivity Score (VFSS). We demonstrate that the VFSS metric can be used to quantify which feature maps can extract better perceptual features, and which layers have better feature maps. This paper is the first to quantify and explain how good different feature maps are as perceptual quality features, resulting in a novel psychovisual loss that can effectively be used for CNN-based image restoration. In comparison to the popular and widely used VGG perceptual loss, the proposed psychovisual loss applied for a super-resolution problem results in a better perception-distortion trade-off.




%\section*{\centering{\refname}}
\bibliographystyle{IEEEtran}
%\small{\bibliography{Ref.bib}}
%{\bibliography{egbib.bib}}
{% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{3}
K.~B. V~Kamble, ``No-reference image quality assessment algorithms: A survey,''
  \emph{Optik - International Journal for Light and Electron Optics}, vol.
  abs/1501.00092, 2015.

\bibitem{4}
H.~R. Sheikh, M.~F. Sabir, and A.~C. Bovik, ``A statistical evaluation of
  recent full reference image quality assessment algorithms,'' \emph{IEEE
  Transactions on Image Processing}, vol.~15, pp. 3440--3451, 2006.

\bibitem{6}
Z.~Wang, A.~C. Bovik, H.~R. Sheikh, and E.~P. Simoncelli, ``Image quality
  assessment: From error visibility to structural similarity,'' \emph{IEEE
  Transactions on Image Processing}, vol.~13, no.~4, pp. 600--612, 2004.

\bibitem{5}
Y.~Blau and T.~Michaeli, ``The perception-distortion tradeoff,'' \emph{IEEE
  CVPR}, 2018.

\bibitem{7}
J.~Johnson, A.~Alahi, and L.~Fei-Fei, ``Perceptual losses for real-time style
  transfer and super-resolution,'' 2016.

\bibitem{9}
C.~Ledig, L.~Theis, F.~Huszar, J.~Caballero, A.~P. Aitken, A.~Tejani, J.~Totz,
  Z.~Wang, and W.~Shi, ``Photo-realistic single image super-resolution using a
  generative adversarial network,'' \emph{2017 IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR)}, pp. 105--114, 2017.

\bibitem{10}
X.~Wang, K.~Yu, S.~Wu, J.~Gu, Y.~Liu, C.~Dong, C.~C. Loy, Y.~Qiao, and X.~Tang,
  ``Esrgan: Enhanced super-resolution generative adversarial networks,'' 2018.

\bibitem{11}
L.~A. Gatys, A.~S. Ecker, and M.~Bethge, ``Image style transfer using
  convolutional neural networks,'' \emph{2016 IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR)}, pp. 2414--2423, 2016.

\bibitem{12}
R.~Zhang, P.~Isola, A.~A. Efros, E.~Shechtman, and O.~Wang, ``The unreasonable
  effectiveness of deep features as a perceptual metric,'' \emph{IEEE CVPR},
  2018.

\bibitem{13}
J.~J. Kulikowski, S.~Marvcelja, and P.~O. Bishop, ``Theory of spatial position
  and spatial frequency relations in the receptive fields of simple cells in
  the visual cortex,'' \emph{Biological Cybernetics}, vol.~43, pp. 187--198,
  1982.

\bibitem{14}
L.~Maffei and A.~Fiorentini, ``The visual cortex as a spatial frequency
  analyser.'' \emph{Vision research}, vol. 13 7, pp. 1255--67, 1973.

\bibitem{15}
\emph{Emergent Techniques for Assessment of Visual Performance.}\hskip 1em plus
  0.5em minus 0.4em\relax Committee on Vision, National Research Council, 1985.

\bibitem{16}
M.~G. B. W. K. L. S. E.~K. R.~M.~Everson, A. K.~Prashanth, ``Representation of
  spatial frequency and orientation in the visual cortex,'' \emph{Proceedings
  of the National Academy of Sciences}, vol. 95 14, 1998.

\bibitem{17}
N.~P. Issa, C.~Trepel, and M.~P. Stryker, ``Spatial frequency maps in cat
  visual cortex.'' \emph{The Journal of neuroscience : the official journal of
  the Society for Neuroscience}, vol. 20 22, pp. 8504--14, 2000.

\bibitem{30}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' \emph{CoRR}, vol. abs/1409.1556, 2014.

\bibitem{18}
D.~H. Hubel and T.~N. Wiesel, ``Receptive fields, binocular interaction and
  functional architecture in the cat's visual cortex,'' \emph{The Journal of
  Physiology}, vol. 160 1, pp. 106--154.

\bibitem{19}
J.~J. Kulikowski, S.~Marvcelja, and P.~O. Bishop, ``Theory of spatial position
  and spatial frequency relations in the receptive fields of simple cells in
  the visual cortex,'' \emph{Biological Cybernetics}, vol.~43, pp. 187--198,
  1982.

\bibitem{20}
S.~K. O.~I. Kato~D, Baba~M, ``Effects of generalized pooling on binocular
  disparity selectivity of neurons in the early visual cortex,''
  \emph{Philosophical Transactions of the Royal Society B: Biological
  Sciences.}, vol. 371 1697, 2016.

\bibitem{21}
L.~G.~T. Russell L. De~Valois, Duane G.~Albrecht, ``Spatial frequency
  selectivity of cells in macaque visual cortex,'' \emph{Vision Research}.

\bibitem{23}
M.~van Wyk, H.~W{\"a}ssle, and W.~R. Taylor, ``Receptive field properties of
  on- and off-ganglion cells in the mouse retina.'' \emph{Visual neuroscience},
  vol. 26 3, pp. 297--308, 2009.

\bibitem{22}
M.~A.~W. Bhuvanesh~Awasthi, Jason~Friedman, ``Faster, stronger, lateralized:
  Low spatial frequency information supports face processing,''
  \emph{Neuropsychologia}, vol. 49 13, pp. 3583--3590, 2011.

\bibitem{25}
S.~Bosse, D.~Maniry, K.-R. Muller, T.~Wiegand, and W.~Samek, ``Deep neural
  networks for no-reference and full-reference image quality assessment,''
  \emph{IEEE Transactions on Image Processing}, vol.~27, pp. 206--219, 2017.

\bibitem{26}
J.~Kim, J.~K. Lee, and K.~M. Lee, ``Accurate image super-resolution using very
  deep convolutional networks,'' \emph{2016 IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pp. 1646--1654, 2016.

\bibitem{27}
E.~Agustsson and R.~Timofte, ``Ntire 2017 challenge on single image
  super-resolution: Dataset and study,'' \emph{2017 IEEE Conference on Computer
  Vision and Pattern Recognition Workshops (CVPRW)}, pp. 1122--1131, 2017.

\bibitem{28}
A.~Mittal, R.~Soundararajan, and A.~C. Bovik, ``Making a completely blind‚Äù
  image quality analyzer,'' \emph{IEEE Signal Processing Letters}, vol.~20, pp.
  209--212, 2013.

\bibitem{31}
C.~Ma, C.~Yang, X.~Yang, and M.~Yang, ``Learning a no-reference quality metric
  for single-image super-resolution,'' \emph{CoRR}, vol. abs/1612.05890, 2016.

\bibitem{29}
Y.~Blau, R.~Mechrez, R.~Timofte, T.~Michaeli, and L.~Zelnik-Manor, ``2018 pirm
  challenge on perceptual image super-resolution,'' 2018.

\bibitem{32}
``Recommendation itu-r bt.500-11 - methodology for the subjective assessment of
  the quality of television pictures,'' 2002.

\bibitem{33}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ''Deep residual learning for image recognition, '' \emph{2016 IEEE     Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 770--778, 2016.

\bibitem{34}
F.~N. Iandola, M.~W. Moskewicz, K.~Ashraf, S.~Han, W.~J. Dally, and K.~Keutzer, ''Squeezenet:          Alexnet-level accuracy with 50x fewer parameters and
 <0.5mb model size,'' \emph{CoRR}, abs/1602.07360, 2017.

\bibitem{35}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ''Imagenet classification with deep convolutional neural networks, '' \emph{Commun. ACM}, 60:84--90, 2012.

\bibitem{36}
D.~Jayaraman, A.~Mittal, A.~K. Moorthy, and A.~C. Bovik, ''Objective quality assessment of multiply distorted images, '' \emph{2012 Conference Record of the Forty Sixth Asilomar Conference on
  Signals, Systems and Computers (ASILOMAR)}, pages 1693--1697, 2012.

\end{thebibliography}}

\appendix
\section{Supplementary Material}
\section{Implementation Details}
In this section, we present details behind the generation of sinusoidal gratings of different spatial frequencies.

The contrast sensitivity function is expressed on the domain of spatial frequency in cycles per degree (cyc/deg). The cycles per degree express the number of sine cycles captured by the observer per unit degree of observation. Obviously, the distance of viewing and dimensions of the screen play an important part in this measurement.

We essentially generate gratings in the computer simulation in cycles per pixel. Let the display screen being used in the experiment have a height '\textit{h}' inches and resolution '\textit{r}' pixels per inch. The optimal viewing distance in psychovisual experiments should satisfy a function called the PVD \cite{1}. The PVD is a function that expresses the optimal ratio of viewing distance to the height of the display screen. The optimal viewing distance '\textit{d}' for the screen with height '\textit{h}' can be calculated using the PVD.

The transformation between cycles/degree and cycles/pixel is
\begin{equation}
   \frac{cycles}{pixel} = \frac{cycles}{degree} \times \frac{degrees}{pixel}
\end{equation}

Where
\begin{equation}
   \frac{pixels}{degree} = \frac{180}{\pi \times d \times r}
\end{equation}

Therefore,
\begin{equation}
   \frac{cycles}{pixel} = \frac{cycles}{degree} \times \frac{\pi \times d \times r}{180}
\end{equation}

We have tested with a number of different display systems of SD, 2K and 4K resolutions. Considering that the PVD takes the viewing angle into account, the changes in the resultant spatial frequencies of the gratings are small and insignificant. Therefore, it can be concluded that the choice of display system has a negligible effect on the experiment.

\begin{figure}[bt!]

  \subfloat[Top-15\%]{\includegraphics[width=0.25\textwidth]{t15_curve_relu2_2.png}\label{fig:f5}}
  \subfloat[Bottom-15\%]{\includegraphics[width=0.25\textwidth]{b15_curve_relu2_2.png}\label{fig:f7}}
  \hfill
  \caption{Human subjective scores for Gaussian Blur images fitted against metric scores for the \textit{ReLU2\_2} layer in the VGG-16. It can be seen that the high VFSS scores feature maps are better perceptual quality features as a metric based on them is more correlated with human judgment of quality. }
\end{figure}

\section{Additional Experiments and Results}
\subsection{Additional Networks and Distortions}
So far, considering its wide use, we have only investigated the VGG-16 feature maps and demonstrated the efficacy of our approach. In this section, we will extend our approach to additional networks such as;
\begin{itemize}
 \item AlexNet \cite{35}
 \item ResNet-18 \cite{33}
 \item SqueezeNet \cite{34}
\end{itemize}

and a variety of distortions such as ;
\begin{itemize}
 \item{Gaussian Blur}
 \item Fast-Fading distortions.
 \item White Noise.
 \item Multiple Distortions (camera image acquisition process where images are first blurred due to narrow depth of field or other defocus and then corrupted by white Gaussian noise to simulate sensor noise.)
\end{itemize}

The fast-fading and white noise corrupted images and subjective scores were obtained the LIVE Image Quality Assessment Database \cite{4}. The multiple distortion images and subjective scores were obtained from the LIVE Multiply Distorted Image Quality Database \cite{36}.

We demonstrate that our quantification for the efficacy of feature maps as perceptual quality features is correct and our proposed perceptual loss delivers superior performance and is much more correlated with human subjective assessment of quality compared to the traditional perceptual loss. Considering the results in Table. - (1) to - (6), it can be observed that our proposed psychovisual loss (\textbf{$l_{pv}$}) is a superior metric compared to the perceptual loss (\textbf{$l_{p}$}) for a wide variety of networks, their layers and different types of distortions. The superiority is verified by its better correlation with human subjective assessment of perceptual quality and deliverance of better perception-distortion trade-off for the x4 SR experiment we have performed, as shown in Fig. 4 of the main text. Considering the widespread use of the perceptual loss in image restoration, we believe that our proposed psychovisual loss can serve as a much better alternative.

As supplements to Table. 2 in the main text, Table. - (7) and -(8) demonstrate that our proposed quantification about how good different feature maps are as perceptual quality features is correct for other networks and distortions as well. It can clearly be seen that the 15\% of the feature maps with the highest VFSS scores are a much better set of perceptual quality features compared to the 15\% of the feature maps with the lowest non-zero VFSS scores.

\begin{table*}[]
\centering
\caption{Our proposed Psychovisual loss is more correlated with human subjective assessment of perceptual quality compared to the perceptual loss.}
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{5}{|l|}{\textbf{SqueezeNet- Multiple Distortions}}                                                   \\ \hline
\textbf{Layer}                                  & \textbf{Metric} & \textbf{RMSE} & \textbf{LCC} & \textbf{SROCC} \\ \hline
\multirow{2}{*}{\textbf{fire2-RelU\_expand3x3}} & \textbf{$l_{p}$}              & 15.2720       & 0.6132       & 0.5589         \\ \cline{2-5}
                                                & \textbf{$l_{pv}$}             & 14.2447       & 0.6761       & 0.6110         \\ \hline
\multirow{2}{*}{\textbf{fire4-RelU\_expand1x1}} & \textbf{$l_{p}$}              & 14.5749       & 0.6570       & 0.6051         \\ \cline{2-5}
                                                & \textbf{$l_{pv}$}             & 14.1847       & 0.6795       & 0.6292         \\ \hline
\multirow{2}{*}{\textbf{fire6-RelU\_expand3x3}} & \textbf{$l_{p}$}              & 15.5951       & 0.5910       & 0.5482         \\ \cline{2-5}
                                                & \textbf{$l_{pv}$}             & 14.0247       & 0.6873       & 0.6607         \\ \hline
\end{tabular}
\end{table*}

\begin{table*}[]
\centering
\caption{Our proposed Psychovisual loss is more correlated with human subjective assessment of perceptual quality compared to the perceptual loss.}
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{5}{|l|}{\textbf{SqueezeNet - Gaussian Blur}}                                                         \\ \hline
\textbf{Layer}                                  & \textbf{Metric} & \textbf{RMSE} & \textbf{LCC} & \textbf{SROCC} \\ \hline
\multirow{2}{*}{\textbf{fire2-ReLU\_expand3x3}} & \textbf{$l_{p}$}              & 11.0894       & 0.7185       & 0.7375         \\ \cline{2-5}
                                                & \textbf{$l_{pv}$}             & 9.4110        & 0.8072       & 0.8323         \\ \hline
\multirow{2}{*}{\textbf{fire4-ReLU\_expand1x1}} & \textbf{$l_{p}$}              & 10.2650       & 0.7652       & 0.7867         \\ \cline{2-5}
                                                & \textbf{$l_{pv}$}             & 8.9756        & 0.8265       & 0.8525         \\ \hline
\multirow{2}{*}{\textbf{fire8-ReLU\_expand1x1}} & \textbf{$l_{p}$}              & 10.8534       & 0.7326       & 0.7531         \\ \cline{2-5}
                                                & \textbf{$l_{pv}$}             & 10.4643       & 0.7545       & 0.7683         \\ \hline
\end{tabular}
\end{table*}

\begin{table}[]
\caption{Our proposed Psychovisual loss is more correlated with human subjective assessment of perceptual quality compared to the perceptual loss.}
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{5}{|l|}{\textbf{AlexNet - Gaussian Blur}}                                              \\ \hline
\textbf{Layer}                    & \textbf{Metric} & \textbf{RMSE} & \textbf{LCC} & \textbf{SROCC} \\ \hline
\multirow{2}{*}{\textbf{ReLU\_2}} &\textbf{$l_{p}$}              & 9.5549        & 0.8006       & 0.8229         \\ \cline{2-5}
                                  & \textbf{$l_{pv}$}             & 9.2554        & 0.8143       & 0.8611         \\ \hline
\multirow{2}{*}{\textbf{ReLU\_3}} & \textbf{$l_{p}$}              & 6.1683        & 0.9221       & 0.9327         \\ \cline{2-5}
                                  & \textbf{$l_{pv}$}             & 4.4803        & 0.9597       & 0.9588         \\ \hline
\multirow{2}{*}{\textbf{ReLU\_5}} & \textbf{$l_{p}$}              & 8.4921        & 0.8464       & 0.8615         \\ \cline{2-5}
                                  & \textbf{$l_{pv}$}             & 6.8048        & 0.9044       & 0.9078         \\ \hline
\end{tabular}
\end{table}

\begin{table}[]
\caption{Our proposed Psychovisual loss is more correlated with human subjective assessment of perceptual quality compared to the perceptual loss.}
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{5}{|l|}{\textbf{ResNet18 - Gaussian Blur}}                                                 \\ \hline
\textbf{Layer}                        & \textbf{Metric} & \textbf{RMSE} & \textbf{LCC} & \textbf{SROCC} \\ \hline
\multirow{2}{*}{\textbf{Res4a\_Relu}} & \textbf{$l_{p}$}              & 5.6106        & 0.9360       & 0.9349         \\ \cline{2-5}
                                      & \textbf{$l_{pv}$}             & 4.7957        & 0.9537       & 0.9509         \\ \hline
\multirow{2}{*}{\textbf{Res4b\_Relu}} & \textbf{$l_{p}$}              & 5.9010        & 0.9290       & 0.9284         \\ \cline{2-5}
                                      & \textbf{$l_{pv}$}             & 5.3772        & 0.9414       & 0.9370         \\ \hline
\multirow{2}{*}{\textbf{Res5a\_Relu}} & \textbf{$l_{p}$}              & 6.2196        & 0.9208       & 0.9242         \\ \cline{2-5}
                                      & \textbf{$l_{pv}$}            & 5.6946        & 0.9340       & 0.9321         \\ \hline
\end{tabular}
\end{table}


\begin{table}[]
\caption{Our proposed Psychovisual loss is more correlated with human subjective assessment of perceptual quality compared to the perceptual loss.}
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{5}{|l|}{\textbf{AlexNet- Fast Fading Noise}}                                           \\ \hline
\textbf{Layer}                    & \textbf{Metric} & \textbf{RMSE} & \textbf{LCC} & \textbf{SROCC} \\ \hline
\multirow{2}{*}{\textbf{ReLU\_3}} & \textbf{$l_{p}$}              & 4.2471        & 0.9670       & 0.9631         \\ \cline{2-5}
                                  & \textbf{$l_{pv}$}             & 3.6872        & 0.9753       & 0.9710         \\ \hline
\multirow{2}{*}{\textbf{ReLU\_4}} & \textbf{$l_{p}$}              & 4.5008        & 0.9629       & 0.9585         \\ \cline{2-5}
                                  & \textbf{$l_{pv}$}             & 3.9098        & 0.9721       & 0.9667         \\ \hline
\multirow{2}{*}{\textbf{ReLU\_5}} & \textbf{$l_{p}$}              & 5.5131        & 0.9438       & 0.9388         \\ \cline{2-5}
                                  & \textbf{$l_{pv}$}            & 4.9294        & 0.9553       & 0.9511         \\ \hline
\end{tabular}
\end{table}

\begin{table}[]
\caption{Our proposed Psychovisual loss is more correlated with human subjective assessment of perceptual quality compared to the perceptual loss.}
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{5}{|l|}{\textbf{ResNet18- White Noise}}                                                    \\ \hline
\textbf{Layer}                        & \textbf{Metric} & \textbf{RMSE} & \textbf{LCC} & \textbf{SROCC} \\ \hline
\multirow{2}{*}{\textbf{Res4a\_Relu}} & \textbf{$l_{p}$}              & 3.8925        & 0.9707       & 0.9703         \\ \cline{2-5}
                                      & \textbf{$l_{pv}$}             & 3.1727        & 0.9806       & 0.9758         \\ \hline
\multirow{2}{*}{\textbf{Res5a\_Relu}} & \textbf{$l_{p}$}              & 5.6544        & 0.9370       & 0.9504         \\ \cline{2-5}
                                      & \textbf{$l_{pv}$}             & 4.4505        & 0.9615       & 0.9688         \\ \hline
\multirow{2}{*}{\textbf{Res5b\_Relu}} & \textbf{$l_{p}$}              & 5.6111        & 0.9380       & 0.9444         \\ \cline{2-5}
                                      & \textbf{$l_{pv}$}             & 4.8910        & 0.9533       & 0.9578         \\ \hline
\end{tabular}
\end{table}

\begin{table*}[]
\caption{Our VFSS quantification of how good different deep features are as perceptual features is correct as the Top 15\% highest VFSS scored features maps perform much better compared to the 15\% lowest VFSS scored feature maps.}
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{5}{|l|}{\textbf{SqueezeNet - Gaussian Blur}}                                                         \\ \hline
\textbf{Layer}                                  & \textbf{Metric} & \textbf{RMSE} & \textbf{LCC} & \textbf{SROCC} \\ \hline
\multirow{2}{*}{\textbf{fire2-RelU\_expand3x3}} & Bottom-15\%            & 10.9308       & 0.7280       & 0.7469         \\ \cline{2-5}
                                                & Top-15\%             & 9.0796        & 0.8220       & 0.8501         \\ \hline
\multirow{2}{*}{\textbf{fire4-RelU\_expand1x1}} & Bottom-15\%             & 9.5820        & 0.7993       & 0.8262         \\ \cline{2-5}
                                                & Top-15\%             & 8.2463        & 0.8559       & 0.8809         \\ \hline
\multirow{2}{*}{\textbf{fire6-RelU\_expand3x3}} & Bottom-15\%             & 11.1143       & 0.7170       & 0.7347         \\ \cline{2-5}
                                                & Top-15\%             & 7.8518        & 0.8703       & 0.8810         \\ \hline
\end{tabular}
\end{table*}

\begin{table*}[]
\centering
\caption{Our VFSS quantification of how good different deep features are as perceptual features is correct as the Top 15\% highest VFSS scored features maps perform much better compared to the 15\% lowest VFSS scored feature maps.}
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{5}{|l|}{\textbf{AlexNet - Fast Fading}}                                                \\ \hline
\textbf{Layer}                    & \textbf{Metric} & \textbf{RMSE} & \textbf{LCC} & \textbf{SROCC} \\ \hline
\multirow{2}{*}{\textbf{ReLU\_1}} & Bottom-15\%             & 8.4147        & 0.8634       & 0.8573         \\ \cline{2-5}
                                  & Top-15\%             & 7.3503        & 0.8977       & 0.9011         \\ \hline
\multirow{2}{*}{\textbf{ReLU\_3}} & Bottom-15\%             & 4.9245        & 0.9554       & 0.9538         \\ \cline{2-5}
                                  & Top-15\%             & 3.6900        & 0.9752       & 0.9702         \\ \hline
\multirow{2}{*}{\textbf{ReLU\_4}} & Bottom-15\%             & 5.0769        & 0.9526       & 0.9471         \\ \cline{2-5}
                                  & Top-15\%             & 3.9109        & 0.9721       & 0.9672         \\ \hline
\end{tabular}
\end{table*}

\begin{figure*}[]
\centering
\subfloat[\textit{ReLU1\_1}]{\includegraphics[width=0.33\textwidth]{VFSS_relu1_1.png}\label{fig:f7}}
  \hfill
  \subfloat[\textit{ReLU2\_2}]{\includegraphics[width=0.33\textwidth]{VFSS_relu2_2.png}\label{fig:f6}}
  \hfill
  \subfloat[\textit{ReLU3\_2}]{\includegraphics[width=0.33\textwidth]{VFSS_relu3_2.png}\label{fig:f5}}
  \hfill
  \subfloat[\textit{ReLU3\_3}]{\includegraphics[width=0.33\textwidth]{VFSS_relu3_3.png}\label{fig:f7}}
  \hfill
  \subfloat[\textit{ReLU4\_2}]{\includegraphics[width=0.33\textwidth]{VFSS_relu4_2.png}\label{fig:f7}}
  \hfill
  \subfloat[\textit{ReLU5\_2}]{\includegraphics[width=0.33\textwidth]{VFSS_relu5_2.png}\label{fig:f7}}
  \hfill
  \caption{VFSS distributions in different layers of the VGG-16 show that only a small proportion of feature maps in a layer are most effective as perceptual features.}
\end{figure*}

\begin{figure*}[]
\centering
\subfloat[Ground Truth]{\includegraphics[width=0.33\textwidth]{im_gt_croptt1.png}\label{fig:f7}}
  \hfill
  \subfloat[{\textit{L}\textsubscript{pv} (SSIM: 0.749, PI: 3.5686)} ]{\includegraphics[width=0.33\textwidth]{im_wt_croptt1.png}\label{fig:f6}}
  \hfill
  \subfloat[{\textit{L}\textsubscript{p}} (SSIM: 0.695, PI: 3.9853)]{\includegraphics[width=0.33\textwidth]{im_uwt_croptt1.png}\label{fig:f5}}
  \hfill
  \subfloat[Ground Truth]{\includegraphics[width=0.33\textwidth]{im_gt_croptt2.png}\label{fig:f7}}
  \hfill
  \subfloat[{\textit{L}\textsubscript{pv}} (SSIM: 0.973, PI: 5.5422)]{\includegraphics[width=0.33\textwidth]{im_wt_croptt2.png}\label{fig:f6}}
  \hfill
  \subfloat[{\textit{L}\textsubscript{p}} (SSIM: 0.964, PI: 5.8690)]{\includegraphics[width=0.33\textwidth]{im_uwt_croptt2.png}\label{fig:f5}}
  \hfill
  \subfloat[Ground Truth]{\includegraphics[width=0.33\textwidth]{gt_crop3.png}\label{fig:f7}}
  \hfill
  \subfloat[{\textit{L}\textsubscript{pv}} (SSIM: 0.864, PI: 4.1285)]{\includegraphics[width=0.33\textwidth]{wt_crop3.png}\label{fig:f7}}
  \hfill
  \subfloat[{\textit{L}\textsubscript{p}} (SSIM: 0.831, PI: 4.8412)]{\includegraphics[width=0.33\textwidth]{uwt_crop3.png}\label{fig:f7}}
  \hfill
  \subfloat[Ground Truth]{\includegraphics[width=0.33\textwidth]{gt_crop6.png}\label{fig:f7}}
  \hfill
  \subfloat[{\textit{L}\textsubscript{pv}} (SSIM: 0.778, PI: 4.96)]{\includegraphics[width=0.33\textwidth]{wt_crop6.png}\label{fig:f7}}
  \hfill
  \subfloat[{\textit{L}\textsubscript{p}} (SSIM: 0.761, PI: 6.07)]{\includegraphics[width=0.33\textwidth]{uwt_crop6.png}\label{fig:f7}}
  \hfill
  \caption{Supplement single Image x4 SR results for the VDSR to demonstrate the better perception-distortion trade-off using our proposed psychovisual loss $L_{pv}$ compared to the perceptual loss $L_{p}$. }
\end{figure*}






\end{document}
