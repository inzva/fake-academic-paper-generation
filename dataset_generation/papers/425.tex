\pdfoutput=1

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{multirow}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm} % For bold alpha
\usepackage{tabulary}
\usepackage{booktabs}
\usepackage{lipsum}
\usepackage{hhline}
\usepackage[table,xcdraw]{xcolor}
\usepackage{enumitem}
\setitemize{noitemsep,topsep=5pt,parsep=5pt,partopsep=5pt}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\softmax}{softmax}
%\DeclareMathOperator*{\tanh}{tanh}

\renewcommand{\arraystretch}{1.1}  % little more room between rows in tables
\newcommand{\cev}[1]{\reflectbox{\ensuremath{\vec{\reflectbox{\ensuremath{#1}}}}}}
\newcommand{\authcomment}[3]{\textcolor{#1}{#2:[#3]}}
\newcommand{\rowan}[1]{{\color{red}rz:[#1]}}
\newcommand{\sam}[1]{\authcomment{blue}{sam}{#1}}
\newcommand{\yc}[1]{{\color{purple}yc:[#1]}}
\newcommand{\my}[1]{{\color{orange}my:[#1]}}
\newcommand{\model}{\textsc{MotifNet}}
\newcommand{\modellong}{Stacked Motif Network}

\newcommand{\p}[1]{\textrm{Pr}( #1 )}  % probability
\newcommand{\R}{\mathbb{R}}   % real numbers
\newcommand{\card}[1]{\left\vert{#1}\right\vert}  % size of set
\newcommand{\bg}{\textsc{bg}}  % background label
\newcommand{\term}[1]{\emph{#1}}  % defining a new term
\newcommand{\vect}[1]{\mathbf{#1}}   % vector var
\newcommand{\mat}[1]{\mathbf{#1}}    % matrix var
\newcommand{\appropto}{\mathrel{\vcenter{
  \offinterlineskip\halign{\hfil$##$\cr
    \propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{4272} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi

\begin{document}
%%%%%%%%% TITLE
%\title{Iterative Conditioned Context for Scene Graph Generation}
% Neural Motifs in Scene Graphs
% Neural Motifs: Higher order Scene Graph Parsing
% Neural Motifs: Higher Order Scene Graph Parsing
% Stacked Motif Network.
%\title{Neural Motifs: High Order Scene Graph Parsing}
%Accurate
\title{\vspace{-6mm}Neural Motifs: Scene Graph Parsing with Global Context}
%\title{Neural Motifs: \\Globally Conditioned Context (GCC) for Compiling Scene Graphs}
% Authors:
% Rowan, Mark, Sam, Yejin
\author{Rowan Zellers\textsuperscript{1}\quad Mark Yatskar\textsuperscript{1,2}\quad Sam Thomson\textsuperscript{3}\quad Yejin Choi\textsuperscript{1,2}
	\\ \textsuperscript{1}Paul G. Allen School of Computer Science \& Engineering, University of Washington
	\\ \textsuperscript{2}Allen Institute for Artificial Intelligence
	\\ \textsuperscript{3}School of Computer Science, Carnegie Mellon University \vspace{-1mm}
	\\ {\tt\small \{rowanz, my89, yejin\}@cs.washington.edu, sthomson@cs.cmu.edu}
	\\ \tt\normalsize \href{https://rowanzellers.com/neuralmotifs}{https://rowanzellers.com/neuralmotifs} \vspace{-5mm}
}

% \author{
% Rowan Zellers \\
% Paul G. Allen School of Computer Science & Engineering
% University of Washington, Seattle, WA, USA\\
% {\tt\small rowanz@cs.washington.edu}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}

We investigate the problem of producing structured graph representations of visual scenes. 
Our work analyzes the role of motifs: regularly appearing substructures in scene graphs.
We present new quantitative insights on such repeated structures in the Visual Genome dataset.
Our analysis shows that object labels are highly predictive of relation labels but not vice-versa.
We also find that there are recurring patterns even in larger subgraphs: more than 50\% of graphs contain motifs involving at least two relations.
Our analysis motivates a new baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set.
This baseline improves on the previous state-of-the-art by an average of 3.6\% relative improvement across evaluation settings. 
We then introduce~{\modellong}s, a new architecture designed to capture higher order motifs in scene graphs that further improves over our strong baseline by an average 7.1\% relative gain.
Our code is available at \href{github.com/rowanz/neural-motifs}{github.com/rowanz/neural-motifs}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
% Outline
We investigate scene graph parsing: the task of producing graph representations of real-world images that provide semantic summaries of objects and their relationships.
For example, the graph in Figure~\ref{fig:teaser} encodes the existence of key objects such as people (``man'' and ``woman''), their possessions (``helmet'' and ``motorcycle'', both possessed by the woman), and their activities (the woman is ``riding'' the ``motorcycle'').
Predicting such graph representations has been shown to improve natural language based image tasks~\cite{johnson_image_2015,Teney2016GraphStructuredRF,Yin2017Obj2TextGV}%because graphs map well to language structure
and has the potential to significantly expand the scope of applications for computer vision systems.
Compared to object detection ~\cite{ren_faster_2015, redmon_yolo9000:_2016} , object interactions ~\cite{yao2010modeling,chao:iccv2015} and activity recognition ~\cite{2014survey}, % which have been studied in isolation,
scene graph parsing poses unique challenges since %since predictions across all of these components may influence each other, and ultimately must be consistent.
it requires reasoning about the complex dependencies across all of these components.

\begin{figure}
    \centering
    \includegraphics[scale=.23]{teaser.pdf}
    \caption{A ground truth scene graph containing entities, such as \texttt{woman}, \texttt{bike} or \texttt{helmet}, that are localized in the image with bounding boxes, color coded above, and the relationships between those entities, such as \texttt{riding}, the relation between \texttt{woman} and \texttt{motorcycle} or \texttt{has} the relation between \texttt{man} and \texttt{shirt}.
    }
    \label{fig:teaser}
\end{figure}% \yc{only COCO, not Flickr? just double-checking.} \my{yes it is only COCO images.}
Elements of visual scenes have strong structural regularities.
For instance, people tend to wear clothes, as can be seen in Figure~\ref{fig:teaser}.
We examine these structural repetitions, or \emph{motifs}, using the Visual Genome~\cite{visualgenome} dataset, which provides annotated scene graphs for 100k images from COCO~\cite{mscoco}, consisting of over 1M instances of objects and 600k relations.
%Such correlations between objects and their relations are a type of semantic context and we analyze their presence in scene graphs from the Visual Genome~\cite{visualgenome}, a dataset containing 100k images from COCO~\cite{mscoco}, over 1M object instances and 600k labeled relationships.
Our analysis leads to two key findings.
%(1) the local relation structure between each pair of objects  exhibits strong priors such that relation categories are easily determined once the corresponding object categories are given, but not vise-versa
First, there are strong regularities in the local graph structure such that
the distribution of the relations is highly skewed once the corresponding object categories are given, but not vice versa.
%the distribution of the local relation structure conditioning on a pair of objects is highly skewed%in part because frequent visual patterns include facts that are almost always true%and (2)
Second, structural patterns exist even in larger
subgraphs; we find that over half of images contain previously occurring graph motifs. %, including motifs containing 16 elements of objects and relations.

Based on our analysis, we introduce a simple yet powerful baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set.
The baseline improves over prior state-of-the-art by 1.4 mean recall points (3.6\% relative), suggesting that an effective scene graph model must capture both the asymmetric dependence between objects and their relations, along with larger contextual patterns.

We introduce the \term{\modellong~(\model)}, a new neural network architecture
%to better address the  challenges of learning the complex structural patterns in scene graphs.
that complements existing approaches to scene graph parsing.
%Capturing sufficient graph-based context for structural consistency can pose significant computational challenges for both learning and inference.
We posit that the key challenge in modeling scene graphs lies in devising an efficient mechanism to encode the global context that can directly inform the local predictors (i.e., objects and relations).
While previous work has used graph-based inference to propagate information in both directions between objects and relations~\cite{xu_scene_2017, li2017msdn, li_vip-cnn:_2017}, our analysis suggests strong independence assumptions in local predictors limit the quality of global predictions. %s that conditioning on objects when predicting relations is the more important direction.% While graph-based inference can model aspects of the graph structure, Markov assumptions limit the quality of local predictors.
Instead, our model predicts graph elements by staging bounding box predictions, object classifications, and relationships such that the global context encoding of all previous stages establishes rich context for predicting subsequent stages, as illustrated in Figure~\ref{fig:ourmodel}.
We represent %we propose to approximate
the global context via recurrent sequential architectures such as Long Short-term Memory Networks (LSTMs) \cite{Hochreiter:1997:LSM:1246443.1246450}.

Our model builds on Faster-RCNN~\cite{ren_faster_2015}  for predicting bounding regions, fine tuned and adapted for Visual Genome.
%Information among
Global context across bounding regions is computed and propagated through bidirectional LSTMs, which is then used by another LSTM that labels each bounding region conditioned on the overall context and all previous labels.
Another specialized layer of bidirectional LSTMs then computes and propagates information for predicting edges given bounding regions, their labels, and all other computed context.
Finally, we classify all $n^2$ edges in the graph, combining globally %conditioned
contextualized representations of head, tail, and image representations using using low-rank outer products~\cite{Kim2016HadamardPF}.
The method can be trained end-to-end.

%The model starts from a bottom layer that specializes on encoding the overall object context, based on an ordered list of pooled bounding boxes based on .%%Motivated by our analysis, we present our \modellong~(\model) Model.%From a sequence of pooled bounding boxes over an image, we use an LSTM sequence-to-sequence model to commit to object labels conditioned on the full sequence of candidate boxes\cite{sutskever2014sequence, Hochreiter:1997:LSM:1246443.1246450}.%We then use another layer of bidirectional LSTMs that specializes on encoding the relational (edge) context, conditioning on both the object context encodings as well as the individual labels predicted for each bounding box.%We then use another LSTM to condition on the sequence of labeled commitments for edge prediction.%This simple linear recurrent structure allows us to lift Markov assumptions present in many graph-based models, thereby enabling the model to incorporate the global context without yet committing to a particular graph structure.%By stacking the edge (relation) layer on top of the object layer, we allow the model to leverage the notable directionality in the prediction dependency between objects and their relations. The graph-level loss propagates down to these individual context layers, thus the network can learn to embed useful global context for the overall scene graph parsing, and can be trained end-to-end including the Faster-RCNN base detectors.%Our model is integrated with a Faster-RCNN based detector~\cite{ren_faster_2015} and thus can be trained end-to-end.%The context LSTM allows us to more accurately predict highly correlated object detections, while conditioning on all detected objects allows the model to effectively exploit the local correlation between object and edge labels. Our model is integrated with a Faster-RCNN based detector~\cite{ren_faster_2015} and thus can be trained end-to-end.% These correlations present an opportunity to leverage semantic context~\cite{} to improve prediction quality, a fact many methods have tried to leverage~\cite{}.% In general, trying to incorporate context introduces significant computational challenges both at learning time and inference time.% In this work, we provide an analysis of correlated structures in the Visual Genome (VG) Dataset~\cite{}, a dataset of XXX scene graphs, and use our findings to create an effective model for integrating semantic context into scene graph predictions that significantly outperforms existing state of the art.% Our analysis explores context in scene graphs in two ways.% First, we show that the local object-relation-object structure in scene graphs exhibits strong regularity in that relationship categories are highly determined given object categories, but not vise-versa.% Second, we examine higher order structures by mining \term{memes} - subsets of object-relation-object graph elements and find that half of images contain a higher order meme (with some memes even as long as 16 elements).% Our analysis shows that previous methods that do not commit to objects before predicting relations or don't predict multiple objects simultaneously do not capture important features of scene graphs.% Our models use a Region Proposal Network~\cite{}(RPN) and encoding LSTMs ~\cite{} to create a scene graph detector that alternates between computing context for some predictions with LSTMs and then conditioning on those predictions to create context features for further predictions.% Our method includes three round, (1) predicting region proposal, (2) predicting objects, (3) predicting edges.% Between each round, an LSTM is used to compute a global context representation which, crucially, can incorporate information from all previous predictions.% The context LSTM allows us to more accurately predict highly correlated object detections, while conditioning on all detected objects allows the model to effectively exploit the local correlation between object and edge labels.% The model is trained end-to-end, allowing for the fine tuning of a base detector as well as training scene-graph detection specific components.

Experiments on Visual Genome demonstrate the efficacy of our approach. First, we update existing work by pretraining the detector on Visual Genome, setting a new state-of-the-art (improving on average across evaluation settings 14.0 absolute points).
Our new simple baseline improves over previous work, using our updated detector, by a mean improvement of 1.4 points.
% Our strong baseline improves further:% %, by 22 points:% given independent object detections, it predicts the most frequent relation between objects of those types in the training set.
Finally, experiments show {\modellong}s is effective at modeling global context, with a mean improvement of 2.9 points (7.1\% relative improvement) over our new strong baseline. %, demonstrating that our method for capturing structural motifs in scene graphs is effective.% on detection recall@50, we improve over the new implementation of previous work by another 6.5\% absolute (31\% relative gain).~\model~provides a mean improvement of 4.3\% absolute (11\% relative gain) across evaluation settings, demonstrating that our method for capturing structural motifs in scene graphs is effective.%Our method proceeds in three rounds: (1) an RPN proposes regions (2) then a contextualized representation is formed using an encoding LSTM, which is then used to prediction object categories for regions and (3) a contextualized representation is formed using an encoding LSTM which is is then used to predict edges.%Inspired by the success of recurrent models in predicting structured output such as grammar~\cite{}, semantic parses~\cite{} with remarkable structural consistency%Motivated by our analysis, our method predicts sequences%%... some statement of why previous models will have a hard time with Such long correlated structures are challenging to predict without committing to ...%Our methods leverage .%that have high mutual information%In this work, we carefully explore the nature of these correlated structures in scene graphs from the Visual Genome (VG) Dataset~\cite{} and how to effectively leverage them.%First, we provide a quantitative analysis of VG and%Then, .%We find that more than half of images contain a higher order meme, and that over 6\% of images containing memes of length at least 4 (our largest discovered meme was 16 elements).%high level%Correlation between structures in scene graphs%methods%results%provide an opportunity to reason globally across multiple instances of all these recognition problems.%Such graphs not only include objects, for example those in Figure~\ref{fig:teaser}, but also how objects relate with each other.%This is critical because much of human vision and perception consists not only of understanding entities, but also relationships between entities - for example, the fact that in Figure~\ref{fig:teaser}, the woman is riding on the motorcycle rather than simply standing next to it.%Predicting a structured representation is tremendously useful for other downstream tasks.%Intermediate structured representations can be used to retrieve images \cite{}, answer visual questions \cite{}, and describe images \cite{}.%More broadly, structured prediction is heavily used in natural language processing as an intermediary between a noisy unstructured domain (raw text) and downstream applications, and can be used for natural language generation \rowan{cite}, text classification....%However, producing scene graphs is a challenging algorithmic problem. Care must be taken to not only \emph{detect} objects in the scene, but also to identify pairwise relationships.%As we describe in Section~\ref{sec:definition}, a scene graph consists of a set of hard assignments to image regions as well as image region pairs. Even on a fixed set of region proposals, this is a challenging combinatorial optimization problem in which finding an optimal solution in theory might involve looping over all node and edge assignments.\rowan{elaborate and connect to prior work?}%One recent source of progress towards our underlying goal of producing structured representations for images is the recently introduced Visual Genome Dataset \cite{xu_scene_2017, krishna_visual_2017}. This large-scale dataset contains \rowan{N} images. Recent work has studied this dataset.... %We further work on this dataset by introducing several new baselines that test to what degree state-of-the-art models understand the image.%Here, we take past efforts a step further and investigate the memetic structures contained in this dataset (as possibly representative of cooccurrences in the real world).\sam{idk about ``memetic''. ``network motifs'' is maybe a good term.}%In this paper, we take a step forward on this dataset \rowan{reword} by producing a model that effectively and efficiently integrates context.%Motivated by our analysis of the dataset, our model conditionally decodes objects and uses their discrete assignments to create a global representation for the graph. This global representation is then used to obtain pairwise edge potentials. More details of our model are in Section~\ref{sec:model}.%In summary, we offer three key contributions:%\begin{enumerate}%    \item We present the first deep analysis of the Visual Genome Scene Graphs dataset.\sam{forward cite section} \yc{this reads as unnecessarily dismissive of prior work. can we instead say, we present new insights, which are important for both modeling and design choices for future dataset development.}%    \item We introduce new baselines that outperform prior state-of-the-art, despite being oblivious to the overall semantics of the image.\sam{forward cite section} \sam{what does ``overall semantics'' mean? they don't look at any pixels, right?}%    \item We present a model that accurately incorporates context, improving on our already strong baselines.\sam{forward cite section} \sam{this is most important and should probably go first.}%\end{enumerate}% In this paper, we study the problem of obtaining a rich semantic representation of real-world image scenes. For instance, consider the image in Figure~\ref{fig:teaser}. Our ultimate goal is to build a model that understands the relationships between salient entities: for instance, that there is a woman riding a motorcycle in the foreground, who is likely waiting for the traffic to clear. In the background, there is a man who is looking at a cycle rickshaw. \yc{I really like this example! but these are basically more higher-level action / intent interpretations than what are annotated in the visual genome, and somehow this focus on ``salient/high-level'' stuff seems to go against visual genome type parsing that aims to recover ``dense/low-level'' annotations.}% Our ultimate goal is to use such a model as an intermediary for other tasks. \yc{We can't say what our ultimate goal is, because it sounds like that's the goal of THIS paper.} Structured representations can be queried and analyzed as a possible source of visual commonsense knowledge, or used in a pipeline for image captioning, visual question answering, and retrieval. However, effective use of Scene Graphs in downstream applications requires that they convey important semantic information.
\label{sec:introduction}
\section{Formal definition}
\label{sec:definition}
\vspace{-5pt}
A \term{scene graph}, $G$, is a structured representation of the semantic content of an image~\cite{johnson_image_2015}.
It consists of:
\vspace*{-1mm}\begin{itemize}
  \item a set $B = \{ b_1, \ldots, b_n \}$ of \term{bounding boxes}, %with each 
  $b_i \in \mathbb{R}^4$,
  \vspace*{-1mm}
  \item a corresponding set $O = \{o_1, \ldots, o_n\}$ of \term{objects}, assigning a class label $o_i \in \mathcal{C}$ to each $b_i$, and
%   \item a set $A$ of unary \term{attributes} of those objects \sam{do I need to even mention this?}, and \rowan
  \vspace*{-1mm}
  \item a set $R = \{r_1, \ldots, r_m\}$ of binary relationships between those objects.
\end{itemize}\vspace*{-1mm}
Each relationship $r_k \in \mathcal{R}$ is a triplet of a start node $(b_i, o_i) \in B \times O$, an end node $(b_j, o_j) \in B \times O$, and a relationship label $x_{i \to j} \in \mathcal{R}$, where $\mathcal{R}$ is the set of all predicate types, including the ``background'' predicate, \bg, which indicates that there is no edge between the specified objects.
See Figure~\ref{fig:teaser} for an example scene graph.

%($\argmax_{G}{\textrm{Pr}_\theta(G|I)}$, which may have fewer than $K$ relations), but rather the one that maximizes expected recall: $\argmax_{G}{\sum_{G^\prime}\textrm{Pr}_\theta(G^\prime|I)r(G, G^\prime)}$, where $r$ is the recall function.}% Can we connect this to our model?%The Visual Genome dataset \cite{krishna_visual_2017} is a corpus of 108,077 images with corresponding crowdsourced scene graphs.%We use the the version of the dataset with the preprocessing from \cite{xu_scene_2017} to make our model directly comparable to their work.%This cleaned up dataset contains 150 labeled object categories $\mathcal{C}$ and 50 predicate categories $\mathcal{R}$. 
\label{sec:problem}
\section{Scene graph analysis}
\label{sec:analysis}
\begin{table}[t]
    \small
    \centering
    \begin{tabular}{@{}c c c c@{}}
        \toprule

        Type & Examples & Classes & Instances \\

        \midrule

        \multicolumn{4}{c}{Entities}\\

        \cmidrule{1-4}

         Part & arm, tail, wheel & 32 & 200k (25.2\%)\\
         Artifact & basket, fork, towel & 34 & 126k (16.0\%) \\
         Person & boy, kid, woman & 13 & 113k (14.3\%) \\
         Clothes & cap, jean, sneaker & 16 & 91k (11.5\%) \\
         Vehicle & airplane, bike, truck,  & 12 &  44k (5.6\%)\\
         Flora & flower, plant, tree & 3 & 44k (5.5\%)\\
         Location & beach, room, sidewalk & 11 & 39k (4.9\%) \\
         Furniture & bed, desk, table & 9  & 37k (4.7\%)\\
         Animal & bear, giraffe, zebra  & 11 & 30k (3.8\%) \\
         Structure & fence, post, sign & 3 & 30k (3.8\%)\\
         Building & building, house & 2 & 24k (3.1\%)\\
         Food & banana, orange, pizza & 6  & 13k (1.6\%) \\

        \cmidrule{1-4}

        \multicolumn{4}{c}{Relations}\\

        \cmidrule{1-4}

         Geometric & above, behind, under & 15 & 228k (50.0\%)\\
         Possessive & has, part of, wearing & 8 & 186k (40.9\%)\\
         Semantic & carrying, eating, using & 24 & 39k (8.7\%)\\
         Misc & for, from, made of & 3 & 2k (0.3\%) \\

         \bottomrule
    \end{tabular}
    \caption{Object and relation types in Visual Genome, organized by super-type. Most, 25.2\% of entities are parts and 90.9\% of relations are geometric or possessive.}
    \label{tab:data_stat}
\end{table}%In this section we provide a systematic analysis of (a) what types of entities and relations are in the Visual Genome scene graphs, and (b) how correlated substructures are within scene graphs. These two analyses motivate our baselines and models in Section~\ref{sec:model}.
In this section, we seek quantitative insights on the structural regularities of scene graphs. In particular, (a) how different types of relations correlate with different objects, and (b) how higher order graph structures recur over different scenes. These insights motivate both the new baselines we introduce in this work and our model that better integrates the global context, described in  Section~\ref{sec:model}.

\subsection{Prevalent Relations in Visual Genome}
To gain insight into the Visual Genome scene graphs, we first categorize objects and relations into high-level types.
As shown in  Table~\ref{tab:data_stat}, the predominant relations are \term{geometric} and \term{possessive}, with clothing and parts making up over one third of entity instances.
Such relations are often obvious, e.g., houses tend to have windows.
In contrast, \term{semantic} relations, which correspond to activities, are less frequent and less obvious.
Although nearly half of relation types are semantic in nature, they comprise only 8.7\% of relation instances.
The relations ``using'' and ``holding''  account for 32.2\% of all semantic relation instances.

Using our high-level types, we visualize the distribution of relation types between object types in Figure~\ref{fig:edges}.
Clothing and part entities are almost exclusively linked through possessive relations while furniture and building entities are almost exclusively linked through geometric relations.
Geometric and spatial relationships between certain entities are interchangeable, for example,
when a ``part'' is the head object, it tends to connect to other entities through a geometric relation (e.g. wheel on bike); when a ``part'' is the tail object, it tends to be connected with possessive relations (e.g. bike has wheel).
Nearly all semantic relationship are headed by people, with the majority of edges relating to artifacts, vehicles, and locations.
Such structural predictability and the prevalence of geometric and part-object relations suggest that common sense priors play an important role in generating accurate scene graphs.

\begin{figure}[t]
    \vspace{-.2cm}
    \centering
    \includegraphics[scale=.23]{visual_genome_edge_bias_v2.pdf}
    \caption{Types of edges between high-level categories in Visual Genome. Geometric, possessive and semantic edges cover 50.9\%, 40.9\%, and 8.7\%, respectively, of edge instances in scene graphs. The majority of semantic edges occur between people and vehicles, artifacts and locations. %The most frequent edge is ``wearing'' between people and clothes, corresponding to 12\% of all edges.
    Less than 2\% of edges between clothes and people are semantic.}
    \label{fig:edges}
\end{figure}\begin{figure}[t]
    \vspace{-.3cm}
    \centering
    \includegraphics[scale=.23]{visual_genome_dataset_bias_rowanfix.pdf}
    \caption{The likelihood of guessing, in the top-k, head, tail, or edge labels in a scene graph, given other graph components (i.e. without image features). Neither head nor tail labels are strongly determined by other labels, but given the identity of head and tail, edges (\texttt{edge} $|$ \texttt{head, tail}) can be determined with 97\% accuracy in under 5 guesses. Such strong biases make it critical to condition on objects when predicting edges.} %model co-occurrence of structure in scene graphs.}
    \label{fig:conditionals}
\end{figure}
In Figure~\ref{fig:conditionals}, we examine how much information is gained by knowing the identity of different parts %of a relationship
in a scene graphs.
In particular, we consider how many guesses are required to determine the labels of head (h), edge (e) or tail (t) given labels of the other elements, only using label statistics computed on scene graphs.
Higher curves imply that the element is highly determined given the other values.
The graph shows that the local distribution of relationships has significant structure.
In general, the identity of edges involved in a relationship is not highly informative of other elements of the structure while the identities of head or tail provide significant information, both to each other and to edge labels.
Adding edge information to already given head or tail information provides minimal gain.
Finally, the graph shows edge labels are highly determined given the identity of object pairs:
the most frequent relation is correct 70\% of the time, and
the five most frequent relations for the pair contain the correct label 97\% of the time.

\subsection{Larger Motifs}

Scene graphs not only have local structure but have higher order structure as well.
We conducted an analysis of repeated motifs in scene graphs by mining combinations of object-relation-object labels that have high pointwise mutual information with each other.
Motifs were extracted iteratively: first we extracted motifs of two combinations, replaced all instances of that motif with an atomic symbol and mined new motifs given previously identified motifs.
Combinations of graph elements were selected as motifs if both elements involved occurred at least 50 times in the Visual Genome training set and were at least 10 times more likely to occur together than apart.
Motifs were mined until no new motifs were extracted.
Figure~\ref{fig:motifs} contains example motifs we extracted on the right, and the prevalence of motifs of different lengths in images on the left.
Many motifs correspond to either combinations of parts, or objects that are commonly grouped together.
Over 50\% of images in Visual Genome contain a motif involving at least two combinations of object-relation-object, and some images contain motifs involving as many as 16 elements.

\begin{figure}[t]
    \vspace{-.3cm}
    \centering
    \includegraphics[scale=.23]{meme-yness.pdf}
    \caption{On the left, the percent of images that have a graph motif found in Visual Genome using pointwise mutual information, composed of at least a certain length (the number of edges it contains). Over 50\% of images have at least one motif involving two relationships. On the right, example motifs, where structures repeating many times is indicated with plate notation. For example, the second motif is length 8 and consists of 8 flower-in-vase relationships. Graph motifs commonly result from groups (e.g., several instances of ``leaf on tree''), and correlation between parts (e.g., ``elephant has head,'' ``leg,'' ``trunk,'' and ``ear.''). }
    \label{fig:motifs}
\end{figure}% \begin{table}[h]% \label{tab:memes}% \centering% \begin{tabular}{@{}l | l@{}}% n & frequency \\ \hline% 6 & 0.000387372\\% 5 & 0.00605268\\% 4 & 0.0605752 \\% 3 & 0.141504\\% 2 & 0.51882\\% \bottomrule% \end{tabular}% \caption{cdf of (o1,r,o2)}% \end{table}
\section{Model}
\label{sec:model}
\begin{figure*}[t]
    \vspace{-3mm}
    \centering
    \includegraphics[scale=.63]{modelfig.pdf}
    \caption{A diagram of a \modellong~ (\model). The model breaks scene graph parsing into stages predicting bounding regions, labels for regions, and then relationships. Between each stage, global context is computed using bidirectional LSTMs and is then used for subsequent stages. In the first stage, a detector proposes bounding regions and then contextual information among bounding regions is computed and propagated (object context). The global context is used to predict labels for bounding boxes. Given bounding boxes and labels, the model constructs a new representation (edge context) that gives global context for edge predictions. Finally, edges are assigned labels by combining contextualized head, tail, and union bounding region information with an outer product.\vspace{-1mm}}
    \label{fig:ourmodel}
 %   \vspace{-0.3cm}

\end{figure*}

Here we present our novel model, \term{\modellong}~(\model).
\model\decomposes the probability of a graph $G$ (made up of a set of bounding regions $B$, object labels $O$, and labeled relations $R$) into three factors:
\begin{equation}
\p{G \mid I} = \p{B \mid I} \ \p{O \mid B, I} \ \p{R \mid B, O, I}.
\end{equation}
Note that this factorization makes no independence assumptions.
Importantly, predicted object labels may depend on one another, and predicted relation labels may depend on predicted object labels.
The analyses in Section~\ref{sec:analysis} make it clear that capturing these dependencies is crucial.

The \emph{bounding box} model ($\p{B \mid I}$) is a fairly standard object detection model, which we describe in Section~\ref{subsec:model:bounding_boxes}.
The \emph{object} model ($\p{O \mid B, I}$; Section~\ref{subsec:model:objects}) conditions on a potentially large set of predicted bounding boxes, $B$.
To do so, we linearize $B$ into a sequence that an LSTM then processes to create a contextualized representation of each box.
Likewise, when modeling \emph{relations} ($\p{R \mid B, O, I}$; Section~\ref{subsec:model:relations}), we linearize the set of predicted labeled objects, $O$, and process them with another LSTM to create a representation of each object in context.
Figure~\ref{fig:ourmodel} contains a visual summary of the entire model architecture.

\subsection{Bounding Boxes}\label{subsec:model:bounding_boxes}

We use Faster R-CNN as an underlying detector~\cite{ren_faster_2015}.
For each image $I$, the detector predicts a set of region proposals $B = \{ b_1, \ldots, b_n \}$.
For each proposal $b_i \in B$ it also outputs a feature vector $\vect{f}_i$ and a vector $\vect{l}_i \in \R^{\card{\mathcal{C}}}$ of (non-contextualized) object label probabilities.
Note that because \bg\is a possible label, our model has not yet committed to any bounding boxes.
See Section~\ref{subsec:setup:model_details} for details.

\subsection{Objects}\label{subsec:model:objects}\paragraph{Context}
We construct a contextualized representation for object prediction based on the set of proposal regions $B$.
Elements of $B$ are first organized into a linear sequence, $[(b_1,\vect{f}_1,\vect{l}_1), \ldots, (b_n,\vect{f}_n, \vect{l}_n)]$.\footnote{We consider several strategies to order the regions, see Section ~\ref{subsec:setup:model_details}.}
The \term{object context}, $\mat{C}$, is then computed using a bidirectional LSTM~\cite{Hochreiter:1997:LSM:1246443.1246450}:
\begin{equation}
\mat{C} = \text{biLSTM}(
  [\vect{f}_i; \mat{W}_1\vect{l}_i]_{i=1,\ldots,n}
),
\end{equation}$\mat{C} = [\vect{c}_1, \ldots, \vect{c}_n]$ contains the final LSTM layer's hidden states for each element in the linearization of $B$, and $\mathbf{W}_1$ is a parameter matrix that maps the distribution of predicted classes, $\vect{l}_1$, to $\R^{100}$. The biLSTM allows all elements of $B$ to contribute information about potential object identities.

\paragraph{Decoding}
The context $\mat{C}$ is used to sequentially decode labels for each proposal bounding region, conditioning on previously decoded labels.
We use an LSTM to decode a category label for each contextualized representation in $\mat{C}$:
\begin{align}
    \vect{h}_i &= \text{LSTM}_i\left( [\vect{c}_{i}; \vect{\hat{o}}_{i-1}] \right)\\
    % \vect{\hat{p}}_i &=
    % \softmax{\left( \mat{W}_o~\vect{h}_i \right)}  \in \R^{\card{\mathcal{C}}} \\
    \label{eqn:max_o}
    \vect{\hat{o}}_i &= \argmax{\left( \mat{W}_o~\vect{h}_i \right)} \in \R^{\card{\mathcal{C}}} \text{ (one-hot)}
\end{align}
We then discard the hidden states $\vect{h}_i$ and use the object class commitments $\vect{\hat{o}}_i$ in the relation model (Section \ref{subsec:model:relations}).

\subsection{Relations}\label{subsec:model:relations}\paragraph{Context}
We construct a contextualized representation of bounding regions, $B$, and objects, $O$, using additional bidirectional LSTM layers:
\begin{equation}
\mat{D} = \text{biLSTM}(
  [\vect{c}_i; \mat{W}_2 \vect{\hat{o}}_i]_{i=1,\ldots,n}
),
\end{equation}
where the \term{edge context}$\mat{D} = [\vect{d}_{1}, \ldots, \vect{d}_{n}]$ contains the states for each bounding region at the final layer, and $\mat{W}_2$ is a parameter matrix mapping $\vect{\hat{o}}_i$ into $\R^{100}$.

\paragraph{Decoding}
There are a quadratic number of possible relations in a scene graph.
For each possible edge, say between $b_i$ and $b_j$, we compute the probability the edge will have label $x_{i \to j}$ (including \bg).
%Our relation model approximates the outer product between head, tail, and image representations, using a low-rank approximation through Hadamard products~\cite{Kim2016HadamardPF}.\sam{TODO: awkward phrasing}
The distribution uses global context, $\mat{D}$, and a feature vector for the union of boxes~\footnote{A union box is the convex hull of the union of two bounding boxes.}, $\vect{f}_{i,j}$:
\begin{align}
\label{eq:hadamard}
      \vect{g}_{i, j} &= (\mat{W}_h \vect{d}_{i}) \circ (\mat{W}_t \vect{d}_{j}) \circ \vect{f}_{i, j} \\
      \p{ x_{i \to j} \mid B, O } &=
 \softmax{\left(
   \mat{W}_r \vect{g}_{i, j} + \vect{w}_{o_i,o_j}
 \right)}.
\end{align}$\mat{W}_h$ and $\mat{W}_t$ project the head and tail context into $\R^{4096}$.
$\vect{w}_{o_i,o_j}$ is a bias vector specific to the head and tail labels.
%To obtain the edge conditional probabilities, we take the softmax over $r_{i,j}$.% Let $B_I$ denote the collection of boxes extracted from our Faster RCNN detector from image $I$, let $\mathbf{x}_i$ refer to a single box and its visual features, and let $x_i^{cls}$ be its object label. Let $x_{i\to j}$ represent the edge between boxes $i$ and $j$.% After extracting all detections from Faster RCNN, we then sort them in decreasing order of confidence of the highest ranked non-background label: $\max_{x_i^{cls} \ne \textrm{bg}}\textrm{Pr}(x_i^{cls}|b_i, I)$. We then use a sequence-to-sequence model to assign hard labels onto the boxes. This is necessary because ultimately, the network must commit to a single object label per box at test time. As discussed in the analysis section \rowan{add backward ref}, because the edge types are highly dependent on the object identities, conditioning on the discrete object label is important.% We use a Bidirectional LSTM \cite{Hochreiter:1997:LSM:1246443.1246450} to obtain a contextual representation of all of the object candidates. Let $\bm{\alpha}_t$ denote the probability distribution for the labels of box $t$ as given to us by the detector. This is used to softly attend over label embeddings from the embedding matrix $\mathbf{W}_{emb}$. There is one 100-dimensional embedding per object label, including background. Then, we encode the context sequentially as follows:% \begin{equation}% \vec{\mathbf{h}}_t = \textrm{LSTM}(\mathbf{x}_t \| \mathbf{W}_{emb}\bf{\alpha}_t, \vec{\mathbf{h}}_{t-1})% \end{equation}% Here, $\|$ represents concatenation. To obtain the full encoding, we concatenate the forward and backwards directions together, along with the original image features $\mathbf{x}_t$. For decoding, we use another LSTM% \begin{align}% \vec{\mathbf{g}}_t &= \textrm{LSTM}(\mathbf{x}_t \| \vec{\mathbf{h}}_t \| \cev{\mathbf{h}}_t \| \mathbf{W}_{emb}x_{t-1}^{cls}, \vec{\mathbf{g}}_{t-1})\\% x_{t}^{cls} &= \argmax \mathbf{W}_{dec}\vec{\mathbf{g}}_t.% \end{align}% Here, we apply the additional constraint that $x_{t}^{cls}$ may not be background. This is important because otherwise at test time, if the model predicts background for all nodes, then we will have a recall of 0.% Next, we use a third LSTM to encode the edge context. This takes as input the image features, object context, as well as hard and soft embeddings for the object label. This allows us the model to downweight unlikely object predictions while also being aware that it will condition on them later. This takes the form% \begin{align}% \vec{\mathbf{e}}_t &= LSTM(\mathbf{x}_t \| \vec{\mathbf{h}}_t \| \cev{\mathbf{h}}_t \| \mathbf{W}_{emb}x_t^{cls} \|  \mathbf{W}_{emb}\mathbf{x}_t^{cls},  \vec{\mathbf{e}}_{t-1})\\% \end{align}% Last, we represent the probability of a relationship given the objects and context as an outer product between image features and box context for the two boxes. Since computing the outer product isn't feasible, we use an approximation: head, tail, and vision features are each mapped to 2048 dimensional space, along with the predicates. Then, we use a softmax to compute the desired probability:% \begin{equation}% \begin{split}% \textrm{Pr}(x_{i \to j} | &\hat{\mathbf{x}}^{cls},I) = \\% & P \left( (\mathbf{W}_{head}\mathbf{e}_i)  \odot (\mathbf{W}_{tail}\mathbf{e}_j) \odot f(b_i, b_j, I)  \right),% \end{split}% \end{equation}% \sam{missing a ``$\odot \vec{1}$''? what's $P$?}% where $f$ denotes the visual features (without a ReLU before the average pool).
\section{Experimental Setup}
\label{sec:setup}
In the following sections we explain (1) details of how we construct the detector, order bounding regions, and implement the final edge classifier (Section~\ref{subsec:setup:model_details}), (2) details of training (Section~\ref{subsec:setup:training}), and (3) evaluation (Section~\ref{subsec:setup:evaluation}).

\subsection{Model Details}\label{subsec:setup:model_details}\paragraph{Detectors}
Similar to prior work in scene graph parsing~\cite{xu_scene_2017, li2017msdn}, we use Faster RCNN with a VGG backbone as our underling object detector \cite{ren_faster_2015,Simonyan14c}. Our detector is given images that are scaled and then zero-padded to be 592x592. We adjust the bounding box proposal scales and dimension ratios to account for different box shapes in Visual Genome, similar to YOLO-9000~\cite{redmon_yolo9000:_2016}. To control for detector performance in evaluating different scene graph models, we first pretrain the detector on Visual Genome objects. We optimize the detector using SGD with momentum on 3 Titan Xs, with a batch size of $b=18$, and a learning rate of $lr=1.8\cdot 10^{-2}$ that is divided by 10 after validation mAP plateaus.
%\footnote{As measured by mAP at an IOU threshold of 0.5.}
For each batch we sample 256 RoIs per image, of which 75\% are background. The detector gets 20.0 mAP (at 50\% IoU) on Visual Genome; the same model, but trained and evaluated on COCO, gets 47.7 mAP at 50\% IoU. Following \cite{xu_scene_2017}, we integrate the use the detector freezing the convolution layers and duplicating the fully connected layers, resulting in separate branches for object/edge features.
\paragraph{Alternating Highway LSTMs}
To mitigate vanishing gradient problems as information flows upward, we add highway connections to all LSTMs \cite{he2017deep, Srivastava:2015:TVD:2969442.2969505, zhang_highway_lstm}. %This is particularly important for computing object context, the visual features $\vect{f}$ are already highly informative of the object label.
To additionally reduce the number of parameters, we follow \cite{he2017deep} and alternate the LSTM directions. Each alternating highway LSTM step can be written as the following wrapper around the conventional LSTM equations \cite{Hochreiter:1997:LSM:1246443.1246450}:
\begin{align}
\mathbf{r}_i &= \sigma(\mathbf{W}_g[\mathbf{h}_{i-\delta}, \mathbf{x}_i] + \mathbf{b}_g) \\
\mathbf{h}_i &= \mathbf{r_i} \circ \text{LSTM}(\mathbf{x}_i, \mathbf{h}_{i-\delta}) + (1-\mathbf{r_i}) \circ \mathbf{W}_i\mathbf{x}_i,
\end{align}
where $\mathbf{x}_i$ is the input, $\mathbf{h}_i$ represents the hidden state, and $\delta$ is the direction: $\delta=1$ if the current layer is even, and $-1$ otherwise. For \model, we use 2 alternating highway LSTM layers for object context, and 4 for edge context.
\paragraph{RoI Ordering for LSTMs}
We consider several ways of ordering the bounding regions:
\begin{enumerate}[label={(\arabic*)},noitemsep,nolistsep]
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
    \item \textsc{LeftRight} (default): Our default option is to sort the regions left-to-right by the central x-coordinate: we expect this to encourage the model to predict edges between nearby objects, which is beneficial as objects appearing in relationships tend to be close together.
    \item \textsc{Confidence}: Another option is to order bounding regions based on the confidence of the maximum non-background prediction from the detector: $\max_{j \ne \bg} \vect{l}_i^{(j)}$, as this lets the detector commit to ``easy'' regions, obtaining context for more difficult regions.\footnote{When sorting by confidence, the edge layer's regions are ordered by the maximum non-background object prediction as given by Equation~\ref{eqn:max_o}.}
    \item \textsc{Size}: Here, we sort the boxes in descending order by size, possibly predicting global scene information first.
    %to predict global scene information before predicting individual objects.
    \item \textsc{Random}: Here, we randomly order the regions.
%\vspace{-5mm}
\end{enumerate}\paragraph{Predicate Visual Features} To extract visual features for a predicate between boxes $b_i, b_j$, we resize the detector's features corresponding to the union box of $b_i,b_j$ to 7x7x256. We model geometric relations using a 14x14x2 binary input with one channel per box. We apply two convolution layers to this and add the resulting 7x7x256 representation to the detector features. Last, we apply finetuned VGG fully connected layers to obtain a 4096 dimensional representation.\footnote{We remove the final ReLU to allow more interaction in Equation~\ref{eq:hadamard}.}\subsection{Training}\label{subsec:setup:training}
We train~\model~on ground truth boxes, with the objective to predict object labels and to predict edge labels given ground truth object labels. For an image, we include all annotated relationships (sampling if more than 64) and sample 3 negative relationships per positive.
In cases with multiple edge labels per directed edge (5\% of edges), we sample the predicates. Our loss is the sum of the cross entropy for predicates and cross entropy for objects predicted by the object context layer. We optimize using SGD with momentum on a single GPU, with $lr=6\cdot10^{-3}$ and $b=6$.

\paragraph{Adapting to Detection}
Using the above protocol gets good results when evaluated on scene graph classification, but models that incorporate context underperform when suddenly introduced to non-gold proposal boxes at test time.

To alleviate this, we fine-tune using noisy box proposals from the detector.
We use per-class non-maximal suppression (NMS) \cite{NMSCITATION} at 0.3 IoU to pass 64 proposals to the object context branch of our model. We also enforce NMS constraints during decoding given object context.
We then sample relationships between proposals that intersect with ground truth boxes and use relationships involving these boxes to finetune the model until detection convergence.

We also observe that in detection our model gets swamped with many low-quality RoI pairs as possible relationships, which slows the model and makes training less stable. To alleviate this, we observe that nearly all annotated relationships are between overlapping boxes,\footnote{A hypothetical model that perfectly classifies relationships, but only between boxes with nonzero IoU, gets 91\% recall.} and classify all relationships with non-overlapping boxes as \bg.

\subsection{Evaluation}\label{subsec:setup:evaluation}
We train and evaluate our models on Visual Genome, using the publicly released preprocessed data and splits from \cite{xu_scene_2017}, containing 150 object classes and 50 relation classes, but sample a development set from the training set of 5000 images. %, and report results on the test set of 26785 images with non-empty scene graphs.
We follow three standard evaluation modes: (1) \textbf{predicate classification} (\textsc{PredCls}): given a ground truth set of boxes and labels, predict edge labels, (2) \textbf{scene graph classification} (\textsc{SGCls}): given ground truth boxes, predict box labels and edge label and (3) \textbf{scene graph detection} (\textsc{SGDet}): predict boxes, box labels, and edge labels.
%in which we must jointly predict $O$ and $R$ given only the image.
The annotated graphs are known to be incomplete,
%, since an exhaustive annotation of every possible entity and relation would be unfeasible.
thus systems are evaluated using recall@$K$ metrics.\footnote{Past work has considered these evaluation modes at recall thresholds R$@50$ and R$@100$, but we also report results on R$@20$.}% rather than precision.

In all three modes, recall is calculated for relations; a ground truth edge $(b_h, o_h, x, b_t, o_t)$ is counted as a ``match'' if there exist predicted boxes $i,j$ such that $b_i$ and $b_j$ respectively have sufficient overlap with $b_h$ and $b_t$,\footnote{As in prior work, we compute the intersection-over-union (IoU) between the boxes and use a threshold of 0.5.} and the objects and relation labels agree.
%The task is to output a set of relations $\{r_1, \ldots, r_K\}$.\footnote{Given this evaluation, systems are incentivized to predict exactly $K$ relations, never fewer.%The optimal scene graph prediction $G^*$ is \emph{not} the one with highest probability under the model ($\argmax_{G}{\textrm{Pr}(G|I)}$, which may have fewer than $K$ relations), but rather the one that maximizes expected recall: $\argmax_{G}{\sum_{G^\prime}\textrm{Pr}(G^\prime|I)r(G, G^\prime)}$, where $r$ is the recall function.\rowan{Is this a good thing to have here?}}
We follow previous work in enforcing that for a given head and tail bounding box, the system must not output multiple edge labels \cite{xu_scene_2017, lu_visual_2016}.
%in requiring that %predictions be made on the ground truth set of boxes when given - otherwise, a model could duplicate objects and edges so as to get extra guesses for the same relationships, while implicitly violating the scene-graph constraints.%two identical boxes can't have multiple directed edge labels.%the predicted graphs must be internally consistent, meaning that no object may have an intersection-over-union (IoU) of more than 0.5 with any other predicted object.\subsection{Frequency Baselines}
To support our finding that object labels are highly predictive of edge labels, we additionally introduce several frequency baselines built off training set statistics. The first, \textsc{Freq}, uses our pretrained detector to predict object labels for each RoI. To obtain predicate probabilities between boxes $i$ and $j$, we look up the empirical distribution over relationships between objects $o_i$ and $o_j$ as computed in the training set.\footnote{Since we consider an edge $x_{i \to j}$ to have label \bg\ if $o$ has no edge to $j$, this gives us a valid probability distribution.} Intuitively, while this baseline does not look at the image to compute $\textrm{Pr}(x_{i\to j} | o_i, o_j)$, it displays the value of \emph{conditioning} on object label predictions $o$. The second, \textsc{Freq-Overlap}, requires that the two boxes intersect in order for the pair to count as a valid relation.

\section{Results}
\label{sec:results}
% \begin{table*}[htbp]% \vspace{-.2cm}% \small% \centering% %\ra{1.2}% \begin{tabular}{@{}l ccc ccc ccc@{}}% \toprule%       & \multicolumn{2}{c}{Scene Graph Detection} &  \phantom{} & \multicolumn{2}{c}{Scene Graph Classification} &  \phantom{} & \multicolumn{2}{c}{Predicate Classification} & Mean\\%       \cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} %\my89{--blame->\cmidrule{10}<--blame-- this change made it not compile and crash the project. whhhhyyy?}% Model & R$@$50  & R$@$100 && R$@$50  & R$@$100  && R$@$50  & R$@$100 &  \\% \midrule% \sc Vrd \cite{lu_visual_2016} & 0.3  & 0.5 && 11.8  & 14.1  && 27.9  & 35.0 & 14.9\\% \sc Message Passing \cite{xu_scene_2017}   & 3.4  & 4.2 &&  21.7 & 24.4 && 44.8 & 53.0 & 25.3 \\% \sc Message Passing$+$ & 20.7 & 24.5 && 34.6 & 35.4 && 59.3 & 61.3 & 39.3 \\% \sc Associative Embedding \cite{DBLP:journals/corr/NewellD17}$\star$ & 8.1 & 8.2 && 21.8 & 22.6 && 54.1 & 55.4 & 28.3 \\ \hline% \textsc{Freq} & 23.5 & 27.6 && 32.4 & 34.0 && 59.9 & 64.1 & 40.2 \\% \textsc{Freq-Overlap} & 26.2 & 30.1 && 32.3 & 32.9 && 60.6 & 62.2 & 40.7 \\% \model~~(default \textsc{LeftRight} order)& {\bf 27.2} & {\bf 30.3} && {\bf 35.8} & {\bf 36.5} && {\bf 65.2} & {\bf 67.1} & {\bf 43.6} \\% \bottomrule% \end{tabular}% \label{tab:superresults}% \caption{Results table, adapted from \cite{xu_scene_2017} which ran VRD \cite{lu_visual_2016} without language priors. All numbers in \%. $\star$: results in \cite{DBLP:journals/corr/NewellD17} are without scene graph constraints; we evaluated performance with constraints using saved predictions given to us by the authors (see Table~\ref{tab:supptable} in supp).% } % \end{table*}\begin{table*}[htbp]
\vspace{-.2cm}
\small
\centering
%\ra{1.2}
\begin{tabular}{@{}c@{\hspace{0.4em}} l c@{\hspace{0.2em}} ccc c@{\hspace{0.2em}} ccc c@{\hspace{0.2em}} ccc c@{\hspace{0.2em}} c@{}}
\toprule
      && \phantom{} & \multicolumn{3}{c}{Scene Graph Detection} &  \phantom{} & \multicolumn{3}{c}{Scene Graph Classification} &  \phantom{} & \multicolumn{3}{c}{Predicate Classification} & \phantom{} & Mean\\
    \cmidrule{4-6} \cmidrule{8-10} \cmidrule{12-14} %\my89{--blame->\cmidrule{10}<--blame-- this change made it not compile and crash the project. whhhhyyy?}
& Model && R$@$20 & R$@$50  & R$@$100 && R$@$20 & R$@$50  & R$@$100  && R$@$20 & R$@$50  & R$@$100 &&  \\
\midrule
\multirow{7}{*}{\rotatebox[origin=c]{90}{models}} & \sc Vrd \cite{lu_visual_2016} && & 0.3  & 0.5 && & 11.8  & 14.1  && & 27.9  & 35.0 && 14.9\\
&\sc Message Passing \cite{xu_scene_2017}&&  & 3.4  & 4.2 &&  &21.7 & 24.4 && & 44.8 & 53.0 && 25.3 \\
&\sc Message Passing$+$ && 14.6 & 20.7 & 24.5 && 31.7 & 34.6 & 35.4 && 52.7 & 59.3 & 61.3 && 39.3 \\
&\sc Assoc Embed \cite{DBLP:journals/corr/NewellD17}$\star$ && 6.5 & 8.1 & 8.2 && 18.2 & 21.8 & 22.6 && 47.9 & 54.1 & 55.4 && 28.3 \\ %\hline
& \textsc{Freq} && 17.7 & 23.5 & 27.6 && 27.7 & 32.4 & 34.0 && 49.4 & 59.9 & 64.1 && 40.2 \\
&\textsc{Freq+Overlap} && 20.1 & 26.2 & 30.1 && 29.3 & 32.3 & 32.9 && 53.6 & 60.6 & 62.2 && 40.7 \\
& \model-\textsc{LeftRight} &&21.4 & 27.2 & 30.3 && {\bf 32.9} & {\bf 35.8} & {\bf 36.5} && {\bf 58.5} & {\bf 65.2} & {\bf 67.1} && {\bf 43.6}\\ \hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{ablations}}
&\model-\textsc{NoContext} && 21.0 & 26.2 & 29.0 && 31.9 & 34.8 & 35.5 && 57.0 & 63.7 & 65.6 && 42.4\\ %\hline
%\multirow{3}{*}{\rotatebox[origin=c]{90}{order}} 
& 
\model-\textsc{Confidence} &&{\bf 21.7} & {\bf 27.3} & {\bf 30.5} && 32.6 & 35.4 & 36.1 && 58.2 & 65.1 & 67.0 && 43.5 \\
&\model-\textsc{Size} && 21.6 & {\bf 27.3} & 30.4 && 32.2 & 35.0 & 35.7 && 58.0 & 64.9 & 66.8 && 43.3\\
&\model-\textsc{Random} && 21.6 & {\bf 27.3} & 30.4 && 32.5 & 35.5 & 36.2 && 58.1 & 65.1 & 66.9 && 43.5\\ %\hline
\bottomrule

\end{tabular}
\label{tab:superresults}
\caption{Results table, adapted from \cite{xu_scene_2017} which ran VRD \cite{lu_visual_2016} without language priors. All numbers in \%. Since past work doesn't evaluate on R$@$20, we compute the mean by averaging performance on the 3 evaluation modes over R$@$50 and R$@$100. $\star$: results in \cite{DBLP:journals/corr/NewellD17} are without scene graph constraints; we evaluated performance with constraints using saved predictions given to us by the authors 
(see Table~\ref{tab:supptable} in supp).
% (see Table in supp).
} 
\vspace{-0.1cm}
\end{table*}

We present our results in Table~\ref{tab:superresults}.
We compare \model~to previous models not directly incorporating context (\textsc{Vrd}\cite{lu_visual_2016} and \textsc{Assoc Embed}\cite{DBLP:journals/corr/NewellD17}), a state-of-the-art approach for incorporating graph context via message passing (\textsc{Message Passing}) \cite{xu_scene_2017}, and its re-implemenation using our detector, edge model, and NMS settings (\textsc{Message Passing+}). Unfortunately, many scene graph models are evaluated on different versions of Visual Genome; see 
Table~\ref{tab:supptable} in 
the supp for more analysis. %more comparison. %between them.

Our best frequency baseline, \textsc{Freq+Overlap}, improves over prior state-of-the-art by 1.4 mean recall, primarily due to improvements in detection and predicate classification, where it outperforms \textsc{Message Passing+} by 5.5 and 6.5 mean points respectively. 
\model~improves even further, by 2.9 additional mean points over the baseline (a 7.1\% relative gain).  
%Most notably, our model for detection has a relative gain over prior work by roughly 31\%.%In aggregate across evaluation settings, our method outperforms previously results by 4.3\% (11\% relative gain). % \begin{table*}[htbp]% \small% \centering% %\ra{1.2}% \begin{tabular}{@{}l lll c lll c lll@{}}% \toprule%       & \multicolumn{3}{c}{Scene Graph Detection} &  \phantom{} & \multicolumn{3}{c}{Scene Graph Classification} &  \phantom{} & \multicolumn{3}{c}{Predicate Classification} \\%       \cmidrule{2-4} \cmidrule{6-8} \cmidrule{10-12}% Model & R$@$20 & R$@$50  & R$@$100 && R$@$20  & R$@$50  & R$@$100  && R$@$20 & R$@$50  & R$@$100 \\% \midrule% \textsc{Freq} & 17.7 & 23.5 & 27.6 && 27.7 & 32.4 & 34.0 && 49.4 & 59.9 & 64.1  \\% \textsc{Freq-Overlap} & 20.1 & 26.2 & 30.1 && 29.3 & 32.3 & 32.9 && 53.6 & 60.6 & 62.2  \\% \model-\textsc{NoContext} & 21.0 & 26.2 & 29.0 && 31.9 & 34.8 & 35.5 && 57.0 & 63.7 & 65.6 \\% \model-\textsc{LeftRight} &21.4 & 27.2 & 30.3 && {\bf 32.9} & {\bf 35.8} & {\bf 36.5} && {\bf 58.5} & {\bf 65.2} & {\bf 67.1} \\% \model-\textsc{Confidence} &{\bf 21.7} & {\bf 27.3} & {\bf 30.5} && 32.6 & 35.4 & 36.1 && 58.2 & 65.1 & 67.0 \\% \model-\textsc{Size} & 21.6 & {\bf 27.3} & 30.4 && 32.2 & 35.0 & 35.7 && 58.0 & 64.9 & 66.8 \\% \model-\textsc{Random} & 21.6 & {\bf 27.3} & 30.4 && 32.5 & 35.5 & 36.2 && 58.1 & 65.1 & 66.9 \\% \bottomrule% \end{tabular}% \label{tab:ablations}% \caption{Ablations of our models.\vspace{-1mm}} % \end{table*}\paragraph{Ablations} To evaluate the effectiveness of our main model,~\model,~we consider several ablations in Table~\ref{tab:superresults}. In \model-\textsc{NoContext}, we predict objects based on the fixed detector, and feed non-contexualized embeddings of the head and tail label into Equation~\ref{eq:hadamard}. Our results suggest that there is signal in the vision features for edge predictions, as \model-\textsc{NoContext} improves over \textsc{Freq-Overlap}. Incorporating context is also important: 
%but that context is still important.
our full model~\model~improves by 1.2 mean points, with largest gains at the lowest recall threshold of R$@$20. \footnote{The larger improvement at the lower thresholds suggests that our models mostly improve on relationship ordering rather than classification. Indeed, it is often unnecessary to order relationships at the higher thresholds: 51\% of images have fewer than 50 candidates and 78\% have less than 100.}
We additionally validate the impact of the ordering method used, as discussed in Section~\ref{subsec:setup:model_details}; the results vary less than 0.3 recall points, suggesting that~\model~is robust to the RoI ordering scheme used. 

%achieving %a 1.3 point gain on detection, a 2.6 point gain on Scene Graph Classification, and a 4.9 point gain on Predicate Classification (all with Left-Right ordering), implying that our model effectively incorporates global context without losing gains from conditioning. %We get the best results when using \model: achieving %a 1.3 point gain on detection, a 2.6 point gain on Scene Graph Classification, and a 4.9 point gain on Predicate Classification (all with Left-Right ordering), implying that our model effectively incorporates global context without losing gains from conditioning. These results are relatively insensitive to ordering scheme: while \model-\textsc{Confidence} has a 0.3-point R@20 boost in detection, it is outperformed by 0.3 R@20 points in both classification settings.%Additionally, we observe that the ordering scheme used matters relatively little. Ordering by x-coordinate, as in \model-\textsc{LeftRight}, provides only a 0.3-point boost in R@20, for both classification settings over \model-\textsc{Confidence}, whereas \model-\textsc{Confidence} outperforms \model-\textsc{LeftRight} by 0.3 points in detection. %This takes the conditioning that is so effective in \textsc{Freq-Overlap} and \model-\textsc{NoContext} one step further: now, we can condition on the existence (and in Predicate Classification, the labels) of all objects in the scene.%For instance, if we see a scene of a road (as indicated by seeing cars or buses) then we will be incentivized to predict predicates such as ``person standing on sidewalk,'' while in other indoor scenes this might be more sparsely annotated. Our model also outperforms the baselines on Scene Graph detection at all levels, but particularly at the R@20 where it improves by 1 point.
\section{Qualitative Results}
\label{sec:qualitative}
\begin{figure*}[t]
    \vspace{-.3cm}
    \centering
    \includegraphics[scale=0.51]{qualitative.pdf}
    \caption{Qualitative examples from our model in the Scene Graph Detection setting. Green boxes are predicted and overlap with the ground truth, orange boxes are ground truth with no match. Green edges are true positives predicted by our model at the R@20 setting, orange edges are false negatives, and blue edges are false positives. Only predicted boxes that overlap with the ground truth are shown. }
    \vspace{-.4cm}
    \label{fig:examples}
\end{figure*}

Qualitative examples of our approach, shown in Figure~\ref{fig:examples}, suggest that~\model~ is able to induce graph motifs from detection context. Visual inspection of the results suggests that the method works even better than the quantitative results would imply, since many seemingly correct edges are predicted that do not exist in the ground truth.

%Qualitative examples of our approach, shown in Figure~\ref{fig:examples}, suggest that~\model~is able to induce graph motifs from detection context. When shown an image of a woman and man shaking hands, as shown in the upper left of Figure~\ref{fig:examples}, it correctly determines to connect each person with their clothing and body parts using possessive relations such as ``wearing,'' ``has,'' and ``on''. A similar pattern can be observed in the upper right of Figure~\ref{fig:examples}, where all detected objects are connected to the man with possessive relations.
There are two common failure cases of our model. The first, as exhibited by the middle left image in Figure~\ref{fig:examples} of a skateboarder carrying a surfboard, stems from predicate ambiguity (``wearing'' vs ``wears'').
%The model only gets one guess per pair of predicates, so if it guesses the wrong predicate (e.g. ``wearing'' instead of ``wears''), then it has no chance of recovery, even if the guessed predicate is semantically correct.
The second common failure case occurs when the detector fails, resulting in cascading failure to predict any edges to that object.
For example, the failure to predict ``house'' in the lower left image resulted in five false negative relations.
%For instance, in the lower left hand side of Figure~\ref{fig:examples}, the model fails to recognize ``house'' and therefore is unable to produce edges relating to it.
\section{Related Work}
\label{sec:relation_work}
\paragraph{Context}%copied from one of my papers, needs rewording.
Many methods have been proposed for modeling semantic context in object recognition~\cite{divvala2009empirical}. 
Our approach is most closely related to work that models object co-occurrence using graphical models to combine many sources of contextual information~\cite{rabinovich2007objects,galleguillos2010context,li2007,farhadi2010every}.
While our approach is a type of graphical model, it is unique in that it stages incorporation of context allowing for meaningful global context from large conditioning sets.

Actions and relations have been a particularly fruitful source of context~\cite{marszalek2009actions, yatskar_situation_2016}, especially when combined with pose to create human-object interactions~\cite{yao2010modeling,chao:iccv2015}. Recent work has shown that object layouts can provide sufficient context for captioning COCO images~\cite{obj2textEMNLP2017,mscoco}; our work suggests the same for parsing Visual Genome scene graphs.
%Yet on COCO~\cite{mscoco} based image collections, object layouts have been shown to provide sufficient context to predict captions~\cite{obj2textEMNLP2017}. Similarly, we find for Visual Genome, relations are a weak source of context. % and objects are sufficient.
Much of the context we derive could be interpreted as commonsense priors, which have commonly been extracted using auxiliary means~\cite{zhu2014reasoning, viske, neil, Yatskar_VCommonSense_16, emnlp17_zellers}. 
Yet for scene graphs, we are able to directly extract such knowledge.

\paragraph{Structured Models}
Structured models in visual understanding have been explored for language grounding, where language determines the graph structures involved in prediction~\cite{plummer2015flickr30k,fidler_grounding_graph, tellex2011approaching, hu2017learning}.
Our problem is different as we must reason over all possible graph structures. 
Deep sequential models have demonstrated strong performance for tasks such as captioning~\cite{larrycaption,msrcaption, googlecaption, stanfordcaption} and visual question answering~\cite{vqa1,vqa2,vqa3,vqa4,vqa5}, including for problems not traditionally not thought of as sequential, such as multilabel classification~\cite{cnnrnn16}. Indeed, graph linearization has worked surprisingly well for many problems in vision and language, such as generating image captions from object detections~\cite{obj2textEMNLP2017}, language parsing~\cite{vinyals2015grammar}, generating text from abstract meaning graphs~\cite{konstas2017neural}. Our work leverages the ability of RNNs to memorize long sequences in order to capture graph motifs in Visual Genome. Finally, recent works incorporate recurrent models into detection and segmentation~\cite{polygonrnn,ren2016end} and our methods contribute evidence that RNNs provide effective context for consecutive detection predictions.

\paragraph{Scene Graph Methods}
Several works have explored the role of priors by incorporating background language statistics~\cite{lu_visual_2016, Yu_2017_ICCV} or by attempting to preprocess scene graphs~\cite{zhang_learning_2016}.
Instead, we allow our model to directly learn to use scene graph priors effectively. 
Furthermore, recent graph-propagation methods were applied but converge quickly and bottle neck through edges, significantly limiting information exchange ~\cite{xu_scene_2017, li2017msdn, Dai2017DetectingVR, li2017vip}.
On the other hand, our method allows global exchange of information about context through conditioning and avoids uninformative edge predictions until the end.
Others have explored creating richer models between image regions, introducing new convolutional features and new objectives~\cite{DBLP:journals/corr/NewellD17,Zhang2017VisualTE, li2017msdn, liang_deep_2017}. Our work is complementary and instead focuses on the role of context. See the supplemental section for a comprehensive comparison to prior work. 
%creates a model that independently predicts individual relationships on the Visual Relationship Dataset, a subset of Visual Genome with 5000 annotated scene graphs. %Since their dataset is small, they find an improvement when incorporating priors from language. %\cite{xu_scene_2017} uses a model that is a generalization of \cite{lu_visual_2016}.%They model scene graph inference as a CRF and use the mean-field approximation to decompose the problem into two types of GRU units: one type for the edges, and one type for the nodes. %After running the graph-GRUs for three timesteps, each node GRU and edge GRU makes an independent prediction.%Based on our analysis, we hypothesize this is suboptimal for two reasons: first, their model implicitly bottlenecks information on the edge level, whereas Figure~\ref{fig:conditionals} shows that edge labels do not contain much information helpful for predicting node labels.%Second, their model makes an implicit independence assumption between edge labels and node labels, parameterizing them as being independent given graph context. %\cite{DBLP:journals/corr/NewellD17} presents a model that utilizes context in the form of a powerful neural architecture that upsamples and downsamples, called the ``Stacked Hourglass Network.''%This architecture then computes a heatmap over the entire image, which is then used to compose the graph. %Since ground truth boxes are passed in as features, this allows the model to predict multiple edges per box and thus violate the scene graph constraints.%\cite{liang_deep_2017} uses reinforcement learning to decompose the scene graph problem into several parts: their model traverses the image by object and obtains reward when it correctly predicts an object's edges.%However, their model does not achieve the type of conditioning that we do: namely, our edge predictions are conditioned on the existence of \emph{all objects} in the graph, while theirs are only conditioned on previous predictions.%\cite{Zhang2017VisualTE} 
\section{Conclusion}
\label{sec:conclusion}
We presented an analysis of the Visual Genome dataset showing that motifs are prevalent, and hence important to model.
Motivated by this analysis, we introduced strong baselines 
that improve over prior state-of-the-art models by modeling these intra-graph interactions, while mostly ignoring visual cues.
We also introduced our model \model~for capturing higher order structure and global interactions in scene graphs that achieves additional significant gains over our already strong baselines.


\section*{Acknowledgements}
\label{sec:acknowledgements}
We thank the anonymous reviewers along with Ali Farhadi and Roozbeh Mottaghi for their helpful feedback. This work is supported by the National Science Foundation Graduate Research Fellowship (DGE-1256082), the NSF grant (IIS-1524371, 1703166), DARPA CwC program through ARO (W911NF-15-1-0543), IARPA's DIVA grant, and gifts by Google and Facebook.
\section*{Supplemental}
\label{sec:supp}
\begin{table*}[!t]
\setlength\tabcolsep{2.5pt} % default value: 6pt
\small
\centering
\begin{tabular}{@{}l@{\hspace{0.2em}}l cc |cc |cc | cc |cc |cc |cc| cc @{}}
\toprule
& &\multicolumn{6}{c|}{Graph constraints} & \multicolumn{10}{c}{No graph constraints} \\ 
&& \multicolumn{2}{c|}{\textsc{SGDet}} & \multicolumn{2}{c|}{\textsc{SGCls}} & \multicolumn{2}{c|}{\textsc{PredCls}} & \multicolumn{2}{c|}{\textsc{SGDet}} & \multicolumn{2}{c|}{\textsc{SGCls}} & \multicolumn{2}{c|}{\textsc{PredCls}} &
\multicolumn{2}{c|}{\textsc{PhrDet}} & \multicolumn{2}{c}{\textsc{PredDet}} \\
& Model & \tiny{R$@$50}  & \tiny{R$@$100} & \tiny{R$@$50}  & \tiny{R$@$100}& \tiny{R$@$50}  & \tiny{R$@$100}& \tiny{R$@$50}  & \tiny{R$@$100}& \tiny{R$@$50}  & \tiny{R$@$100}& \tiny{R$@$50}  & \tiny{R$@$100}& \tiny{R$@$50}  & \tiny{R$@$100}& \tiny{R$@$50}  & \tiny{R$@$100} \\ \hline
\multirow{8}{*}{\rotatebox[origin=c]{90}{\cite{xu_scene_2017}'s split}} & \textsc{Vrd} \cite{lu_visual_2016}, from \cite{xu_scene_2017} & 0.3  & 0.5 & 11.8  & 14.1  & 27.9  & 35.0 &&&&&&&&&\\ 
&\sc Assoc. Embed 
\cite{DBLP:journals/corr/NewellD17} & 8.1 & 8.2 & 21.8 & 22.6 & 54.2 & 55.5 & 9.7 & 11.3 & 26.5 & 30.0 & 68.0 & 75.2&&&&\\ 
&\sc Message Passing \cite{xu_scene_2017}   & 3.4  & 4.2 &  21.7 & 24.4 & 44.8 & 53.0 &&&&&&&&&\\ 
&\sc Message Passing$+$ & 20.7 & 24.5 & 34.6 & 35.4 & 59.3 & 61.3 &  22.0 & 27.4 & 43.4 & 47.2 & 75.2 & 83.6 & 34.4 & 42.2 & 93.5 & 97.2 \\
&\textsc{Freq} & 23.5 & 27.6 & 32.4 & 34.0 & 59.9 & 64.1 &
25.3& 30.9& 40.5& 43.7& 71.3& 81.2&37.2&45.0& 88.3& 90.1 \\
&\textsc{Freq-Overlap} & 26.2 & 30.1 & 32.3 & 32.9 & 60.6 & 62.2 &
28.6& 34.4& 39.0& 43.4& 75.7& 82.9& 41.6& 49.9 & 94.6& 96.9\\ 
&\model-\textsc{NoContext} & 26.2 & 29.0 & 34.8 & 35.5 & 63.7 & 65.6 & 29.8 & 34.7 & 43.4 & 46.6 & 78.8 & 85.9 & 43.5 & 50.9 & 94.2 & 97.1 \\
&\model & {\bf 27.2} & {\bf 30.3} & {\bf 35.8} & {\bf 36.5} & {\bf 65.2} & {\bf 67.1} & {\bf 30.5} & {\bf 35.8} & {\bf 44.5} & {\bf 47.7} & {\bf 81.1} & {\bf 88.3} & {\bf 44.2} & {\bf 52.1} & {\bf 96.0} & {\bf 98.4} \\ \hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{\cite{li2017msdn} split}} &\textsc{MSDN} \cite{li2017msdn}$\star$  & 10.7 & 14.2 & 24.3 & 26.5 & 67.0 & 71.0 &&&& &&&&& \\ 
&\textsc{MSDN}  & 11.7 & 14.0 & 20.9 & 24.0 & 42.3 & 48.2 &&&& &&&&& \\ 
&\textsc{MSDN}-\textsc{Freq} & {\bf 13.5} & {\bf 15.7} & {\bf 25.8} & {\bf 27.8} & {\bf 56.0} & {\bf 61.0} &&&& &&&&& \\ 
&\textsc{SCR}\cite{li2017vip} &&&&&&&10.67 & 13.81 &&&&&16.58 & 21.54&&  \\
\hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{other split}} &\textsc{DR-Net}\cite{Dai2017DetectingVR} & &&&&&& 20.79 & 23.76 && & & & 23.95 & 27.57 & 88.26 & 91.26 \\
&\textsc{VRL}\cite{liang_deep_2017} &12.57 & 13.34&&&&&&& &&&&14.36 & 16.09 && \\
&\textsc{VTE}\cite{Zhang2017VisualTE}  &&&&&&& 5.52 & 6.04 &&&&& 9.46 & 10.45 && \\
&\textsc{LKD}\cite{Yu_2017_ICCV} &&&&&&& &  &&&&&&& 92.31  &  95.68 \\
\bottomrule
\end{tabular}
\caption{Results with and without scene graph constraints. Horizontal lines indicate different dataset preprocessing settings (the ``other split'' results, to the best of our knowledge, are reported on different splits). $\star$: \cite{li2017msdn} authors acknowledge that their paper results aren't reproducible for \textsc{SGCls} and \textsc{PredCls}; their current best reproducible numbers are one line below. \textsc{MSDN}-\textsc{Freq}: Results from using node prediction from \cite{li2017msdn} and edge prediction from \textsc{Freq}.
}\label{tab:supptable}
\end{table*}
Current work in scene graph parsing is largely inconsistent in terms of evaluation and experiments across papers are not completely comparable. 
In this supplementary material, we attempt to classify some of the differences and put the works together in the most comparable light. 
\subsection*{Setup}
In our paper, we compared against papers that (to the best of our knowledge) evaluated in the same way as \cite{xu_scene_2017}. 
Variation in evaluation consists of two types:
\begin{itemize}
    \item Custom data handling, such as creating paper-specific dataset splits, changing the data pre-processing, or using different label sets.
    \item Omitting graph constraints, namely, allowing a head-tail pair to have multiple edge labels in system output. We hypothesize that omitting graph constraints should always lead to higher numbers, since the model is then allowed multiple guesses for challenging objects and relations.
\end{itemize}
Table~\ref{tab:supptable} provides a best effort comprehensive review against all prior work that we are aware of. Other works also introduce slight variations in the tasks that are evaluated:\footnote{We use task names from \cite{lu_visual_2016}, despite inconsistency in whether the underlying task actually involves classification or detection.}\begin{itemize}
    \item \textbf{Predicate Detection} (\textsc{PredDet}). The model is given a list of labeled boxes, as in predicate classification, and a list of head-tail pairs that have edges in the ground truth (the model makes no edge predictions for head-tail pairs not in the ground truth).
    %. For each image, the model must produce a ranked list of candidate edges.
    \item \textbf{Phrase Detection} (\textsc{PhrDet}). The model must produce a set of objects and edges, as in scene graph detection.  An edge is counted as a match if the objects and predicate match the ground truth, with the IOU between the {\bf union-boxes} of the prediction and the ground truth over 0.5 (in contrast to scene graph detection where each object box must independently overlap with the corresponding ground truth box). 
\end{itemize}\subsection*{Models considered}
In Table~\ref{tab:supptable}, we list the following additional methods:
\begin{itemize}
    \item \textsc{MSDN} \cite{li2017msdn}: This model is an extension of the message passing idea from \cite{xu_scene_2017}. In addition to using an RPN to propose boxes for objects, an additional RPN is used to propose regions for captioning. The caption generator is trained using an additional loss on the annotated regions from Visual Genome.
    \item \textsc{MSDN-Freq}: To benchmark the performance on \cite{li2017msdn}'s split (with more aggressive preprocessing than \cite{xu_scene_2017} and with small objects removed), we evaluated a version of our \textsc{Freq} baseline in \cite{li2017msdn}'s codebase. We took a checkpoint from the authors and replaced all edge predictions with predictions from the training set statistics from  \cite{li2017msdn}'s split. 
    \item \textsc{SCR} \cite{li2017vip}: This model uses an RPN to generate triplet proposals. Messages are then passed between the head, tail, and predicate for each triplet.
    \item \textsc{DR-Net} \cite{Dai2017DetectingVR}: Similar to \cite{xu_scene_2017}, this model uses an object detector to propose regions, and then messages are passed between relationship components using an approximation to CRF inference. 
    \item \textsc{VRL} \cite{liang_deep_2017}: This model constructs a scene graph incrementally. During training, a reinforcement learning loss is used to reward the model when it predicts correct components.
    \item \textsc{VTE} \cite{Zhang2017VisualTE}: This model learns subject, predicate, and object embeddings. A margin loss is used to reward the model for predicting correct triplets over incorrect ones. \item \textsc{LKD} \cite{Yu_2017_ICCV}: This model uses word vectors to regularize a CNN that predicts relationship triplets.
\end{itemize}\subsection*{Summary}
The amount of variation in Table~\ref{tab:supptable} requires extremely cautious interpretation. 
As expected, removing graph constraints significantly increases reported performance and both predicate detection and phrase detection are significantly less challenging than predicate classification and scene graph detection, respectively.
On \cite{li2017msdn}'s split, the \textsc{MSDN-Freq} baseline outperforms \textsc{MSDN} on all evaluation settings, suggesting baseline is robust across alternative data settings.
In total, the results suggest that our model and baselines are at least competitive with other approaches on different configurations of the task.
\newpage


{\small
\bibliographystyle{ieee}
\bibliography{zoterobib}
}

\end{document}
