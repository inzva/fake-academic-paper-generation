\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage[caption=false]{subfig}
\usepackage{dsfont}
\usepackage[acronym]{glossaries}
\usepackage{tabularx}
\usepackage{icomma}
\usepackage{url}
%\usepackage[section]{placeins}
\DeclareMathOperator*{\argmax}{argmax}
\renewcommand{\floatpagefraction}{.8}%

\newcommand\todo[1]{\textcolor{red}{#1}}
%\newcommand\todo[1]{{#1}}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[noabbrev,capitalize]{cleveref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{805} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{\vspace{-24pt}Integrating domain knowledge:\\using hierarchies to improve deep classifiers}

\author{Clemens-Alexander Brust$^1$ and Joachim Denzler$^{1,2}$\\
$^1$Computer Vision Group, Friedrich Schiller University Jena\\
$^2$Michael Stifel Center Jena\\
Jena, Germany\\
{\tt\small \{clemens-alexander.brust,joachim.denzler\}@uni-jena.de}
}
\maketitle
% TEMPLATE ENDS HERE

%\newcommand{\eg}{\emph{e.g.} }
%\newcommand{\ie}{\emph{i.e.} }
%\newcommand{\etal}{\emph{et al.} }
\newacronym{dag}{DAG}{directed acyclic graph}
\newacronym{mlnp}{MLNP}{mandatory labeled node prediction}
\newacronym{anp}{ANP}{arbitrary node prediction}
\newacronym{cnn}{CNN}{convolutional neural network}


\begin{abstract}
One of the most prominent problems in machine learning in the age of deep learning is the availability of sufficiently large annotated datasets. While for standard problem domains (ImageNet classification), appropriate datasets exist, for specific domains, \eg classification of animal species, a long-tail distribution means that some classes are observed and annotated insufficiently. Challenges like iNaturalist show that there is a strong interest in species recognition. Acquiring additional labels can be prohibitively expensive. First, since domain experts need to be involved, and second, because acquisition of new data might be costly. Although there exist methods for data augmentation, which not always lead to better performance of the classifier, there is more additional information available that is to the best of our knowledge not exploited accordingly. 

In this paper, we propose to make use of existing class hierarchies like WordNet to integrate additional domain knowledge into classification. We encode the properties of such a class hierarchy into a probabilistic model. From there, we derive a special label encoding together with a corresponding loss function. Using a convolutional neural network, on the ImageNet and NABirds datasets our method offers a relative improvement of $10.4\%$ and $9.6\%$ in accuracy over the baseline respectively. After less than a third of training time, it is already able to match the baseline's fine-grained recognition performance. Both results show that our suggested method is efficient and effective.
\end{abstract}

\section{Introduction}
\begin{figure*}[t]
  \begin{center}
  \subfloat[Class Set]{%
    \includegraphics[scale=0.5]{figures/pdf/teaserb.pdf}%
    }%
  \subfloat[Class Hierarchy]{%
    \includegraphics[scale=0.5]{figures/pdf/teasera.pdf}%
    }%
  \caption{Comparison between a loose set of independent classes and a class hierarchy detailing inter-class relations.}
  \label{fig:teaser}
  \end{center}
\end{figure*}

In recent years, \glspl{cnn} have achieved outstanding performance
in a variety of machine learning tasks, especially in computer vision, such as
image classification \cite{Krizhevsky2012CNN,He2015Res} and semantic segmentation \cite{Long2014FCN}.
Training a \gls{cnn} from scratch in an end-to-end fashion not only requires considerable
computational resources and experience, but also large amounts of labeled training data \cite{Sun2017Data}.
Using pre-trained \gls{cnn} features \cite{Razavian2014Features}, adapting
existing \glspl{cnn} to new tasks \cite{Hoffman2014LSDA} or performing data augmentation
can reduce the need for
labeled training data, but may not always be applicable or effective.

For specific problem domains, \eg with a long-tailed distribution, the amount of labeled training
data available is not always sufficient for training a \gls{cnn}.
When unlabeled data already exists, which is not always the case,
active learning \cite{Settles2009ALS}
to select valuable instances for labeling may be applied.
However, labels still have to be procured which is not always feasible
because of the cost of domain experts.

Besides information from more training data, domain knowledge in the form of high-level information about the structure
of the problem can be considered. In contrast to annotations of training data, this kind of domain knowledge is already available
in many cases from projects like
iNaturalist \cite{VanHorn2017iNat}, Visual Genome \cite{Krishna2017VG}, Wikidata \cite{Vrandevcic2014Wikidata}
and WikiSpecies\footnote{\url{https://species.wikimedia.org/wiki/Main_Page}}.

In this paper, we use class hierarchies, \eg WordNet \cite{Fellbaum1998WordNet}, as
an example of domain knowledge. In contrast to approaches based on attributes, where annotations
are often expected to be per-image, class hierarchies offer the option of domain knowledge integration on the highest level with the least additional annotation effort.
We encode the properties of such a class hierarchy into a probabilistic model that is based on common assumptions around hierarchies. From there, we derive a special label encoding together with a corresponding loss function.
These components are applied to a \gls{cnn} and evaluated systematically. The construction could also be interpreted as
a special case of learning using privileged information (LUPI, \cite{Vapnik2009LUPI}).


%We propose a probabilistic
%model for hierarchical classification from which we derive a special loss function
%and a label encoding scheme for \glspl{cnn} as a concrete implementation.


% Contributions
Our main \textbf{contributions} are: (i) a deep learning method based on a probabilistic model to improve existing classifiers by adding a
class hierarchy which (ii) works with any form of hierarchy representable using a
\glsfirst{dag}, \ie does not require a tree hierarchy. We evaluate our method in
experiments on the CIFAR-100 \cite{Krizhevsky2009CIFAR}, ImageNet and NABirds \cite{VanHorn2015NAB} datasets to represent problem
domains of various scales.


% When training these classifiers, it is often implicitly assumed that all classes are independent
% of each other and unrelated. The widely used one-hot encoding
% in combination with a distance-based loss function represents this assumption.
%
% However, when considering the classification of everyday categories typical for
% benchmark datasets \cite{Russakovsky2015ImageNet,Krizhevsky2009CIFAR,Lin2014MSCOCO},
% this assumption does not generally hold. Certain categories are more visually similar
% than others, \eg english bulldog and frech bulldog vs. english bulldog and sports car.
% These relations is not reflected in the labels of a classification benchmark dataset.
% Semantic similarity, often associated with visual similarity, is also seldom considered.
% Both kinds of similarity are of practical importance to classification modeling, because
% ideally, they can be exploited to improve certain aspects of, or even overall, performance.
% While typical datasets often lack information on class similarity, relationships between classes
% can be extracted from other sources such as WordNet \cite{Fellbaum1998WordNet}
% and used in combination with the data. In this paper,
% this additional information is processed to create a class hierarchy on top of
% existing dataset classes (see \cref{fig:teaser}).
%
% % TODO Biodiv, Medical, Data Expensive Hierarchy Free etc.
% There are many applications where automated visual recognition can have a
% large impact, \eg biodiversity and medical research, but where labeled data
% is scarce and expensive because of the expertise required to create annotations.
% In these situations, augmenting small labeled datasets using domain knowledge such as
% class hierarchies is a reasonable proposition, especially in cases where the acquisition
% of new labeled data is prohibitively expensive while class hierarchies can often
% be found for free or estimated in a short amount of time.
%


\subsection{Related Work}
In the following, we describe how our work relates to existing work in the field
of hierarchical classification, where hierarchical data may be encountered
and how it relates to knowledge transfer.
\paragraph{Hierarchical Classification}
% HC by Silla et al.
The authors of \cite{Silla2011HClass} offer an extensive survey and a generalized view on hierarchical classification.
Most hierarchical classifiers can be categorized under their proposed notation. Criteria
include (i) how many classes may be predicted at once, (ii) whether non-leaf classes can be
predicted, (iii) if the underlying hierarchy is a tree or a \gls{dag} and (iv)
how the classifier's architecture maps to the hierarchy. A categorization scheme is also
proposed for the hierarchical classification problems themselves, where they are grouped by
(i) tree or \gls{dag} structure, (ii) how many classes are labeled per sample and (iii) whether
the labels are always as specific as possible.

Considering this categorization, our method has the following properties: (i) it can predict
many classes at once, but is limited to one for our experiments, (ii) it can predict
non-leaf classes (see \cref{sec:expmlnp}) if desired, (iii) it supports \glspl{dag} as underlying
hierarchies, not just trees, and (iv) it does not fit any of the described policies (see \cref{sec:loss}).
The problems tackled in our experiments are (i) defined by a \gls{dag} structure, (ii) have
one class label per sample and (iii) the labels are always as specific as possible.


\paragraph{Hierarchical Data}
% WordNet
Typical image classification datasets rarely offer hierarchical information. There are exceptions
such as the iNaturalist challenge dataset \cite{VanHorn2017iNat} where a class hierarchy is
derived from biological taxonomy. Exceptions also include specific hierarchical classification
benchmarks, \eg \cite{Partalas2015LSHTC,Torralba2008Tiny} as well as datasets where the labels
originate from a hierarchy such as ImageNet \cite{Deng2009ImageNet}.
The Visual Genome dataset \cite{Krishna2017VG} is another notable exception, with available metadata
including attributes, relationships, visual question answers, bounding boxes and more, all
mapped to elements from WordNet.

To augment existing non-hierarchical datasets, class hierarchies can be used.
For a typical object classification scenario, concepts from the WordNet database
\cite{Fellbaum1998WordNet} can be mapped to object classes. WordNet contains
nouns, verbs and adjectives that are grouped into \emph{synsets} of synonymous concepts.
Relations such as hyponymy (\textbf{is-a}), antonymy (\textbf{is-not}), troponymy (\textbf{is-a-way-of}) and meronymy (\textbf{is-part-of}) are encoded in a graph structure
where synsets are represented by nodes and relations by edges respectively. In this paper,
we use the hyponymy relation to infer a class hierarchy. A hyponym is a specific instance
of a more general concept, whereas a hyperonym is a more abstract notion.

\paragraph{Knowledge Transfer}
Adding domain knowledge in the form of a class hierarchy
can be seen as a special case of knowledge transfer, where multiple tasks are learned
simultaneously, sharing knowledge between tasks to ultimately improve
results, possibly because there are too few training examples for individual tasks.

Several methods of knowledge transfer between object classes
aimed at scalability towards large numbers of classes are presented in \cite{Rohrbach2011Zero}.
The authors note that while knowledge transfer does not generally improve classification
in settings where training data is available for all classes, it is valuable
in zero-shot learning scenarios \cite{Palatucci2009Zero}, where some classes do not have any labeled training examples.
One of their methods performs knowledge transfer based on the WordNet hierarchy underlying
the ImageNet challenge dataset they use. In a zero-shot setting, it outperforms other
methods based on part attributes and semantic similarity.

In \cite{Rodner2010OSL}, a method for learning from few examples using Gaussian processes
is presented. Knowledge is transferred between related object categories to improve performance
when only very few training examples are available. The relationships between categories
are extracted from the WordNet hierarchy \cite{Fellbaum1998WordNet} to guide the knowledge
transfer. The authors show improvements using their method compared to learning the categories
individually.

% Visual Genome


% \paragraph{Zero-Shot Learning from Hierarchies}
% % TODO Zero-shot learning Mensink2012 Rohrbach2011
% A related scenario where external hierarchical data is beneficial is zero-shot learning \cite{Palatucci2009Zero},
% which uses such information to predict novel classes not originally part of the training set.
% It is shown that the incorporation of external information enables prediction beyond random guesses
% without the use of actual training examples. This relates to our task, where learning is also improved
% using data other than labeled images. However, in our case, there is no missing data that is replaced
% with other knowledge.
%
% In \cite{Mensink2012Metric}, the authors use metric learning to enable large scale
% classification using k-nearest neighbor as well as nearest class mean classifiers.
% They evaluate a zero-shot learning setting on the ImageNet dataset \cite{Deng2009ImageNet},
% where they successfully estimate the class mean of novel, unseen classes using relation information from the
% ImageNet subset of the WordNet class hierarchy. Applying the learned metric, they can make predictions over
% new classes without any labeled training examples.
%



\section{Hierarchical Classification with Deep Classifiers}
In this section, we propose a method to adapt existing classifiers
to hierarchical classification. We start by acquiring a hierarchy and then define a probabilistic
model based on it. From this probabilistic model, we derive an encoding and a loss function
that can be used in a machine learning environment.

\subsection{Class Hierarchy}
For our model, we assume that a hierarchy of object categories is supplied,
\eg from a database such as WordNet\cite{Fellbaum1998WordNet} or WikiSpecies.
It is modeled in the form of a graph $W=(S,h)$, where $S$ denotes the set of all possible
object categories, called \emph{synsets} in the WordNet terminology.
% The terms class, category and synsets are used interchangeably.
These are the nodes of the graph. Note that $S$ is typically a superset of the dataset
categories $C \subseteq S$, since parent categories are included to connect existing categories,
\eg \texttt{vehicle} is a parent of \texttt{car} and \texttt{bus}, but not originally part
of the dataset.

A hyponymy relation $h \in S \times S$ over the classes, which can be
interpreted as directed edges in the graph, is also given. For example, $(s,s')\in h$ means that
$s'$ is a hyperonym of $s$, or $s$ is a hyponym of $s'$, meaning $s$ \textbf{is-a} $s'$.
In general, the \textbf{is-a} relation is transitive. However, WordNet only models direct
relationships between classes to keep the graph manageable and to represent different
levels of abstraction as graph distances. The relation is also irreflexive and
asymmetric. 

For the following section, we assume that $W$ is a \glsfirst{dag}. However,
the WordNet graph is commonly reduced to a tree, for example by using a voting algorithm \cite{Torralba2008Tiny}
or selecting task-specific subsets that are trees \cite{Deng2009ImageNet}. The supplementary material
outlines such a procedure. In this paper, we work on the \gls{dag} directly with no modifications needed.

\subsection{Probabilistic Model}
Elements of a class hierarchy are not always mutually exclusive, \eg a \texttt{corgi} is also a \texttt{dog} and an \texttt{animal} at the same time.
Hence, we do not model the class label as one categorical random variable, but
assume multiple independent Bernoulli variables $Y_{s}, s \in S$ instead. 
Formally, we model the probability of any class $s$ occurring independently (and thus allowing even multi-label
scenarios), given an example $x$:
\begin{equation}
P(Y_{s}=1 | X=x),
\end{equation}
or, more concisely,
\begin{equation}
P(Y_s^+ | X).
\label{eqn:themodel}
\end{equation}

The aforementioned model on its own is overly flexible considering the problem at hand, since
it allows for any combination of categories co-occurring. At this point, assumptions are similar
to those behind a one-hot encoding.
However, from the common definition of a hierarchy, we can infer a few additional properties to restrict the model.

\paragraph{Hierarchical decomposition}
A class $s$ can have many independent parents $S'=s'_1,\ldots,s'_n$. We choose $Y_{S'}^+$ to denote
an observation of at least one parent and $Y_{S'}^-$ to indicate that no parent class
has been observed:
\begin{eqnarray*}
Y_{S'}^+ & \Leftrightarrow Y_{s'_1}^+ \vee \ldots \vee Y_{s'_n}^+   \Leftrightarrow Y_{s'_1}=1 \vee \ldots \vee Y_{s'_n}=1,\\
Y_{S'}^- & \Leftrightarrow Y_{s'_1}^- \land \ldots \land Y_{s'_n}^- \Leftrightarrow Y_{s'_1}=0 \land \ldots \land Y_{s'_n}=0.
\end{eqnarray*}
Based on observations $Y_{S'}$, we can decompose the model from \cref{eqn:themodel}
in a way to capture the hierarchical nature. We start by assuming a marginalization
of the conditional part of the model over the parents $Y_{s'}$:
\begin{equation}
\begin{split}
P(Y_s^+|X) &= P(Y_s^+|X, Y_{S'}^+)P(Y_{S'}^+|X)\\
&+ P(Y_s^+|X, Y_{S'}^-)P(Y_{S'}^-|X).
\end{split}
\label{eqn:decompose}
\end{equation}
The details of this decomposition are given in the supplementary material.

\paragraph{Simplification}
We now constrain the model and add assumptions to better reflect the hierarchical problem.
If none of the parents $S'=s'_1,\ldots,s'_n$ of a class $s$ occur, we assume the
probability of $s$ being observed for any given example to be zero:
\begin{equation}
P(Y_s^+ | X, Y_{S'}^-) = P(Y_s^+ | Y_{S'}^-) = 0.
\label{eqn:hcritzero}
\end{equation}
This leads to a simpler hierarchical model, omitting the second half of \cref{eqn:decompose}
by setting it to zero:
\begin{equation}
P(Y_s^+|X) = P(Y_s^+|X, Y_{S'}^+)P(Y_{S'}^+|X).
\end{equation}

\paragraph{Parental independence}
To make use of recursion in our model, we require the random variables $Y_{s'_1}, \ldots, Y_{s'_n}$
to be independent of each other in a naive fashion. Using the definition of $Y_{S'}^+$, we
derive:
\begin{equation}
P(Y_{S'}^+|X) = 1 - \prod_{i=1}^{|S'|}{1-P(Y_{s'_i}^+|X)}.
\label{eqn:parindep}
\end{equation} 

\paragraph{Parentlessness}
In a non-empty \gls{dag}, we can expect there to be at least one node with no incoming edges, \ie a
class with no parents. In the case of WordNet, there is exactly one node with no parents, the root synset \texttt{entity.n.01}.
A marginalization over parent classes does not apply there. We assume that all observed
classes are children of \texttt{entity} and thus set the probability to one for a class without parents:
\begin{equation}
P(Y_s^+|X,S'=\emptyset) = 1.
\label{eqn:entity}
\end{equation}
Note that this is not reasonable for all hierarchical classification problems. If the
hierarchy is composed of many disjoint components, $P(Y_s^+|X,S'=\emptyset)$ should
be modeled explicitly. Even if there is only a single root, explicit modeling could
be used for tasks such as novelty detection.

\subsection{Inference}
\label{sec:inference}
The following section describes the details of the inference process in our model.

\paragraph{Restricted Model Outputs}
\label{sec:mlnp}
Depending on the setting, when the model is used for inference, the possible outputs can be restricted
to the classes $C$ that can actually occur in the dataset as opposed to all modeled
classes $S$ including parents that exist only in the hierarchy. This assumes a fixed class set at test time and does not apply to open-set problems.
We denote this setting \emph{\gls{mlnp}}. The unrestricted alternative is named \emph{\gls{anp}}.

\paragraph{Prediction} To predict a \emph{single} class $s$ given a specific example $x$, we look for the class where
the joint probability of the following observations is high: (i) the class $s$ itself occurring ($Y_s^+$)
and (ii) none of the children $S''=s''_1,\ldots,s''_m$ occurring ($Y_{S''}^-$):

\begin{equation}
    s(x) = \argmax_{s \in C \subseteq S}{P(Y_s^+|X)}P(Y_{S''}^-|X,Y_s^+). %\prod_{i=1}^{|'S|}{P(Y_{'s_i}^+|X,Y_s^+)}
\end{equation}
Requiring the children to
be pairwise independent similar to \cref{eqn:parindep}, inference is performed in the following way:
\begin{equation}
   s(x) =  \argmax_{s \in C \subseteq S}{P(Y_s^+|X)\prod_{i=1}^{|S''|}{1-P(Y_{s''_i}^+|X,Y_s^+)}}.
\end{equation}
Because $P(Y_s^+|X)$ can be decomposed according to \cref{eqn:decompose} and expressed as a product (cf. \cref{eqn:parindep}),
we infer using:
\begin{equation}
  \begin{split}
   s(x) = \argmax_{s \in C \subseteq S}\,&
   P(Y_s^+|X, Y_{S'}^+)\\
   &\cdot \underbrace{(1 - \prod_{i=1}^{|S'|}{1-P(Y_{s'_i}^+|X)})}_{\text{Parent nodes }S'}\\
   &\cdot \underbrace{\prod_{i=1}^{|S''|}{1-P(Y_{s''_i}^+|X,Y_s^+)}}_{\text{Child nodes }'S}.
   \label{eqn:argmax}
  \end{split}
\end{equation}
Again, $P(Y_{s'_i}^+|X)$ can be decomposed. This decomposition is performed recursively
following the scheme laid out in \cref{eqn:decompose} until a parentless node is reached,
where the probability is assumed to be one (cf. \cref{eqn:entity}) or modeled explicitly.

\subsection{Training}
\label{sec:loss}
\begin{figure}[t]
  \begin{center}
  \subfloat[Encoding $e(y)$]{%
    \includegraphics[scale=0.5]{figures/pdf/mdlenc.pdf}%
    }%
  \\
  \subfloat[Loss Mask $m(y)$]{%
    \includegraphics[scale=0.5]{figures/pdf/mdlmask.pdf}%
    }%
  \caption{Hierarchical encoding and loss mask for $y=\texttt{animal}$. Shaded
  nodes represent $1$ and light nodes $0$ respectively.}
  \label{fig:henc}
  \end{center}
\end{figure}
In this section, we describe how to implement our proposed model in a machine learning context.
Instead of modeling the probabilities $P(Y_s^+|X)$ for each class $s$ directly, we want to estimate
the conditional probabilities $P(Y_s^+|X, Y_{S'}^+)$. This changes each individual estimator's task
slightly, because it only needs to discriminate among siblings and not all classes. It also enables the implementation of
the hierarchical recursive inference used in \cref{eqn:argmax}.

The main components comprise of a label encoding $e: S \to \{0,1\}^{|S|}$ as well as a special loss function.
A label $y \in S$ is encoded using the hyponymy relation $h \in S \times S$, specifically its transitive
closure $\mathcal{T}(h)$, and the following function:
\begin{equation}
e(y)_s = \begin{cases}
1 & \text{if }y = s\text{ or }(y,s) \in \mathcal{T}(h),\\
0 & \text{otherwise}. 
\end{cases}
\end{equation}

A machine learning method can now be used to estimate encoded labels directly. However,
a suitable loss function needs to be provided such that the conditional nature of each
individual estimator is preserved. This means that, given a label $y$, a component $s$ should be trained only
if one of its parents $s'$ is related to the label $y$ by $\mathcal{T}(h)$, or if $y$ is one of its parents.
We encode this requirement using a \emph{loss mask} $m: S \to \{0,1\}^{|S|}$, defined by the
following equation:

\begin{equation}
m(y)_s = \begin{cases}
1 & \text{$y=s$ or} \\
 & \exists (s,s') \in h: y = s' \text{ or }(y,s') \in \mathcal{T}(h),\\
0 & \text{otherwise}. 
\end{cases}
\end{equation}

\Cref{fig:henc} visualizes the encoding $e(y)$ and the corresponding loss mask $m(y)$ for a
small example hierarchy. Using the encoding and loss mask, the
complete loss function $\mathcal{L}$ for a given data point $(x,y)$ and estimator $f: X \to \{0,1\}^{|S|}$ is then defined
by:
\begin{equation}
\mathcal{L}_f(x,y) = m(y)^T(e(y) - f(x))^2.
\end{equation}

The function $f(x)_s$ is then used to estimate the conditional probabilities $P(Y_s^+|X, Y_{S'}^+)$.
Applying the inference procedure in \cref{sec:inference}, a prediction is made
using the formula in \cref{eqn:argmax} and substituting $f(x)_s$ for $P(Y_s^+|X, Y_{S'}^+)$.

\section{Experiments and Evaluation}
The following section describes the setup of our experiments as well as the evaluation protocol.
We aim to assess the effects of applying our method on three different scales of problems using datasets
described in \cref{sec:expdata}. We also validate the requirement of restricting the predictions to
a known set of possible classes (see \cref{sec:inference}) as opposed to allowing any element within the hierarchy.

\subsection{Datasets}
\label{sec:expdata}
% CIFAR 100
\paragraph{CIFAR-100}
For our experiments, we want to work with a dataset that does not directly supply hierarchical labels,
but where we can reasonably assume that an underlying hierarchy exists.
The CIFAR-100 dataset \cite{Krizhevsky2009CIFAR} fulfills this requirement. It is an
object classification dataset composed of natural images. Because there are only 100
classes, each can be mapped to a specific synset in the WordNet hierarchy without
relying on potentially faulty automation.
%\Cref{tbl:mapping} shows our mapping between CIFAR-100 classes and WordNet synsets.
Our mapping between CIFAR-100 classes and WordNet synsets is detailed in the supplementary material.
% What happens after the mapping, how is the hierarchy extracted etc.

The target hierarchy is composed in three steps. First, the synsets mapped from all CIFAR-100
classes make up the foundation. Then, parents of the synsets are added in a recursive fashion.
With the nodes of the graph complete, directed edges are determined using the WordNet
hyponymy relation.

% Post-processing of the graph

Mappings are not always obvious or unique. For instance, the CIFAR dataset contains
\texttt{aquarium\_fish} -- a concept not captured by any WordNet synset.
Intuitively, one would expect all CIFAR classes to map to leaves in the resulting
hierarchy. This is true, with one exception. \texttt{dolphin} and \texttt{whale} have
a direct hyperonymy relationship in WordNet because technically, dolphins are whales.
As a consequence, classifiers that mandate prediction of only leaf nodes cannot be
used in this context. The labels need to be encoded in a manner that allows a sample
to be a \texttt{whale}, but not a \texttt{dolphin}.

Mapping all classes to the WordNet synsets results in 99 classes being mapped to leaf nodes
and one class to an inner node (\texttt{whale}). In total, there are 267 nodes as a result of
the recursive adding of hyperonyms. 

\paragraph{ImageNet} 
The aforementioned ambiguity in the mapping from dataset labels to WordNet synsets
can only be reduced to a certain degree. A complete solution would require a
dataset using WordNet as its label space. Because of WordNet's popularity, such
datasets exist, \eg ImageNet \cite{Deng2009ImageNet} and 80 Million Tiny Images \cite{Torralba2008Tiny}.
We use ImageNet, specifically the dataset of the 2012 ImageNet Large Scale Visual Recognition Challenge.

It contains around 1000 training images each for
1000 synsets. The categorization into synsets eliminates mapping from classes as a potential error source.
All 1000 synsets are leaf nodes in the resulting hierarchy with a total of 1860 nodes.

\paragraph{NABirds}
Quantifying performance on object recognition datasets such as CIFAR and
ImageNet is important to prove the general usefulness of a method. However, more niche applications
such as fine-grained recognition stand to benefit more from improvements because the availability
of labeled data is much more limited.
We use the NABirds dataset \cite{VanHorn2015NAB} to verify our method in a fine-grained recognition
setting. NABirds is a challenge where 555 categories of North American birds have to be differentiated.
These categories are comprised of 404 species as well as several variants of sex, age and plumage.
It contains 48,562 images split evenly into training and validation sets.

Offered annotations include not only image labels, but also bounding boxes and parts.
Additionally, a class hierarchy based on taxonomy is supplied. 
It contains 1010 nodes, where all of the 555 visual categories are leaf nodes.

\subsection{Experimental Setup}
\paragraph{Convolutional Neural Networks}
\label{sec:expsetup}
For our experiments on the CIFAR-100 dataset, we use a ResNet-32 \cite{He2015Res}
in the configuration originally designed for CIFAR. The network is initialized randomly as specified
in \cite{He2015Res}.

We use a minibatch size of 128 and the adaptive stochastic optimizer Adam \cite{Kingma2014Adam} with a
constant learning rate of $0.001$ as recommended by the authors. Although SGD can lead to better
performance of the final models, its learning rate is more dependent on the range of the loss function.
We choose an adaptive optimizer to minimize the influence of different ranges of loss values.

In our NABirds and ImageNet experiments, we use a ResNet-50 \cite{He2015Res,He2016Identity}
originally designed for ImageNet classification because of the larger image size and overall scale of the dataset.
The minibatch size is reduced to 64 and training is extended to $120,000$ steps for NABirds and $234,375$ steps for ImageNet.
We crop all images using the given bounding box annotations and resize them to $224\times 224$ pixels.

To improve generalization, all settings use random shifts of up to 4 pixels for CIFAR-100 and
up to 32 pixels for NABirds and ImageNet as well as random horizontal flips during training. All images
are normalized per channel to zero mean and standard deviation one, using parameters estimated over the
training data. Code will be published along with the paper.
We choose our ResNet-50 and ResNet-32 baselines to be able to judge effects across datasets, which would not be possible
when selecting a state-of-the-art method for each dataset individually. Furthermore, the moderately sized
architecture enables faster training and therefore more experimental runs compared to a high performing
design such as PNASNet \cite{Liu2017PNASNet}.

% Metrics
\paragraph{Evaluation}
We report the overall accuracy, not normalized w.r.t class instance counts,
averaged over different random initializations of the classifier.
Each experiment consists of six random initializations per method for the CIFAR-100
dataset and three for the larger-scale NABirds and ImageNet datasets.
We choose to compare the methods using a measure that does not
take hierarchy into account to gauge the effects of adding hierarchical data to a task
that is not normally evaluated with a specific hierarchy in mind. Using a hierarchical measure would
achieve the opposite: we would measure the loss sustained by omitting hierarchical data.

\subsection{Overall Improvement --- ImageNet}
\label{sec:exp-in}
\label{sec:exp-first}

\begin{figure}
\centering
\includegraphics{plots/mlnp-large-imagenet/accuracy.pdf}
\caption{Accuracy on the ImageNet validation set over time. Our hierarchical training method gains accuracy faster than the
flat classifier baseline. We report overall classification accuracy in percent.}
\label{fig:expingraph}
\end{figure}


\begin{table}
\centering
\caption{Results on the ImageNet validation set. We report classification accuracy in percent
as well as the number of optimization steps needed by the baseline method to reach comparable accuracy.}
\label{tbl:expin}
\small
\begin{tabular}{l||l|l||l|l}
 &  \multicolumn{2}{c||}{Accuracy (\%)} & \multicolumn{2}{c}{vs. Baseline}\\
Steps       &      Baseline     &       w/Hierarchy        & Steps & Speedup\\\hline

31250  &  20.5 $\pm$ 0.32  &  28.9 $\pm$ 0.06  &   62500  &  2.00 \\
46875  &  27.4 $\pm$ 0.20  &  35.8 $\pm$ 0.18  &   78125  &  1.67 \\
62500  &  32.3 $\pm$ 0.09  &  41.0 $\pm$ 0.33  &  125000  &  2.00 \\
93750  &  38.4 $\pm$ 0.19  &  45.9 $\pm$ 0.20  &  187500  &  2.00 \\
187500  &  46.9 $\pm$ 0.21  &  53.3 $\pm$ 0.42  &  ---  &  --- \\
234375  &  49.0 $\pm$ 0.33  &  54.2 $\pm$ 0.04  &  ---  &  --- \\


\end{tabular}
\end{table}


In this experiment, we quantify the effects of using our hierarchical classification method
to replace the common combination of one-hot encoding and mean squared error loss function.
% Dataset
We use ImageNet, specifically the ILSVRC2012 dataset. This is a classification challenge with 1000 classes
whose labels are
taken directly from the WordNet hierarchy of nouns. 

% Results
\Cref{fig:expingraph} shows the evolution over time of accuracy on the validation set.
After around 240,000 gradient steps, training converges. The one-hot baseline reaches
a final accuracy of $49.1\%$, while our method achieves $54.2\%$ with no changes to training
except for our loss function and hierarchical encoding. This is a relative improvement of $10.4\%$. There is no discernible difference
in computation times between both methods.

While an improvement of accuracy at the end of training is always welcome,
the effects of hierarchical classification more drastically show in the change in accuracy over
time. The strongest improvement is observed during the first training steps.
After training for 31250 steps using our method, the network already performs with $28.9\%$ accuracy.
The one-hot baseline matches this performance after 62500 gradient steps, taking twice as long.
The baseline's final accuracy of $49.1\%$ is matched by our method after only $125,000$ training
steps, resulting in an overall training speedup of $1.88\text{x}$.
We provide a more detailed picture of the initial training in \cref{tbl:expin}. Speedup indicates
how much longer the baseline needs to match our hierarchical method's performance.

Overall, our method performs better and learns more quickly at the same time with no significant
increase in computational effort.


\subsection{Speedup --- CIFAR-100}
\begin{figure}
\centering
\includegraphics{plots/mlnp-cifar100/speedup-A12-cifar100-dag-conditional-probabilistic-a-mlnp.pdf}
\caption{Initial Training speedup analysis comparing our hierarchical method to the one-hot baseline on CIFAR-100. For each training
step of our method, the speedup indicates how much longer the baseline needs to train to match our performance.}
\label{fig:expcifarspeedup}
\end{figure}

\begin{figure}
\centering
\includegraphics{plots/mlnp-cifar100/accuracy.pdf}
\caption{Results on the CIFAR-100 validation set. Our hierarchical training method gains accuracy faster than the
flat classifier baseline. We report overall classification accuracy in percent.}
\label{fig:expcifargraph}
\end{figure}

% Dataset

% Results
We report the accuracies on the validation set as they develop during training in
\cref{fig:expcifargraph}. As training converges, we observe almost no difference
between both methods, with our hierarchical method reaching $54.6\%$ and the one-hot
encoding baseline at $55.4\%$. However, the methods differ strongly in the way that accuracy is
achieved. After the first $500$ steps, our hierarchical classifier already predicts $10.7\%$ of
the validation set correctly, compared to the baseline's $2.8\%$. It takes the baseline another
1600 steps to match $10.7\%$, or $4.2$ times as many steps.

This advantage in training speed is very strong during initial training, but becomes smaller
over time. \Cref{fig:expcifarspeedup} details the observed speedup during initial training.
After the first half of training, the difference between both methods vanishes almost completely.

% Conclusion

\subsection{Fine-Grained Recognition --- NABirds}
\label{sec:exp-nabirds}
\begin{figure}
\centering
\includegraphics{plots/nabirds-all/accuracy.pdf}
\caption{Accuracy on the NABirds validation set over time.}
\label{fig:expnabirdsgraph}
\end{figure}


% Setup
To evaluate the performance of our hierarchical method in a more specific setting, we
use the NABirds dataset \cite{VanHorn2015NAB}, a fine-grained recognition challenge where
the task is to classify 555 visual categories of birds. The dataset supplies a hierarchy
which we use in this experiment.


% Results
We observe results similar to the ImageNet dataset (see \cref{sec:exp-in}), where
our method leads to an improvement in both training speed and overall accuracy.
\Cref{fig:expnabirdsgraph} shows the development of validation accuracy over time.
The one-hot baseline converges to an accuracy of $56.5\%$. Our hierarchical
classifier reaches $61.9\%$ after the full $120,000$ steps of training. It already
matches the baseline's final accuracy at $39,000$ iterations, reducing training
time to less than a third. The relative improvement when applying the
full training time is $9.6\%$.

% Conclusion
Overall, our method leads to a faster and better solution of the fine-grained recognition task,
while keeping the same classifier and requiring no discernible amount of addition computation time.

\subsection{Mandatory Labeled Node Prediction}
\label{sec:expmlnp}
\begin{figure}
\centering
\includegraphics{plots/all-large-imagenet/accuracy.pdf}
\caption{Effects of restricting the predicted classes. Graph shows validation performance on the ImageNet dataset.
  \Glsfirst{mlnp} offers better performance than \glsfirst{anp} throughout training.}
\label{fig:expmlnpgraph}
\end{figure}
\begin{table*}[t]
\centering
\caption{Results Overview.}
\label{tbl:expoverview}
\begin{tabular}{l|l||c|c||c|c}
 &  & \multicolumn{2}{c||}{Accuracy (\%)} & \multicolumn{2}{c}{Speedup w/Hierarchy}\\
Dataset       &  \# of classes  &     Baseline     &       w/Hierarchy        & Overall & Initial \\\hline
CIFAR-100     &  100     & $\textbf{55.4} \pm 0.84$ &  $54.6 \pm 1.03$ & ---            &  $7.00$\\
NABirds       &  555     & $56.5 \pm 0.49$ &  $\textbf{61.9} \pm 0.27$ & $3.08$ &  $10.00$\\
ImageNet (ILSVRC2012)  &  1000    & $49.1 \pm 0.33$  & $\textbf{54.2} \pm 0.04$ & $1.88$ & ---
\end{tabular} 
\end{table*}

To assess the effects of restricting the class set as specified in \cref{sec:mlnp}, we compare two variants
of our method by observing
the validation accuracy on ImageNet over time. The first, \glsfirst{mlnp}, restricts the possible
predictions of the classifier to the 1000 classes available in the original dataset. The test set does
not contain samples of any other class. The second variant, \glsfirst{anp}, removes this restriction and
can predict any of the 1860 classes present in the complete hierarchy. This includes inner nodes as
well as the root.

This distinction does not affect training, as both variants use the same loss function given in
\cref{sec:loss}. The only difference is the set of classes over which the maximum is calculated (cf.~\cref{sec:mlnp}).
during inference. As a consequence, the accuracy of \gls{anp} cannot be better
than \gls{mlnp}, since any prediction of a non-dataset class is an error.

\Cref{fig:expmlnpgraph} shows the results of this experiment, including the one-hot baseline for
comparison. After training is complete, \gls{mlnp} offers a validation accuracy of $54.2\%$.
\Gls{anp} finishes training with $49.1\%$ validation accuracy, performing comparably to the one-hot baseline at $49.1\%$.
The improvement in performance from adding the \gls{mlnp} restriction over \gls{anp} is $5.1$ percent points, leading to a relative increase in
accuracy of around $10.4\%$.

This result illustrates the benefit of knowing that the test data is restricted
to a specific set of classes, as is the case when our method is employed to augment a classifier
for an existing problem with a predefined label space. It might still be interesting
to observe the unrestricted output and interpret its hierarchical depth as an indicator of confidence.
%FloatBarrier

\subsection{Overview}
\label{sec:exp-overview}

This section provides an overview, combining the results over the different datasets.
\Cref{tbl:expoverview} provides the most important facts for each dataset. We report
the accuracy at the end of training for the one-hot encoding baseline as well as our
method. Speedup is quantified in two ways. Overall speedup indicates how much faster
our hierarchical method achieves the end-of-training accuracy of the baseline,
measured in number of training steps. Initial speedup looks at the accuracy
delivered by our method after the first validation interval. We then measure
how much longer the baseline needs to match that number.

On all 3 datasets, the initial training is faster using our method. However,
we only observe an improvement in classification accuracy on ImageNet
and NABirds. With CIFAR-100, the benefits of adding the hierarchical information
to the training are limited to the speed of learning. There are a few possible
explanations for this observation:

The CIFAR-100 dataset is the only dataset that requires a manual mapping to
an external hierarchy, whereas the other datasets either supply one or have
labels directly derived from one. The manual mapping is a possible error source
and as such, could explain the observation.

The second possible reason lies in the difference between semantic similarity
and visual similarity \cite{Deselaers2011Visual}. Semantic similarity relates two classes using their
meaning. It can be extracted from hierarchies such as WordNet \cite{Fellbaum1998WordNet},
for example by looking at distances in the graph. Other relationships can also
be used to establish semantic distances, \eg synonymy, troponymy or meronymy.
Visual similarity on the other hand relates images that look alike, regardless
of the meaning behind them. When classifying, we group images by semantic similarity
because they contain objects of the same class, even if they share no visual characteristics,
possibly making the classification problem more complex. This may be exacerbated when adding
more information based on semantics and not properties of the images themselves.

% AUC?


\section{Conclusion}
We present a method to modify existing deep classifiers such that knowledge
about relationships between classes can be integrated. The method is derived
from a probabilistic model that is itself based on our understanding of the
meaning of hierarchies. Overall, it is just one example of the integration
of domain knowledge in an otherwise general method. One could also consider
our method a special case of learning using privileged information
\cite{Vapnik2009LUPI}.

Our method can improve classifiers by utilizing information that is freely
available in many cases such as WordNet \cite{Fellbaum1998WordNet} or
WikiSpecies.
There are also datasets which include a hierarchy that is ready to use with
very little effort \cite{Deng2009ImageNet,VanHorn2015NAB}.
The additional computational expense of using our method in the context
of deep learning is negligible.

% TODO CamReady We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPUs used for this research.



% TEMPLATE BEGINS AGAIN HERE


{\small
\bibliographystyle{ieee}
\bibliography{paper}
}


\end{document}

% \subsection{Training}
%
%
% \subsection{Notation}
% \begin{itemize}
% \item $S$: set of all synsets
% \item $h \in S \times S$: hyperonymy relation. asymmetric, irreflexive, not transitive
% \item $(x, s) \in \mathcal{L}$: dataset, pair of data and synset
% \item $e_{\mathcal{Y}}(i)$: one-hot vector in $\mathcal{Y}$ with 1 for $i$-th component and 0 everywhere else
% \end{itemize}
%
% \subsection{Conditional Embedding}
% \begin{itemize}
% \item $S' \subseteq S$: synsets that occur in data and all hyperonyms thereof
% \item $L \subset S'$: leaf synsets; only synsets that occur in data
% \item $I: S \to \{0, \ldots, |S'|\}$: synset indices
% \item $\mathcal{Y} = [0,1]^{|S'|}$: encoded synset space
% \end{itemize}
%
% Encoding function $y: S' \to \mathcal{Y}$. $h^+$ denotes the transitive closure of $h$.
% \begin{equation}
% y(s) = \underbrace{\vphantom{\sum_{(s',s) \in h}x} e_{\mathcal{Y}}(I(s))}_{\text{synset }s} +
% \underbrace{\sum_{(s',s) \in h^+} e_{\mathcal{Y}}(I(s'))}_{\text{all hyperonyms } s' \text{ of }s}
% \end{equation}
%
% Loss function: $L: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}$
% \begin{equation}
% L(y, \hat{y}) = \Big|
% \underbrace{\vphantom{\sum_{s'}x}(y-\hat{y})^T}_{\textrm{objective}}
% \underbrace{\sum_{s'}{\max_{(s'',s') \in h}{y_{I(s'')}}+\mathds{1}_{\{s'\textrm{ is root}\}}}}
% _{\textrm{loss mask}} \Big|^2_2
% \end{equation}
