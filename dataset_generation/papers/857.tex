\documentclass{article} %
\usepackage{iclr2019_conference,times}

\usepackage{amsmath,amsfonts,bm}\newcommand{\figleft}{{\em (Left)}}\newcommand{\figcenter}{{\em (Center)}}\newcommand{\figright}{{\em (Right)}}\newcommand{\figtop}{{\em (Top)}}\newcommand{\figbottom}{{\em (Bottom)}}\newcommand{\captiona}{{\em (a)}}\newcommand{\captionb}{{\em (b)}}\newcommand{\captionc}{{\em (c)}}\newcommand{\captiond}{{\em (d)}}\newcommand{\newterm}[1]{{\bf #1}}\def\figref#1{figure~\ref{#1}}\def\Figref#1{Figure~\ref{#1}}\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}\def\secref#1{section~\ref{#1}}\def\Secref#1{Section~\ref{#1}}\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}\def\eqref#1{equation~\ref{#1}}\def\Eqref#1{Equation~\ref{#1}}\def\plaineqref#1{\ref{#1}}\def\chapref#1{chapter~\ref{#1}}\def\Chapref#1{Chapter~\ref{#1}}\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}\def\algref#1{algorithm~\ref{#1}}\def\Algref#1{Algorithm~\ref{#1}}\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}\def\partref#1{part~\ref{#1}}\def\Partref#1{Part~\ref{#1}}\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}\def\ceil#1{\lceil #1 \rceil}\def\floor#1{\lfloor #1 \rfloor}\def\1{\bm{1}}\newcommand{\train}{\mathcal{D}}\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}\def\eps{{\epsilon}}\def\reta{{\textnormal{$\eta$}}}\def\ra{{\textnormal{a}}}\def\rb{{\textnormal{b}}}\def\rc{{\textnormal{c}}}\def\rd{{\textnormal{d}}}\def\re{{\textnormal{e}}}\def\rf{{\textnormal{f}}}\def\rg{{\textnormal{g}}}\def\rh{{\textnormal{h}}}\def\ri{{\textnormal{i}}}\def\rj{{\textnormal{j}}}\def\rk{{\textnormal{k}}}\def\rl{{\textnormal{l}}}\def\rn{{\textnormal{n}}}\def\ro{{\textnormal{o}}}\def\rp{{\textnormal{p}}}\def\rq{{\textnormal{q}}}\def\rr{{\textnormal{r}}}\def\rs{{\textnormal{s}}}\def\rt{{\textnormal{t}}}\def\ru{{\textnormal{u}}}\def\rv{{\textnormal{v}}}\def\rw{{\textnormal{w}}}\def\rx{{\textnormal{x}}}\def\ry{{\textnormal{y}}}\def\rz{{\textnormal{z}}}\def\rvepsilon{{\mathbf{\epsilon}}}\def\rvtheta{{\mathbf{\theta}}}\def\rva{{\mathbf{a}}}\def\rvb{{\mathbf{b}}}\def\rvc{{\mathbf{c}}}\def\rvd{{\mathbf{d}}}\def\rve{{\mathbf{e}}}\def\rvf{{\mathbf{f}}}\def\rvg{{\mathbf{g}}}\def\rvh{{\mathbf{h}}}\def\rvu{{\mathbf{i}}}\def\rvj{{\mathbf{j}}}\def\rvk{{\mathbf{k}}}\def\rvl{{\mathbf{l}}}\def\rvm{{\mathbf{m}}}\def\rvn{{\mathbf{n}}}\def\rvo{{\mathbf{o}}}\def\rvp{{\mathbf{p}}}\def\rvq{{\mathbf{q}}}\def\rvr{{\mathbf{r}}}\def\rvs{{\mathbf{s}}}\def\rvt{{\mathbf{t}}}\def\rvu{{\mathbf{u}}}\def\rvv{{\mathbf{v}}}\def\rvw{{\mathbf{w}}}\def\rvx{{\mathbf{x}}}\def\rvy{{\mathbf{y}}}\def\rvz{{\mathbf{z}}}\def\erva{{\textnormal{a}}}\def\ervb{{\textnormal{b}}}\def\ervc{{\textnormal{c}}}\def\ervd{{\textnormal{d}}}\def\erve{{\textnormal{e}}}\def\ervf{{\textnormal{f}}}\def\ervg{{\textnormal{g}}}\def\ervh{{\textnormal{h}}}\def\ervi{{\textnormal{i}}}\def\ervj{{\textnormal{j}}}\def\ervk{{\textnormal{k}}}\def\ervl{{\textnormal{l}}}\def\ervm{{\textnormal{m}}}\def\ervn{{\textnormal{n}}}\def\ervo{{\textnormal{o}}}\def\ervp{{\textnormal{p}}}\def\ervq{{\textnormal{q}}}\def\ervr{{\textnormal{r}}}\def\ervs{{\textnormal{s}}}\def\ervt{{\textnormal{t}}}\def\ervu{{\textnormal{u}}}\def\ervv{{\textnormal{v}}}\def\ervw{{\textnormal{w}}}\def\ervx{{\textnormal{x}}}\def\ervy{{\textnormal{y}}}\def\ervz{{\textnormal{z}}}\def\rmA{{\mathbf{A}}}\def\rmB{{\mathbf{B}}}\def\rmC{{\mathbf{C}}}\def\rmD{{\mathbf{D}}}\def\rmE{{\mathbf{E}}}\def\rmF{{\mathbf{F}}}\def\rmG{{\mathbf{G}}}\def\rmH{{\mathbf{H}}}\def\rmI{{\mathbf{I}}}\def\rmJ{{\mathbf{J}}}\def\rmK{{\mathbf{K}}}\def\rmL{{\mathbf{L}}}\def\rmM{{\mathbf{M}}}\def\rmN{{\mathbf{N}}}\def\rmO{{\mathbf{O}}}\def\rmP{{\mathbf{P}}}\def\rmQ{{\mathbf{Q}}}\def\rmR{{\mathbf{R}}}\def\rmS{{\mathbf{S}}}\def\rmT{{\mathbf{T}}}\def\rmU{{\mathbf{U}}}\def\rmV{{\mathbf{V}}}\def\rmW{{\mathbf{W}}}\def\rmX{{\mathbf{X}}}\def\rmY{{\mathbf{Y}}}\def\rmZ{{\mathbf{Z}}}\def\ermA{{\textnormal{A}}}\def\ermB{{\textnormal{B}}}\def\ermC{{\textnormal{C}}}\def\ermD{{\textnormal{D}}}\def\ermE{{\textnormal{E}}}\def\ermF{{\textnormal{F}}}\def\ermG{{\textnormal{G}}}\def\ermH{{\textnormal{H}}}\def\ermI{{\textnormal{I}}}\def\ermJ{{\textnormal{J}}}\def\ermK{{\textnormal{K}}}\def\ermL{{\textnormal{L}}}\def\ermM{{\textnormal{M}}}\def\ermN{{\textnormal{N}}}\def\ermO{{\textnormal{O}}}\def\ermP{{\textnormal{P}}}\def\ermQ{{\textnormal{Q}}}\def\ermR{{\textnormal{R}}}\def\ermS{{\textnormal{S}}}\def\ermT{{\textnormal{T}}}\def\ermU{{\textnormal{U}}}\def\ermV{{\textnormal{V}}}\def\ermW{{\textnormal{W}}}\def\ermX{{\textnormal{X}}}\def\ermY{{\textnormal{Y}}}\def\ermZ{{\textnormal{Z}}}\def\vzero{{\bm{0}}}\def\vone{{\bm{1}}}\def\vmu{{\bm{\mu}}}\def\vtheta{{\bm{\theta}}}\def\va{{\bm{a}}}\def\vb{{\bm{b}}}\def\vc{{\bm{c}}}\def\vd{{\bm{d}}}\def\ve{{\bm{e}}}\def\vf{{\bm{f}}}\def\vg{{\bm{g}}}\def\vh{{\bm{h}}}\def\vi{{\bm{i}}}\def\vj{{\bm{j}}}\def\vk{{\bm{k}}}\def\vl{{\bm{l}}}\def\vm{{\bm{m}}}\def\vn{{\bm{n}}}\def\vo{{\bm{o}}}\def\vp{{\bm{p}}}\def\vq{{\bm{q}}}\def\vr{{\bm{r}}}\def\vs{{\bm{s}}}\def\vt{{\bm{t}}}\def\vu{{\bm{u}}}\def\vv{{\bm{v}}}\def\vw{{\bm{w}}}\def\vx{{\bm{x}}}\def\vy{{\bm{y}}}\def\vz{{\bm{z}}}\def\evalpha{{\alpha}}\def\evbeta{{\beta}}\def\evepsilon{{\epsilon}}\def\evlambda{{\lambda}}\def\evomega{{\omega}}\def\evmu{{\mu}}\def\evpsi{{\psi}}\def\evsigma{{\sigma}}\def\evtheta{{\theta}}\def\eva{{a}}\def\evb{{b}}\def\evc{{c}}\def\evd{{d}}\def\eve{{e}}\def\evf{{f}}\def\evg{{g}}\def\evh{{h}}\def\evi{{i}}\def\evj{{j}}\def\evk{{k}}\def\evl{{l}}\def\evm{{m}}\def\evn{{n}}\def\evo{{o}}\def\evp{{p}}\def\evq{{q}}\def\evr{{r}}\def\evs{{s}}\def\evt{{t}}\def\evu{{u}}\def\evv{{v}}\def\evw{{w}}\def\evx{{x}}\def\evy{{y}}\def\evz{{z}}\def\mA{{\bm{A}}}\def\mB{{\bm{B}}}\def\mC{{\bm{C}}}\def\mD{{\bm{D}}}\def\mE{{\bm{E}}}\def\mF{{\bm{F}}}\def\mG{{\bm{G}}}\def\mH{{\bm{H}}}\def\mI{{\bm{I}}}\def\mJ{{\bm{J}}}\def\mK{{\bm{K}}}\def\mL{{\bm{L}}}\def\mM{{\bm{M}}}\def\mN{{\bm{N}}}\def\mO{{\bm{O}}}\def\mP{{\bm{P}}}\def\mQ{{\bm{Q}}}\def\mR{{\bm{R}}}\def\mS{{\bm{S}}}\def\mT{{\bm{T}}}\def\mU{{\bm{U}}}\def\mV{{\bm{V}}}\def\mW{{\bm{W}}}\def\mX{{\bm{X}}}\def\mY{{\bm{Y}}}\def\mZ{{\bm{Z}}}\def\mBeta{{\bm{\beta}}}\def\mPhi{{\bm{\Phi}}}\def\mLambda{{\bm{\Lambda}}}\def\mSigma{{\bm{\Sigma}}}\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}\def\tA{{\tens{A}}}\def\tB{{\tens{B}}}\def\tC{{\tens{C}}}\def\tD{{\tens{D}}}\def\tE{{\tens{E}}}\def\tF{{\tens{F}}}\def\tG{{\tens{G}}}\def\tH{{\tens{H}}}\def\tI{{\tens{I}}}\def\tJ{{\tens{J}}}\def\tK{{\tens{K}}}\def\tL{{\tens{L}}}\def\tM{{\tens{M}}}\def\tN{{\tens{N}}}\def\tO{{\tens{O}}}\def\tP{{\tens{P}}}\def\tQ{{\tens{Q}}}\def\tR{{\tens{R}}}\def\tS{{\tens{S}}}\def\tT{{\tens{T}}}\def\tU{{\tens{U}}}\def\tV{{\tens{V}}}\def\tW{{\tens{W}}}\def\tX{{\tens{X}}}\def\tY{{\tens{Y}}}\def\tZ{{\tens{Z}}}\def\gA{{\mathcal{A}}}\def\gB{{\mathcal{B}}}\def\gC{{\mathcal{C}}}\def\gD{{\mathcal{D}}}\def\gE{{\mathcal{E}}}\def\gF{{\mathcal{F}}}\def\gG{{\mathcal{G}}}\def\gH{{\mathcal{H}}}\def\gI{{\mathcal{I}}}\def\gJ{{\mathcal{J}}}\def\gK{{\mathcal{K}}}\def\gL{{\mathcal{L}}}\def\gM{{\mathcal{M}}}\def\gN{{\mathcal{N}}}\def\gO{{\mathcal{O}}}\def\gP{{\mathcal{P}}}\def\gQ{{\mathcal{Q}}}\def\gR{{\mathcal{R}}}\def\gS{{\mathcal{S}}}\def\gT{{\mathcal{T}}}\def\gU{{\mathcal{U}}}\def\gV{{\mathcal{V}}}\def\gW{{\mathcal{W}}}\def\gX{{\mathcal{X}}}\def\gY{{\mathcal{Y}}}\def\gZ{{\mathcal{Z}}}\def\sA{{\mathbb{A}}}\def\sB{{\mathbb{B}}}\def\sC{{\mathbb{C}}}\def\sD{{\mathbb{D}}}\def\sF{{\mathbb{F}}}\def\sG{{\mathbb{G}}}\def\sH{{\mathbb{H}}}\def\sI{{\mathbb{I}}}\def\sJ{{\mathbb{J}}}\def\sK{{\mathbb{K}}}\def\sL{{\mathbb{L}}}\def\sM{{\mathbb{M}}}\def\sN{{\mathbb{N}}}\def\sO{{\mathbb{O}}}\def\sP{{\mathbb{P}}}\def\sQ{{\mathbb{Q}}}\def\sR{{\mathbb{R}}}\def\sS{{\mathbb{S}}}\def\sT{{\mathbb{T}}}\def\sU{{\mathbb{U}}}\def\sV{{\mathbb{V}}}\def\sW{{\mathbb{W}}}\def\sX{{\mathbb{X}}}\def\sY{{\mathbb{Y}}}\def\sZ{{\mathbb{Z}}}\def\emLambda{{\Lambda}}\def\emA{{A}}\def\emB{{B}}\def\emC{{C}}\def\emD{{D}}\def\emE{{E}}\def\emF{{F}}\def\emG{{G}}\def\emH{{H}}\def\emI{{I}}\def\emJ{{J}}\def\emK{{K}}\def\emL{{L}}\def\emM{{M}}\def\emN{{N}}\def\emO{{O}}\def\emP{{P}}\def\emQ{{Q}}\def\emR{{R}}\def\emS{{S}}\def\emT{{T}}\def\emU{{U}}\def\emV{{V}}\def\emW{{W}}\def\emX{{X}}\def\emY{{Y}}\def\emZ{{Z}}\def\emSigma{{\Sigma}}\newcommand{\etens}[1]{\mathsfit{#1}}\def\etLambda{{\etens{\Lambda}}}\def\etA{{\etens{A}}}\def\etB{{\etens{B}}}\def\etC{{\etens{C}}}\def\etD{{\etens{D}}}\def\etE{{\etens{E}}}\def\etF{{\etens{F}}}\def\etG{{\etens{G}}}\def\etH{{\etens{H}}}\def\etI{{\etens{I}}}\def\etJ{{\etens{J}}}\def\etK{{\etens{K}}}\def\etL{{\etens{L}}}\def\etM{{\etens{M}}}\def\etN{{\etens{N}}}\def\etO{{\etens{O}}}\def\etP{{\etens{P}}}\def\etQ{{\etens{Q}}}\def\etR{{\etens{R}}}\def\etS{{\etens{S}}}\def\etT{{\etens{T}}}\def\etU{{\etens{U}}}\def\etV{{\etens{V}}}\def\etW{{\etens{W}}}\def\etX{{\etens{X}}}\def\etY{{\etens{Y}}}\def\etZ{{\etens{Z}}}\newcommand{\pdata}{p_{\rm{data}}}\newcommand{\ptrain}{\hat{p}_{\rm{data}}}\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}\newcommand{\pmodel}{p_{\rm{model}}}\newcommand{\Pmodel}{P_{\rm{model}}}\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}\newcommand{\pencode}{p_{\rm{encoder}}}\newcommand{\pdecode}{p_{\rm{decoder}}}\newcommand{\precons}{p_{\rm{reconstruct}}}\newcommand{\laplace}{\mathrm{Laplace}}%\newcommand{\E}{\mathbb{E}}\newcommand{\Ls}{\mathcal{L}}\newcommand{\R}{\mathbb{R}}\newcommand{\emp}{\tilde{p}}\newcommand{\lr}{\alpha}\newcommand{\reg}{\lambda}\newcommand{\rect}{\mathrm{rectifier}}\newcommand{\softmax}{\mathrm{softmax}}\newcommand{\sigmoid}{\sigma}\newcommand{\softplus}{\zeta}\newcommand{\KL}{D_{\mathrm{KL}}}\newcommand{\Var}{\mathrm{Var}}\newcommand{\standarderror}{\mathrm{SE}}\newcommand{\Cov}{\mathrm{Cov}}\newcommand{\normlzero}{L^0}\newcommand{\normlone}{L^1}\newcommand{\normltwo}{L^2}\newcommand{\normlp}{L^p}\newcommand{\normmax}{L^\infty}\newcommand{\parents}{Pa}%\DeclareMathOperator*{\argmax}{arg\,max}\DeclareMathOperator*{\argmin}{arg\,min}\DeclareMathOperator{\sign}{sign}\DeclareMathOperator{\Tr}{Tr}\let\ab\allowbreak

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{graphicx}		%
\usepackage{amsmath}
\usepackage{color}
\usepackage{algorithm}
\usepackage[]{algpseudocode}
\usepackage{bm}
\usepackage[binary-units = true]{siunitx}
\usepackage{mathtools}
\usepackage{ulem}           %
\usepackage{wrapfig}

\usepackage{animate}

\newcommand{\todo}[1]{\textcolor{red}{\bf #1}}
\newcommand{\OR}[1]{\textcolor{blue}{O: \bf #1}}
\newcommand{\KP}[1]{\textcolor{magenta}{K: \bf #1}}

\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}        %
  


\title{Learning what you can do \\ before doing anything}



\author{Oleh Rybkin\thanks{Equal contribution. Ordering determined by a coin flip.}$^{*,1}$, Karl Pertsch$^{*,2}$ \\
\textbf{Konstantinos G. Derpanis$^{3,4}$, Kostas Daniilidis$^{1}$, Andrew Jaegle$^{1}$}  \\
$^{1}$University of Pennsylvania\\
$^{2}$University of Southern California \\
$^{3}$Ryerson University \\
$^{4}$Samsung AI Centre Toronto \\
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy %
\begin{document}


\maketitle

\begin{abstract}
Intelligent agents can learn to represent the action spaces of other agents simply by observing them act. Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences. 
In this work, we address the problem of learning an agent’s action space purely from visual observation. We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content. 
We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions. We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP) . We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings. When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels.\footnote{Project website: \url{https://daniilidis-group.github.io/learned_action_spaces}}
\end{abstract}


\section{Introduction}


Agents behaving in real-world environments rely on perception to judge what actions they can take and what effect these actions will have. Purely perceptual learning may play an important role in how these action representations are acquired and used. In this work, we focus on the problem of learning an agent’s action space from unlabeled visual observations. To see the usefulness of this strategy, consider an infant that is first learning to walk. From around 10 months of age, infants rapidly progress from crawling, to irregular gaits with frequent falling, and finally to reliable locomotion (\cite{Adolph2012Walk}). 
But before they first attempt to walk, infants have extensive sensory exposure to adults walking. Unsupervised learning from sensory experience of this type appears to play a critical role in how humans acquire representations of actions before they can reliably reproduce the corresponding behaviour (\cite{Ullman18215}). Infants need to relate the set of motor primitives they can generate to the action spaces exploited by adults (\cite{Dominici2011Locomotor}), and a representation acquired by observation may allow an infant to more efficiently learn to produce natural, goal-directed walking behavior. 

Reinforcement learning (RL) provides an alternative to the (passive) unsupervised learning approach as it implicitly discovers an agent’s action space and the consequences of its actions. Recent breakthroughs in model-free and model-based RL suggest that end-to-end training can be used to learn mappings between sensory input and actions (\cite{mnih_nature_2015,lillicrap2015continuous,levine2016end,finn2017deep,schulman2015trust}). However, these methods require active observations and the sensorimotor mappings learned in this way cannot be easily generalized to new agents with different control interfaces. Methods for sensorimotor learning from purely visual data may facilitate learning where action information is not available, such as when using video data collected from the Internet. Such methods may also be useful for imitation learning, where ground truth actions are often hard or impossible to collect other than by visual observation (\cite{finn2017one, pathak2018zero}). More generally, learning from passive observations may make it easier to reuse action representations between systems with different effectors and goals. The representations learned by unsupervised methods are invariant to these choices because the model does not have access to motor commands or goals during training.

In this work, we evaluate the proposal that learning \textit{what you can do before doing anything} can lead to action space representations that make subsequent learning more efficient. To this end, we develop a model that learns to represent an agent’s action space given only unlabeled videos of the agent. The resulting representation enables direct planning in the latent space. Given a small number of action-labeled sequences we can execute the plan by learning a simple mapping from latent action representations to the agent’s controls.
This representation may be analogous to those in the parietal and premotor areas of cortex, which include populations of neurons that represent the structure of actions produced both by the self and by others (\cite{rizzolatti_premotor_1996, romo_neuron_2004}) and that are critical for reliably producing flexible, voluntary motor control (see \cite{kandel_principles_2012}, Chapter 38). In the brain, representations of this kind could plausibly be learned using specialized loss functions (\cite{marblestone2016toward}) whose effect is to induce the prior needed to determine the structure of actions in observation data.

\begin{figure}[t]
  \centering
  \includegraphics[width=1\textwidth]{figures/intuition_combined.pdf}\vspace{-15pt}
  \caption{Using latent composition to recover actions from passive data. \textbf{a)} Two sequences starting from different initial states but changing according to the same actions. Without requiring labels, our model learns to represent the action in sequences like these identically. We train a representation $z$ to capture the dynamics of the scene and its compositional structure: applying ($z_1$ and $z_2$) should have the same effect as applying the composed representation $g(z_1, z_2)$. These properties capture the fact that effector systems, such as a robot arm, use the same composable action space in many different states. \textbf{b)} The learned action space $z$ recovered by our method (PCA visualization). Points are colored by the true action $u$: true actions can be easily decoded from $z$, validating that the structure of the action space has been captured. }
  \label{fig:intuition}
  \vspace{-15pt}
\end{figure}

In contrast to most approaches to unsupervised learning of dynamics, which focus on learning the statistical structure of the environment, we focus on disentangling action information from the instantaneous state of the environment (Fig.\\ref{fig:intuition}). We base our work %
on recent stochastic video prediction methods (\cite{babaeizadeh2018stochastic,denton18stochastic,lee2018savp}) and impose two properties on the latent representation. First, we train the representation to be \textit{minimal}, i.e.\containing minimal information about the current world state. This forces the representation to focus on dynamic properties of the sensory input. A similar objective has been used in previous work to constrain the capacity of video prediction models (\cite{denton18stochastic}). Second, we train the representation to be \textit{composable} by introducing a novel loss term that enforces that the cumulative effect of a sequence of actions can be computed from the individual actions' representations (Fig.\\ref{fig:intuition}, left). Composability encourages disentangling: as a composed representation does not have access to the static content of the intermediate frames, a representation is composable only if the individual action representations are disentangled from the static content. Taken together, these two properties lead to a representation of sensory dynamics that captures the structure of the agent's actions.

We make the following three contributions.  First, we introduce 
a method for unsupervised learning of an agent's action space by training the %
latent representation of a stochastic video prediction model for the desiderata of minimality and composability. Second, we show that our method learns a representation of actions that is independent of scene content and visual characteristics on (i) a simulated robot with one degree of freedom and (ii) the BAIR robot pushing dataset (\cite{ebert2017self}).  Finally, we demonstrate that the learned representation can be used for action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action-labeled videos than extant supervised methods.

\section{Related work}

\paragraph{Learning structured and minimal representations.}

Several groups have recently shown how an adaptation of the variational autoencoder (VAE, \cite{kingma2014auto,
rezende2014stochastic}) can be used to learn representations that are minimal in the information-theoretic sense. \cite{alemi2016deep} showed that the
Information Bottleneck (IB) objective function (\cite{tishby1999information, shwartz2017opening}) 
can be optimized with a variational approximation that takes the form of the VAE objective with an additional weighting hyperparameter. %
In parallel, \cite{higgins2017beta} showed that a similar formulation can be used to produce disentangled representations. The connection between disentaglement and minimality of representations was further clarified by \cite{burgess2018understanding}. In this work, we apply the IB principle
to temporal models to enforce minimality of the representation.  %

Several groups have proposed methods to learn disentangled representations of static content and pose from video (\cite{denton2017unsupervised,tulyakov2017mocogan}).  \cite{jaegle2016unsupervised} learn a motion representation by enforcing that the motion acts on video frames as a group-theoretic action. In contrast, we seek a representation that disentangles the motion from the static pose. %
 
\cite{thomas2017independently} attempt to learn a disentangled representation of controllable factors of variation. While the goals of their work are similar to ours, their model relies on active learning and requires an embodied agent with access to the environment. In contrast, our model learns factors of variation purely from passive temporal visual observations, and thus can be applied even if access to the environment is costly or impossible.

\vspace{-8pt}
\paragraph{Unsupervised learning with video data.}

Several recent works have exploited temporal information for representation learning. \cite{srivastava2015unsupervised} used the Long Short-Term Memory (LSTM, \cite{hochreiter1997long}) recurrent neural network architecture to predict future frames and showed that the learned representation was useful for action recognition. \cite{vondrick2016anticipating} showed that architectures using convolutional neural networks (CNNs) can be used to predict actions and objects several seconds into the future. Recently, work such as  \cite{finn2016unsupervised,villegas2017decomposing,denton2017unsupervised} has proposed various modifications to the convolutional LSTM architecture (\cite{xingjian2015convolutional}) for the task of video prediction and shown that the resulting representations are useful for a variety of tasks.

Others have explored applications of video prediction models to RL and control (\cite{racaniere2017imagination,Ha2018WorldModels,merlin}). \cite{ChiappaRWM17} and \cite{Oh:2015:AVP:2969442.2969560} propose models that predict the consequences of actions taken by an agent given its control output. Similar models have been used to control a robotic arm (\cite{agrawal2016learning,finn2017deep,ebert2017self}). %
The focus of this work is on learning action-conditioned predictive models. In contrast, %
our focus is 
on the unsupervised discovery of the space of possible actions from video data. 
 
Our model is inspired by methods for stochastic video prediction that, given a sequence of past frames, capture the multimodal distribution of future images (\cite{goroshin2015learning, henaff2017prediction}). We use the recently proposed recurrent latent variable models based on the variational autoencoder  (\cite{babaeizadeh2018stochastic,denton18stochastic,lee2018savp}). We develop these methods and propose a novel approach to unsupervised representation learning designed to capture an agent's action space. 

\vspace{-8pt}
\paragraph{Sensorimotor abstractions for behavior.}
There is a long history of work developing sensorimotor representations for applications in reinforcement learning and robotics. Previous work in this domain has primarily focused on introducing hand-crafted abstractions and hierarchies to make sensorimotor mappings more generalizable. Methods for aggregating low-level controls into higher-level representations on which planning and RL can be performed are well-studied in the robotics literature: notable examples include the options framework for hierarchical RL (\cite{sutton1999options,bacon2017option}), dynamic motion primitives for manipulation (\cite{schaal2005motion,schaal2006adaptive,niekum2015learning}), and recent work that abstracts a learned policy away from low-level control and perception to ease simulation-to-real transfer (\cite{clavera2017policy,muller2018driving}). Other work has learned to separate robot-instance specific controls from task-related skills through modular policies, but this work does not enforce any structure onto the intermediate representation and requires extensive interaction with the environment (\cite{devin2017learning}). %

 
\section{Approach}
In this section, %
we describe our architecture 
for learning an action representation that is {minimal} and {composable}. 
In Sec.\\ref{sec:video_pred}, we describe a variational video prediction model similar to that of \cite{denton18stochastic} that provides us with a framework for learning a latent representation $z_t$ at time $t$ of the change between the past and the current frames. No labeled actions are considered at this stage. In Sec.\\ref{sec:composability}, we introduce an unsupervised method for imposing composability of the latent that allows us to recover a structured representation that defines CLASP. To verify that the learned representation corresponds to the executed control, we show that we can learn a bijective mapping between the latent representation and the control output executed at that time using a small number of labeled data points (Sec.\\ref{sec:control_mapping}). In the experimental section, we describe how the learned bijective mapping can be used for tasks such as action-conditioned video prediction (Sec.\\ref{sec:action_cond}) and planning in the learned action space (Sec.\\ref{sec:vis_servoing}).

\subsection{Video prediction model}
\label{sec:video_pred}

\begin{figure}[t]
  \centering
   \includegraphics[width=0.95\textwidth]{figures/architecture.png}~~~~~~
  \caption{Components of the proposed architecture. \textbf{Left}: The stochastic video prediction model, shown for one timestep. During training, we estimate the latent variable \({z}_t\) using the approximate inference network (\(\text{MLP}_{\text{infer}}, \text{CNN}_e\)) from the current and previous image. At test time, we produce $z_t$ using the prior distribution \(p({z}) \sim \mathcal{N}(0, I)\). Future frames are estimated by passing  $z_{t}$ together with images $x_{t-1}$ through the generative network (LSTM, \(\text{CNN}_d\)). Please refer to  Appendices \ref{app:svp} and \ref{app:expp} for architectural details. \textbf{Right}: Composability training. Latent samples $z$ are concatenated pairwise and passed through the composition network \(\text{MLP}_{\text{comp}}\) that defines a distribution over $\nu$ in the trajectory space. A sampled value of $\nu$ is decoded into an image through the same generative network (LSTM and CNN$_d$) and matched to the final image in the composed sequence.  }
  \label{fig:graphical_model}
  \vspace{-10pt}
\end{figure}

At the core of our method is a recurrent latent variable model for video prediction based on a temporal extension of the conditional VAE proposed by \cite{chung2015recurrent}.  We consider the generative model shown in Fig.\\ref{fig:graphical_model} (left). At each timestep $t$, the model outputs a latent variable \(z_t \sim p(z) = \mathcal{N}(0,I)\) associated with this timestep. Given a history of frames \(\mathbf{x}_{1:t-1}\) and latent samples \(\mathbf{z}_{2:t}\), the generative distribution over possible next frames is given by \(x_t \sim p_\theta(x_t | \mathbf{x}_{1:t-1}, \mathbf{z}_{2:t}) = \mathcal{N}(\mu_\theta(\mathbf{x}_{1:t-1}, \mathbf{z}_{2:t}), I)\). In practice, we generate the next frame 
by taking the mean of the conditional distribution: \(\hat{x}_t = \mu_\theta(\mathbf{x}_{1:t-1}, \mathbf{z}_{2:t})\).

To optimize the log-likelihood of this generative model, we introduce an additional network approximating the  posterior of the latent variable \(z_t \sim q_\phi(z_t | x_t, x_{t-1}) = \mathcal{N}(\mu_\phi(x_t, x_{t-1}),\sigma_\phi(x_t, x_{t-1}))\). We can optimize the model using the variational lower bound of the log-likelihood in a formulation similar to the original VAE. However, as has been shown recently by \cite{pmlr-v80-alemi18a}, the standard VAE formulation does not constrain the amount of information contained in the latent variable $z$. To overcome this, and to learn a minimal representation of $z$, we reformulate the standard VAE objective in terms of the Information Bottleneck (IB) (\cite{shwartz2017opening}). 

IB minimizes the mutual information, $I$, between the action  representation, $z_{t}$, and input frames, $\mathbf{x}_{t-1:t}$, while maximizing the ability to reconstruct the frame $x_t$ as measured by the mutual information between $(z_t, x_{t-1})$ and $x_t$: 
\begin{equation} \max_{p_\theta,q_{\phi}} I((z_t, x_{t-1}),x_t) - \beta_z I(z_t, x_{t-1:t}).
\end{equation}
The two components of the objective are balanced with a Lagrange multiplier $\beta_z$. When the value of $\beta_z$ is higher, the model learns representations that are more efficient, i.e.\minimal in the information-theoretic sense. We use this property to achieve our first objective of \textit{minimality} of $z$. 

The variational IB (\cite{alemi2016deep}) provides a variational approximation of the IB objective, that simply takes the form of the original VAE objective with an additional constant $\beta_z$. Aggregating over a sequence of frames, the video prediction objective for our model is given by:
\begin{equation} \mathcal{L}_{\theta,\phi}^{pred}(\mathbf{x}_{1:T}) = \sum_{t=1}^T \big{[} \mathbb{E}_{q_{\phi}(\mathbf{z}_{2:t} | \mathbf{x}_{1:t})} \log p_{\theta}(x_t | \mathbf{x}_{1:t-1}, \mathbf{z}_{2:t})  - \beta_z D_{KL}(q_{\phi}(z_t | \mathbf{x}_{t-1:t}) || p(z))\big{]}.
\end{equation}
The full derivation of the variational lower bound is given in the appendix of \cite{denton18stochastic}\footnote{\cite{denton18stochastic} use the objective with $\beta_z$, but formulate this objective in terms of the original VAE. %
}. The full model for one prediction step is shown in the left part of Fig.\\ref{fig:graphical_model}.


\subsection{CLASP: Learning action representations with composability}
\label{sec:composability}


Given a history of frames, the latent variable $z_t$ represents the distribution over possible next frames. It can thus be viewed as a representation of possible changes between the previous and the current frame. We will associate the latent variable $z_t$ with the distribution of such changes. 
In video data of an agent executing actions in an environment, the main source of change is the agent itself. Our model is inspired by the observation that a natural way to represent $z_t$ in such settings is by
the agents’ actions at time $t$. In this section, we describe an objective that encourages the previously described model (Sec.\\ref{sec:video_pred}) to learn action representations.

To encourage \textit{composability} of action representations, we use the procedure illustrated in Fig.\\ref{fig:graphical_model} (right). We define an additional random variable $\nu_t \sim q_\zeta(\nu_t | z_t, z_{t-1}) = \mathcal{N}(\mu_\zeta(z_t, z_{t-1}), \sigma_\zeta(z_t, z_{t-1}))$ that is a representation of the trajectory $z_{t-1:t}$. The process of composing latent samples into a single trajectory can be repeated several times in an iterative fashion, where the inference model $q_\zeta$ observes a trajectory representation $\nu_{t-1}$ and the next latent $z_t$ to produce the composed trajectory representation $\nu_t \sim q_\zeta(\nu_t | \nu_{t-1},z_t)$.  The inference model $q_\zeta$ is parameterized with a multilayer perceptron, $\text{MLP}_\text{comp}$. 


We want $\nu$ to encode entire trajectories, but we also require it to have minimal information about individual latent samples. We can encourage these two properties by again using the IB objective: 
\begin{equation} \max_{p_\theta,q_{\phi,\zeta}} I((\nu_t, x_1), x_t) - \beta_\nu  I(\mathbf{z}_{2:t}, \nu_t). 
\label{eq:comp_ib}
\end{equation}

We maximize this objective using the following procedure. Given a trajectory of $T$ frames, we use $\text{MLP}_\text{infer}$ to retrieve the action representations $z$. Next, we generate a sequence of trajectory representations $\nu_t$, each of which is composed from $C$ consecutive action representations $\mathbf{z}_{t-C:t}$. We obtain $T_C = \lfloor T/C \rfloor$ such representations.  Finally, we use $\nu_t$ to produce the corresponding frames $\hat{x}_t = p_{\theta}(x_t | x_{t-C}, \nu_t)$\footnote{To allow the generative model to distinguish between individual action representations $z$ and trajectory representations $\nu$, we concatenate them with a binary indicator set to $0$ for $z$ and $1$ for $\nu$. With the binary indicator, we can control whether the generative network interprets an input latent as the representation of a single action or a whole trajectory.}. The variational approximation to (\ref{eq:comp_ib}) that we use to impose composability takes the following form:
\begin{align}\begin{split}
\mathcal{L}^{comp}_{\theta,\phi,\zeta}(\mathbf{x}_{1:T}) & = \sum_{t=1}^{T_C} \big{[} \mathbb{E}_{q_{\phi, \zeta}(\mathbf{\nu}_{1:t} | \mathbf{x}_{1:T})} \log p_{\theta}(x_{t\text{\tiny$\times$} T_c} | \mathbf{x}_{1:(t-1)\text{\tiny$\times$} T_C}, \mathbf{\nu}_{1:t}) \\
& - \beta_\nu D_{KL}(q_{\phi,\zeta}(\nu_t | \mathbf{x}_{(t-1)\text{\tiny$\times$} T_C:t\text{\tiny$\times$} T_C}) || p(\nu))\big{]},
\end{split}\end{align}
where the prior distribution over $\nu$ is given by the unit Gaussian $\nu \sim p(\nu) = \mathcal{N}(0,I)$.

The objective above encourages the model to find a minimal representation for the trajectories $\nu$. As the trajectories are composed from only the action representations $z$, this encourages $z$ to assume a form suitable for efficient composition. This allows us to recover an action representation that is \textit{composable}. 
Our overall training objective is the sum of the two objectives:
\begin{equation}\mathcal{L}^{total}_{\theta,\phi,\zeta} = \mathcal{L}^{comp}_{\theta,\phi,\zeta} + \mathcal{L}^{pred}_{\theta,\phi}.\end{equation}

We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP).

\subsection{Grounding the control mapping}
\label{sec:control_mapping}


Our approach allows us to learn a latent representation \(z\) that is minimal and disentangled from the content of previous images. 
To use such a learned representation for control, we want to know which action $u$ a certain sample $z$ corresponds to, or vice versa. %
To determine this correspondence, we learn a simple bijective mapping from a small number of action-annotated frame sequences from the training data. 
We train the bijection using two lightweight multilayer perceptrons, 
\(\hat{z}_t = \text{MLP}_\text{lat}(u_t)\) and  \(\hat{u}_t = \text{MLP}_\text{act}(z_t)\). 
Note that only the $\text{MLP}_\text{lat}$ and $\text{MLP}_\text{act}$ networks are trained in this step, as we \textit{do not} propagate the gradients into the video prediction model. Because we do not have to re-train the video prediction model, this step requires far less data than models with full action supervision (Section \ref{sec:vis_servoing}). 

We note that standard image-based representations of motion, e.g., optical flow, do not directly form a bijection with actions in most settings. For example, the flow field produced by a reacher (as in Fig.\\ref{fig:transferseqs}) rotating from 12 o’clock to 9 o’clock is markedly different from the flow produced by rotating from 3 o’clock to 12 o’clock, even though the actions producing the two flow fields are identical (a 90 degree counter-clockwise rotation in both cases). In contrast, our representation easily learns a bijection with the true action space.%




\vspace{-3pt}\section{Empirical evaluation}\vspace{-4pt}

For evaluation, we consider tasks that involve regression from the latent variable $z$ to actions $u$ and vice versa. By learning this bijection we show that our model finds a representation that directly corresponds to actions and is disentangled from the static scene content. We show that after CLASP is trained, it can be used for both action-conditioned video prediction and planning (see Fig.\\ref{fig:task_overview}), and provide a procedure to plan in the learned representation. We also validate that our approach requires orders of magnitude fewer labels than supervised approaches, and that it is robust to certain visual characteristics of the agent or the environment.  Please refer to Appendix \ref{app:expp} for the exact architectural parameters.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/pca_ours_baseline.pdf}
  \vspace{-10pt}
  \caption{Visualization of the learned action space, $z$, on the reacher dataset. Each of the 1000 points depicts a value of $z$ for a different frame pair from the dataset. We plot the projection of $z$ onto the first two principal components of the data. Each point is colored by the value of the ground truth rotation, in radians, depicted in the two images used to infer $z$ for that point. \textbf{a)} The latent space learned by the baseline model has no discernible correspondence to the ground truth actions. \textbf{b)} Our method learns a latent space with a clear correspondence to the ground truth actions. In the Appendix,
  Fig.\ \ref{fig:pca_app} further investigates why the baseline fails to produce a disentangled representation. 
  }
  \label{fig:pca}
  \vspace{-8pt}
\end{figure}

\begin{wrapfigure}{R}{0.6\textwidth}
  \vspace{-15pt}
  \includegraphics[width=1\linewidth]{figures/task_overview.pdf}\vspace{-10pt}
  \caption{Illustration of how the learned representation can be used for \textbf{a)} action-conditioned prediction by inferring the latent variable, $z_t$, from the action, and \textbf{b)} visual servoing by solving the control problem in latent space through iterated rollouts and then mapping the latent variable to robot control actions, $u_t$.}
  \vspace{-15pt}
  \label{fig:task_overview}
\end{wrapfigure}

\vspace{-8pt}
\paragraph{Datasets.} We conduct experiments on a simple simulated reacher dataset and the real-world Berkeley AI Research (BAIR) robot pushing dataset from \cite{ebert2017self}. The reacher dataset consists of sequences of a robot reacher arm with one degree of freedom rotating counter-clockwise with random angular distances between consecutive images. We simulate it using OpenAI's Roboschool environment (\cite{roboschool}). The actions $u$ are encoded as relative angles between images, and constrained to the range $u \in [0^{\circ}, 40^{\circ}]$. The dataset consists of \SI{100000}{} training and \SI{4000}{} test sequences. Additionally, we create two variations of this dataset, with (i) varying backgrounds taken from the CIFAR-10 dataset (\cite{krizhevsky2009LearningML}) and (ii) varying robot appearance, with 72 different combinations of arm length and width in the training dataset. 

The BAIR robot pushing dataset comprises  \SI{44374}{} training and \SI{256}{} test sequences of \SI{30}{} frames each from which we randomly crop out subsequences of \SI{15}{} frames. We define actions, $u$, as differences in the spatial position of the end effector in the horizontal plane\footnote{The original dataset provides two additional discrete actions: gripper closing and lifting. However, we found that, in this dataset, the spatial position in the horizontal plane explains most of the variance in the end effector position and therefore ignore the discrete actions in this work.}. 
\vspace{-8pt}
\paragraph{Baselines.} We compare to the original model of \cite{denton18stochastic} that does not use the proposed composability objective. To obtain an upper bound on our method's performance we also compare to fully supervised approaches that 
train with action annotations: 
our implementations are based on \cite{Oh:2015:AVP:2969442.2969560} for the reacher dataset and the more complex \cite{finn2017deep} for the BAIR dataset. For planning, we also compare to a model based on the approach of \cite{agrawal2016learning} that learns the forward and inverse dynamics with direct supervision.
\vspace{-8pt}
\paragraph{Metrics.} In case of the action-conditioned video prediction we use the absolute angular position (obtained using a simple edge detection algorithm, see Appendix \ref{app:angdet}) for the reacher dataset and the change of end effector position (obtained via manual annotation) for the BAIR dataset. We choose these metrics as they capture the direct consequences of applied actions, as opposed to more commonly used visual appearance metrics like PSNR or SSIM. For visual servoing in the reacher environment we measure the angular distance to the goal state at the end of servoing.




 


\vspace{-3pt}\subsection{Learned structure of the action representations}\vspace{-5pt}
\label{sec:pca}

First, we inspect the structure of the learned action space for our model. To do so, we train CLASP on the reacher dataset and visualize the learned representation. In Fig.\\ref{fig:pca}, we show two-dimensional projections of samples, $z$, from the inference network, $q$, colored by the corresponding ground truth action, $u$. To find the two-dimensional subspace with maximal variability, we conducted Principal Component Analysis (PCA) on the means of the distributions generated by $q$. The first PCA dimension captures 99\% of the variance, which is explained by the fact that the robot in consideration has one degree of freedom. While the baseline without composability training fails to learn a representation disentangled from the static content, our method correctly recovers the structure of possible actions of the robot. 



\subsection{Action-conditioned video prediction}
\label{sec:action_cond}


\begin{figure}
  \centering
\includegraphics[width=\textwidth]{figures/figure_trans_everything_square_cut_compressed.pdf} \\[0.05cm]
  
\vspace{-10pt}
  \caption{Transplantation of action representations $z$ from one sequence to another. We infer action representations from the donor sequence and use them to create the recipient sequences from a different initial state. \textbf{a)} the reacher dataset. The previous frame is superimposed onto each frame to illustrate the movement. \textbf{b)} the BAIR dataset. The previous and the current position of the end effector are annotated in each frame (red and blue dots, respectively) to illustrate the movement. \textbf{c)} reacher with varying backgrounds. \textbf{d)} reacher with varying agent shape. The synchronization of movement in the sequences suggests that the learned action representation is disentangled from static content. Best viewed on a screen. Additional generated videos are available at: \url{https://daniilidis-group.github.io/learned_action_spaces/}.
  }
  
  \label{fig:transferseqs}
   \vspace{-15pt}
\end{figure}


\begin{wraptable}{R}{0.5\textwidth}
\centering
   \small
  \centering
  \vspace{-15pt}
  \caption{Action-conditioned video prediction results (mean $\pm$ standard deviation across predicted sequences). The "supervised" baseline is taken from \cite{Oh:2015:AVP:2969442.2969560} for the reacher dataset and \cite{finn2017deep} for BAIR.}
  \scalebox{1}{\begin{tabular}{lll}
    \toprule
    & {Reacher} & BAIR \\
    \cmidrule(r){1-3} 
    Method & \specialcell{Abs. Error\\\relax[in deg]} & \specialcell{Rel. Error\\\relax[in px]}\\
    \midrule
    Start State & $90.1\pm 51.8$ &  - \\
    Random & $26.6\pm 21.5$ & - \\
    Denton \& Fergus & $22.6\pm 17.7$ & $3.6 \pm 4.0$ \\    %
    CLASP (Ours) & $\bm{2.9\pm 2.1}$ & $\bm{3.0 \pm 2.1}$ \\
    \cmidrule(r){1-3} %
    Supervised & $2.6\pm 1.8$ & $2.0\pm 1.3$ \\
    \bottomrule
  \end{tabular}}
\vspace{-1pt}
   \label{tab:prediction}
\end{wraptable}

We further verify that our model recovers a representation of actions and show that this allows us to use the model for two tasks. First, we show that it is possible to \textit{transplant} the action representations $z$ from a given sequence into one with a different initial state. We run the approximate inference network $\text{MLP}_\text{infer}$ on the donor sequence to get the corresponding action representation $z$. We then use this sequence of representations $z$ together with a different conditioning image sequence to produce the recipient sequence.
While the content of the scene changes, the executed actions should remain the same. 
Second, we show how our model can be used for \textit{action-conditioned} video prediction. Given a ground truth sequence annotated with actions $u$, we infer the representations $z$ directly from $u$ using \(\text{MLP}_\text{lat}\). The inferred representations are fed into the generative model $p_\theta$ and the resulting sequences are compared to the original sequence.  

The quantitative results in Table\\ref{tab:prediction} show that the model trained with the composability objective on the reacher dataset successfully performs the task, with performance similar to the fully supervised model.  \cite{denton18stochastic} performs the task only slightly better than random guessing. This shows that it is infeasible to infer the latent $z_t$ learned by the baseline model given only the action $u_t$, and confirms our intuition about this from Fig.\\ref{fig:pca}. The qualitative results in Fig.\\ref{fig:transferseqs} (additional results in Figs.\\ref{fig:inf_reacher}, \ref{fig:trans_bair} and \ref{fig:inf_bair} in the Appendix and on the website) further support this conclusion.


On the BAIR dataset, our model performs better than the baseline of \cite{denton18stochastic},
 reducing the difference between the best unsupervised method and the supervised baseline by \SI{30}{\percent}. This is reflected in qualitative results as  frames generated by the baseline model often contain artifacts such as blurriness when the arm is moving or ghosting effects with two arms present in the scene (Figs.\\ref{fig:trans_bair} and \ref{fig:inf_bair} in the Appendix, videos on the website). These results demonstrate the promise of our approach in settings involving more complex, real-world interactions.




\subsection{Planning in the learned action space}
\label{sec:vis_servoing}






\begin{wraptable}{R}{0.5\textwidth}
\centering
\centering
  \vspace{-15pt}
  \caption{Visual servoing performance measured as distance to the goal at the end of servoing (mean $\pm$ standard deviation).}
 \small
  \scalebox{1}{\begin{tabular}{ll}
    \toprule
    \multicolumn{2}{c}{Reacher}   \\
    \cmidrule(r){1-2} 
    Method                       &      Distance [deg]  \\
    \midrule
    Start Position               &       $97.8\pm 23.7$  \\
    Random                       &       $27.0\pm 26.8$  \\
    \cite{denton18stochastic}    &       $14.1\pm 10.7$ \\
    CLASP (Ours)                 &       $\bm{1.6}\pm \bm{1.0}$  \\
    \midrule
    \cite{agrawal2016learning}   &       $2.0\pm 1.5$\\
    \cite{Oh:2015:AVP:2969442.2969560}  & $1.8\pm 1.5$  \\
    \midrule
    CLASP (varied background)         & $3.0 \pm 2.2$\\
    CLASP (varied agents)             & $2.8\pm 2.9$  \\
    \bottomrule
  \end{tabular}}
\vspace{-1pt}
   \label{tab:servo_main}
   \vspace{-12pt}
\end{wraptable}

Similarly to the true action space $u$, we can use the learned action space $z$ for planning. We demonstrate this on a visual servoing task. The objective of visual servoing is to move an agent from a start state to a goal state, given by images \(x_0\) and \(x_{\text{goal}}\), respectively.  
We use a planning algorithm similar to that of \cite{finn2017deep}, but plan trajectories in the latent space $z$ instead of true actions $u$. We use $\text{MLP}_\text{act}$ to retrieve the actions that correspond to a planned trajectory.


Our planning algorithm, based on Model Predictive Control (MPC), is described %
in Appendix \ref{app:serv}. %
The controller plans by sampling a number of action trajectories and iteratively refining them with the Cross Entropy Method (CEM, \cite{blossom2006cross}). The state trajectories are estimated by using the learned predictive model. %
We select the trajectory whose final state is closest to the goal and execute its first action.
The distance between the states is measured using the cosine distance between VGG16 representations (\cite{simonyan2014}). Servoing  terminates once the goal is reached or the maximum %
steps are executed. The baseline of \cite{agrawal2016learning} uses a different procedure, as described in the original paper.


We show qualitative results of a servoing rollout in the reacher environmet in Fig.\ \ref{fig:servo} (left)  and quantitative results in Table\\ref{tab:servo_main}. %
The agent not only reaches the target but also plans accurate trajectories at each intermediate time step. The trajectory planned in the learned space can be correctly decoded into actions, $u$.

\subsection{Data efficiency}
\label{sec:data_efficiency}
To validate the benefits of learning from passive observations, we measure the data efficiency of CLASP in Fig.\\ref{fig:servo} (right). In this setup, we train the methods on a large dataset of passive observations and a varied number of observations labeled with actions (100, 1000, 10000 videos). The supervised baselines, which cannot leverage pre-training with passive observations perform poorly in the low-data regime. In contrast, our model only needs a small number of action-labeled training sequences to achieve good performance, as it learns the structure of actions from passive observations. In the abundant data regime, our model still performs on par with both supervised baselines. We observed similar results for action-conditioned prediction experiments, summarized in Table \ref{tab:pred} in the Appendix. These results suggest that our planning approach can be used when the action-labeled data are limited.

\begin{figure}
\centering

\begin{minipage}[b]{0.38\linewidth}
\centering
\includegraphics[width=\textwidth]{figures/servoing.eps}
  \vspace{-0.25cm}
\end{minipage}
\hspace{0.01\linewidth}
\includegraphics[width=0.48\textwidth]{figures/servoing_graph.png}
\vspace{-7pt}
\caption{Visual servoing on the reacher task. \textbf{Left}: Planned and executed servoing trajectories.  Each of the first five rows shows the trajectory re-planned at the corresponding timestep. The first image of each sequence is the current state of the system, and the images to the right of it show the model prediction with the lowest associated cost. The target state (the reacher pointing to the upper left) is shown superimposed over each image. %
\textbf{Right}: Data efficiency measured as final distance to the goal after servoing, shown depending on the number of videos used in training. Each point represents a model trained on a dataset with a restricted number of action-annotated training sequences. Full results are in Table \ref{tab:servo_data} in the appendix. %
} 
   \label{fig:servo}
   \vspace{-10pt}
\end{figure}


\subsection{Robustness to varying visual characteristics}

To test the robustness of our approach to different kinds of visual variability in the environment, we conduct experiments on two versions of the reacher dataset with additional variability. In the first, the background of each sequence is replaced with a randomly drawn CIFAR-10 image (\cite{krizhevsky2009LearningML}). In the second, we vary the width and length of the reacher arm in each sequence. We test models trained on these datasets on sequences with variations not seen during training but drawn from the same distribution. The experimental setup is described in more detail in Appendix \ref{app:vary}.

As shown in Table\\ref{tab:servo_main}, our model can reliably discover the agent's action space and perform visual servoing under increased visual variability. The transplantation sequences in Fig.\\ref{fig:transferseqs} show that the action semantics are preserved across changes to the appearance of the environment that do not alter the dynamics. This is evidence that the learned representation captures the dynamics of the environment and is not sensitive to changes in visual characteristics that do not affect the agent's action space. In these two settings, CLASP also requires orders of magnitude less action-conditioned data than the supervised baselines (see Table \ref{tab:servo_data} in the appendix). Our results, combined with the data efficiency result, suggest that our method is robust to visual changes and can be used for passive learning from videos that are obtained under different visual conditions, or even videos of different agents, such as videos obtained from the Internet, as long as the action space of the observed agents coincides with the target agent. 



 
\section{Conclusion}

We have shown a way of learning the structure of an agent's action space from visual observations alone by imposing the properties of minimality and composability on a latent variable for stochastic video prediction. 
This strategy offers a data-efficient alternative to approaches that rely on fully supervised action-conditioned methods. The resulting representation can be used for a range of tasks, such as action-conditioned video prediction and planning in the learned latent action space. The representation is insensitive to the static scene content and visual characteristics of the environment.
It captures meaningful structure in synthetic settings and achieves promising results in realistic visual settings. %

\subsubsection*{Acknowledgements}

We thank Nikos Kolotouros and Karl Schmeckpeper for help with annotation, Kenneth Chaney and Nikos Kolotouros for computing support, Stephen Phillips and Nikos Kolotouros for helpful comments on the document, and the members of the GRASP laboratory and CLVR laboratory for many fruitful discussions. We also thank the audiences of the 2018 R:SS workshop on Learning and Inference in Robotics, 2018 International Computer Vision Summer School, and 2018 NeurIPS workshop on Probabilistic Reinforcement Learning and Structured Control for useful feedback.  We are grateful for support through the following grants: NSF-DGE-0966142 (IGERT), NSF-IIP-1439681 (I/UCRC), NSF-IIS-1426840, NSF-IIS-1703319, NSF MRI 1626008, ARL RCTA W911NF-10-2-0016, ONR N00014-17-1-2093, and by Honda Research Institute. K.G.D. is supported by a Canadian NSERC Discovery grant. K.G.D. contributed to this article in his personal capacity as an Associate Professor at Ryerson University.

 \clearpage
\bibliography{bibref_definitions_long,bibtex}
\bibliographystyle{iclr2019_conference}

\appendix

\section{Stochastic video prediction}\label{app:svp}

We use an architecture similar to SVG-FP of \cite{denton18stochastic}. Input images \(x_t\) are encoded using a convolutional neural network 
$\text{CNN}_e(\cdot)$%
to produce a low-dimensional representation $\text{CNN}_e(x_t)$; output image encodings can be decoded with a neural network with transposed convolutions 
$\text{CNN}_d(\cdot)$. We use a Long Short-Term Memory network $\text{LSTM}(\cdot,\cdot)$ for the generative network \(\mu_\theta(x_{t-1},z_t)=\text{CNN}_d(\text{LSTM}(\text{CNN}_e(x_{t-1}),z_t))\), and a multilayer perceptron $\text{MLP}_\text{infer}$ for the approximate inference network \([\mu_\phi(x_t, x_{t-1}),\sigma_\phi(x_t, x_{t-1})] = \text{MLP}_\text{infer}(\text{CNN}_e(x_t),\text{CNN}_e(x_{t-1}))\).  

During training, our model first observes \(K\) past input frames. From these observations, the model generates \(K-1\) corresponding latents \(z_{2:K}\) and predicts \(K-1\) images \(\hat{\mathbf{x}}_{2:K} = \mu_\theta(\mathbf{x}_{1:K-1}, \mathbf{z}_{2:K})\). The model generates $T-K$ further future images: \(\hat{\mathbf{x}}_{K+1:T} = \mu_\theta(\hat{\mathbf{x}}_{1:T-1}, \mathbf{z}_{1:T})\). At test time, latents \(z_t\) are sampled from the prior \(\mathcal{N}(0,I)\), and the model behaves identically otherwise. 
We show samples from the stochastic video prediction model in Fig. \ref{fig:svg}.

Unlike in \cite{denton18stochastic}, the generating network \(p_\theta\) does not observe ground truth frames \(\mathbf{x}_{K+1:T-1}\) in the future during training but autoregressively takes its own predicted frames \(\hat{\mathbf{x}}_{K+1:T-1}\) as inputs. This allows the network \(\text{LSTM}\) to generalize to observing the generated frame encodings \(\text{LSTM}(\text{CNN}_e(x_{t-1}),z_t)\) at test time when no ground truth future frames are available. We use a recurrence relation of the form \(\text{LSTM}(\text{LSTM}(x_{t-2},z_{t-1}),z_t)\). To overcome the generalization problem, \cite{denton18stochastic} instead re-encode the produced frames with a recurrence relation of the form \(\text{LSTM}(\text{CNN}_e(\text{CNN}_d(\text{LSTM}(x_{t-2},z_{t-1}))),z_t)\). Our approach omits the re-encoding, which saves a considerable amount of computation. 

\section{Experimental parameters}\label{app:expp}

For all experiments, we condition our model on five images and roll out ten future images. We use images with a resolution of $64\times 64$ pixels.  The dimension of the image representation is $\text{dim}(g(x)) = 128$, and the dimensions of the learned representation are $\text{dim}(z) = \text{dim}(\nu) = 10$. For the reacher dataset, we use the same architecture as \cite{denton18stochastic} for the $f,g$ and $\text{LSTM}$ networks. For experiments with the standard blue background (i.e. all except the varied background experiment) we do not use temporal skip-connections. For the BAIR dataset, we do not use $f,g$ and use the same model as \cite{lee2018savp} for $\text{LSTM}$. The $\text{MLP}_{\text{infer}}$ has two hidden layers with 256 and 128 units, respectively. The  $\text{MLP}_{\text{comp}},\text{MLP}_{\text{lat}}$, and $\text{MLP}_{\text{act}}$ networks each have two hidden layers with 32 units. For $\text{MLP}_{\text{lat}}$ and $\text{MLP}_{\text{act}}$, we tried wider and deeper architectures, but this did not seem to improve performance of either our method or the baseline without composability. This is probably because the latent space in our experiments had either a simple representation that did not need a more powerful network to interpret it, or was entangled with static content, in which case even a more powerful network could not learn the bijection.  The number of latent samples $z$ used to produce a trajectory representation $\nu$ is $C = 4$. For all datasets, $\beta_z = 10^{-2},\beta_\nu = 10^{-8}$ We use the leaky ReLU activation function in the $g,f$, and MLP networks. We optimize the objective function using the Adam optimizer with parameters $\beta_1=0.9, \beta_2=0.999$ and a learning rate of $2 \times 10^{-4}$. All experiments were conducted on a single high-end NVIDIA GPU. We trained the models for 4 hours on the reacher dataset, for one day on the BAIR dataset.

We found the following rule for choosing both bottleneck parameters $\beta_z$ and $\beta_\nu$ to be both intuitive and effective in practice: they should be set to the highest value at which samples from the approximate  inference $q$ produce high-quality images. If the value is too high, the latent samples will not contain enough information to specify the next image. If the value is too low, the divergence between the approximate inference and the prior will be too large and therefore the samples from the prior will be of inferior quality. We note that the problem of determining $\beta$ is not unique to this work and occurs in all stochastic video prediction methods, as well as VIB and $\beta$-VAE.

\section{Visual servoing}\label{app:serv}

We use Algorithm 1 for visual servoing. At each time step, we initially sample \(M\) latent sequences $\bm{z}_0$ from the prior \(\mathcal{N}(0,I)\) and use the video prediction model to retrieve \(M\) corresponding image sequences $\bm{\tau}$, each with \(K\) frames. We define the cost of an image trajectory as the cosine distance between the VGG16 (\cite{simonyan2014}) feature representations of the target image and the final image of each trajectory. This is a perceptual distance, as in \cite{johnson2016}.
In the update step of the Cross Entropy Method (CEM) algorithm, we rank the trajectories based on their cost and fit a diagonal Gaussian distribution to the latents \(\bm{z}^\prime\) that generated the \(M^\prime\) best sequences. We fit one Gaussian for each prediction time step \(k\in K\). After sampling a new set of latents \(\bm{z}_{n+1}\) from the fitted Gaussian distributions we repeat the procedure for a total of \(N\) iterations.

Finally, we pick the latent sequence corresponding to the best rollout of the last iteration and map its first latent sample to the output control action using the learned mapping: \(u^{\ast} = \text{MLP}_\text{act}(z_{N,0}^{\ast})\). This action is then executed in the environment. The action at the next time step is chosen using the same procedure with the next observation as input. The algorithm terminates when the specified number of servoing steps \(T\) has been executed.

\begin{table}
\centering
  \caption{Average absolute angle error (mean $\pm$ standard deviation) for action-conditioned video prediction. Note that we could not detect angles on some sequences for the action-conditioned baseline of \cite{Oh:2015:AVP:2969442.2969560} trained on only 100 sequences due to bad prediction quality.}
  \centering
  \begin{tabular}{llll}
    \toprule
    \multicolumn{4}{c}{Reacher}   \\
    \cmidrule(r){1-4} 
    Method & \multicolumn{3}{c}{Angle Error [deg]}  \\
    \cmidrule(r){1-4}
    Training Sequences & \SI{100}{} & \SI{1000}{} & \SI{10000}{} \\
    \midrule
    Start Position & \multicolumn{3}{c}{\sout{\hfill} $90.6\pm 52.0$ \sout{\hfill}}  \\
    Random & \multicolumn{3}{c}{\sout{\hfill} $27.7\pm 22.2$ \sout{\hfill}} \\
    \cite{denton18stochastic}& $27.6\pm 22.8$ & $23.8\pm 18.6$ & $23.6\pm 18.3$ \\
    CLASP (Ours) & $\bm{2.9}\pm \bm{2.0}$ & $\bm{2.9}\pm \bm{2.0}$ & $3.0\pm 2.0$  \\
    \midrule
    \cite{Oh:2015:AVP:2969442.2969560} & - & $5.6\pm 4.5$ & $\bm{2.7}\pm \bm{1.9}$  \\
    \bottomrule
  \end{tabular}
  \label{tab:pred}
\end{table}\begin{table}
\centering
  \caption{Visual servoing performance and data efficiency.}
  \label{tab:vs}
  \centering
\begin{tabular}{llll}
    \toprule
    \multicolumn{4}{c}{Reacher}   \\
    \cmidrule(r){1-4} 
    Method                             & \multicolumn{3}{c}{Distance [deg]}  \\
    \cmidrule(r){1-4}
    Training Sequences                 & \SI{100}{} & \SI{1000}{} & \SI{10000}{} \\
    \midrule
    Start Position                     & \multicolumn{3}{c}{\sout{\hfill} $97.8\pm 23.7$ \sout{\hfill}}  \\
    Random                             & \multicolumn{3}{c}{\sout{\hfill} $27.0\pm 26.8$ \sout{\hfill}} \\
    \cite{denton18stochastic}          & $20.9\pm 13.0$         & $15.5\pm 13.1$         & $14.1\pm 10.7$ \\
    CLASP (Ours)                       & $\bm{2.0}\pm \bm{2.2}$ & $\bm{2.2}\pm \bm{1.8}$ & $\bm{1.6}\pm \bm{1.0}$  \\
    \midrule
    \cite{agrawal2016learning}         & $32.7\pm 21.7$         & $3.6\pm 3.1$           & $2.0\pm 1.5$\\
    \cite{Oh:2015:AVP:2969442.2969560} & $21.8\pm 12.9$         & $2.6\pm 2.6$           & $1.8\pm 1.5$  \\
    \midrule
    CLASP (varied background)          & ${1.5}\pm {1.3}$       & $3.8 \pm 3.5$          & $3.0 \pm 2.2$\\
    CLASP (varied agents)              & $2.0\pm 1.0$           & $2.3\pm 3.4$           & $2.8\pm 2.9$  \\
    \bottomrule
  \end{tabular}
  \label{tab:servo_data}
\end{table}\begin{algorithm}[h]
	\caption{Planning in the learned action space} \label{alg:vs}
	\begin{algorithmic}[1]
		\Require $\text{Video prediction model } \hat{x}_{t:t+K} = \mu_\theta(x_{1:t-1}, z_{2:t+K})$
		\Require $\text{Start and goal images } i_0 \text{ and } i_{\text{goal}}$
        \For{$t = 1\dots T$}
          \State $\text{Initialize latents from prior: } \bm{z}_0 \sim \mathcal{N}(0,I)$
          \For{$n = 0\dots N$}
          	\State $\text{Rollout prediction model for }K\text{ steps, obtain }M\text{ future sequences } \bm{\tau} = \hat{\bm{x}}_{t:t+K}$
            \State $\text{Compute cosine distance between final and goal image: } c(\tau) = \text{cos}(\hat{x}_{t+K}, i_{\text{goal}})$
            \State $\text{Choose }M^\prime\text{ best sequences, refit Gaussian distribution: } \bm{\mu}_{n+1}, \bm{\sigma}_{n+1} = \text{fit}(\bm{z}_n^{\prime})$
            \State $\text{Sample new latents from updated distribution: }\bm{z}_{n+1} \sim \mathcal{N}(\bm{\mu}_{n+1}, \bm{\sigma}_{n+1})$
          \EndFor
          \State $\text{Map first latent of best sequence to action: } u^{\ast} = \text{MLP}_\text{act} (z_{N,0}^{\ast})$
          \State $\text{Execute }u^{\ast}\text{ and observe next image}$
		\EndFor
	\end{algorithmic}
\end{algorithm}

The parameters used for our visual servoing experiments are listed in Tab. \ref{tab:vs_param}.

\begin{table}
\caption{Hyperparameters for the visual servoing experiments. We sample an angle uniformly from the angle difference range to create each subsequent image in a sequence.}
  \centering
  \begin{tabular}{ll}
    \toprule
    \multicolumn{2}{c}{Servoing Parameters}\\
    \midrule
    Servoing timesteps ($T$) & 5  \\
    Servoing horizon ($K$) & 5 \\
    \# servoing sequences ($M$) & 10  \\
    \# refit sequences ($M^\prime$) & 3 \\
    \# refit iterations ($N$) & 4 \\
    Angle difference range & [\SI{0}{\degree}, \SI{40}{\degree}] \\
    \bottomrule
  \end{tabular} 
\label{tab:vs_param}
\end{table}\newpage\section{Angle Detection Algorithm}\label{app:angdet}

We employ a simple, hand-engineered algorithm to quickly retrieve the absolute angle values from the images of the reacher environment. First we convert the input to a grayscale image and run a simple edge detector to obtain a binary image of the reacher arm. We smooth out noise by morphological opening. We compute the Euclidean distance to the image center for all remaining non-zero pixels and locate the reacher tip at the pixel closest to the known reacher arm length. This gives us the absolute reacher arm angle.

To evaluate the accuracy of our angle detection algorithm, we estimated the angle for all images of the simulated training dataset and compare it to ground truth. A histogram of the angle errors of our algorithm is displayed in Fig. \ref{fig:angle_error}. All errors are below 10 degrees and the majority are smaller than 5 degrees. This suggests the output of this model is of a suitable quality to serve as surrogate ground truth. A second approach that used a neural network to regress the angle directly from the pixels achieved similar performance. We attribute the errors to the discretization effects at low image resolutions -- it is impossible to achieve accuracy below a certain level due to the discretization.

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/angle_error_hist.eps} 
  \caption{Error histogram of the angle detection algorithm on the reacher training set. The output of this algorithm is used as a form of surrogate ground truth to evaluate model performance.}
  \label{fig:angle_error}
\end{figure}\section{Experiments with varying environments}\label{app:vary}\subsection{Robustness to Changing Static Background}

We test the robustness of our method to different static backgrounds by replacing the uniform blue background with images from the CIFAR-10 training set (\cite{krizhevsky2009LearningML}). For each sequence we sample a single background image that is constant over the course of the entire sequence. At test time we use background images that the model has not seen at training time, i.e. sampled from a held-out subset of the CIFAR-10 training set. As in previous experiments, we first train our model on pure visual observations without action-annotations. We then train the networks $\text{MLP}_\text{lat}$ and $\text{MLP}_\text{act}$ on a small set of action-annotated sequences to convergence. For the visual servoing we follow the same algorithm as in the previous experiments (see Appendix \ref{app:serv}).

\begin{figure}
\centering
\hspace{0.25cm}
\includegraphics[width=\textwidth]{figures/figure_trans_varbg.pdf}
\vspace{-15pt}
\caption{Trajectory transplantation with differing visual characteristics. The trajectory from the top sequence is transplanted to a different environment and initial state in each of the two bottom sequences. Our model achieves almost perfect accuracy, which validates that it has indeed learned a representation of actions disentangled from the static content, such as the background, agent's appearance, and the initial state. The previous frame is superimposed onto each frame to illustrate the movement. \textbf{Top:} dataset with varying backgrounds. \textbf{Bottom:} dataset with varying robots. Additional generated videos are available at: \url{https://daniilidis-group.github.io/learned_action_spaces/}.} 
   \label{fig:trans_varbg}
   \vspace{-12pt}
\end{figure}Qualitative servoing results of our method on the dataset with varied backgrounds are shown in Fig.\,\ref{fig:serv_rand_bg} and quantitative results in Figure \ref{fig:servo} (right). The model accurately predicts the background image into the future and successfully discovers and controls the action space of the agent. The fact that the same bijective mapping between latents and actions works for all backgrounds suggests that the network is able to disentangle the static content of the scene and the dynamics attributed to the moving reacher arm. In addition, we show trajectory transplantation between different backgrounds in Fig. \ref{fig:trans_varbg} (top), which further validates the claim that the learned latent represents the action consistently, independent of the background. 

\subsection{Learning from Agents with Different Visual Appearance}

We test the ability of our method to learn from agents that differ in their visual appearance from the agent used at test time, but that share a common action space. %
We construct a dataset in which we vary parameters that determine the visual characteristics of the reacher arm, specifically its thickness and length (see Fig. \ref{fig:serv_varRobo}, left). In total our training dataset comprises \SI{72}{} different configurations spanning a wide variety of visual appearances.


We show a qualitative example of a servoing trajectory in Fig. \ref{fig:serv_varRobo} (right). We additionally evaluate the efficacy of training on the novel dataset by following the procedure employed in Section \ref{sec:data_efficiency}: we train the mapping between latent representation $z$ and actions to convergence on action-annotated subsets of the training data of varying sizes. The servoing errors in Figure \ref{fig:servo} (right) show that we achieve comparable performance independent of whether we train on the target agent we test on or on a set of agents with different and varied visual appearances. Our model is able to learn a latent representation that captures the action space shared between all the agents seen at training time. We can then learn the mapping between this abstract action space and the actions of the agent with the novel visual appearance from a small number of action-annotated sequences. In addition, we show trajectory transplantation between different agents in Fig. \ref{fig:trans_varbg} (bottom) that further validates our claim that the learned latent represents the action consistently, independent of the agent. 

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/servo_trajs_randbg_compressed.pdf} 
  \caption{Servoing examples with randomly sampled static CIFAR-10 backgrounds. The figure layout follows the layout of Fig. \ref{fig:servo} (left).}
  \label{fig:serv_rand_bg}
\end{figure}\begin{figure}
\centering
\hspace{0.25cm}
\includegraphics[width=0.4\textwidth]{figures/varRobo.eps}
\hspace{0.25cm}
\includegraphics[width=0.48\textwidth]{figures/servo_trajs_varrobo.png}
\vspace{-7pt}
\caption{Learning from agents with varied visual appearance. \textbf{Left}: 
Sample agent configurations from the training set. We cover a variety of visual appearances (i.e. arm lengths and widths) but not the configuration used for testing. \textbf{Right}: Test time servoing example after pre-training on observations of agents with varied visual appearances. The figure layout follows the layout of Fig. \ref{fig:servo} (left).
} 
   \label{fig:serv_varRobo}
   \vspace{-12pt}
\end{figure}\begin{figure}[b]
  \centering
  \includegraphics[width=0.95\textwidth]{figures/svg_figure.pdf}
\vspace{-7pt}
  \caption{Typical sequences sampled from the stochastic video prediction model. In the past, the samples $z$ are generated from the approximate inference distribution and match the ground truth exactly. In the future, $z$ is sampled from the prior, and correspond to various possible futures. These three sequences are different plausible continuations of the same past sequence. This shows that the model is capable of capturing the stochasticity of the data. Only five of ten predicted frames are shown for clarity. Additional generated videos are available at: \url{https://daniilidis-group.github.io/learned_action_spaces/}.}
  \label{fig:svg}
   \vspace{-10pt}
\end{figure}\begin{figure}
  \centering

  \includegraphics[width=0.95\textwidth]{figures/figure_inf_reacher-compressed.pdf}
\vspace{-7pt}
  \caption{Typical action-conditioned prediction sequences on the reacher dataset. Each example shows \textbf{top}: the ground truth sequence,  \textbf{middle}: our predictions, \textbf{bottom:} predictions of the baseline model (\cite{denton18stochastic}). To illustrate the motion, we overlay the previous position of the arm in each image (transparent arm). Our method produces sequences that are perfectly aligned with the ground truth. The baseline never matches the ground truth motion and is only slightly better than executing random actions. Best viewed on a computer, additional generated videos are available at: \url{https://daniilidis-group.github.io/learned_action_spaces/}.}
  \label{fig:inf_reacher}
   \vspace{-10pt}
\end{figure}\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{figures/figure_trans_bair.pdf} \\[0.05cm]
\vspace{-7pt}
  \caption{Failure cases of the baseline model on trajectory transplantation.  Each example shows \textbf{top}: the ground truth sequence,  \textbf{middle}: our predictions, \textbf{bottom:} predictions of the baseline model (\cite{denton18stochastic}). The position of the end effector at the current (blue) and previous (red) timestep is annotated in each frame. The baseline often produces images with two different robot arms and other artifacts. Only six of ten predicted frames are shown for clarity. Best viewed on a computer, additional generated videos are available at: \url{https://daniilidis-group.github.io/learned_action_spaces/}.}
  \label{fig:trans_bair}
   \vspace{-10pt}
\end{figure}\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{figures/figure_inf_bair.pdf} \\[0.05cm]
\vspace{-7pt}
  \caption{Baseline failure cases on action-conditioned video prediction.  Each example shows \textbf{top}: ground truth sequence,  \textbf{middle}: our predictions, \textbf{bottom:} \cite{denton18stochastic} baseline. The previous and the current position of the end effector are annotated in each frame. The baseline often produces images with two different robot arms and other artifacts. Only six of ten predicted frames are shown for clarity. Best viewed on a computer, additional generated videos are available at: \url{https://daniilidis-group.github.io/learned_action_spaces/}.}
  \label{fig:inf_bair}
   \vspace{-10pt}
\end{figure}\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{figures/pca_0.png}~~~\includegraphics[width=0.45\textwidth]{figures/pca_1.png}
  
  \includegraphics[width=0.45\textwidth]{figures/pca_2.png}~~~\includegraphics[width=0.45\textwidth]{figures/pca_3.png}\vspace{-19pt}
  \caption{Visualization of the structure of the learned latent space of the baseline model without composability training on the reacher dataset. The visualization is done in the same manner as in Fig. \ref{fig:pca}. Here, action representations $z_t$ are shown as a function of the absolute angle ($\alpha$) of the reacher arm at time $t-1$  and the relative angle between the reacher at time $t$ and $t-1$. We see that the encoding of action learned by the baseline is entangled with the absolute position of the reacher arm. While this representation can be used to predict the consequences of actions given the previous frame, it is impossible to establish a bijection between $u_t$ and $z_t$ as the correspondence depends on the previous frame $x_{t-1}$.
  Moreover, it is impossible to compose two samples of such a $z$ without access to the intermediate frame. This representation is \textit{minimal}, as it is a linear transformation (a rotation) of the known optimal representation $u_t$ (the ground truth actions). This suggests that composability plays an important role in learning a disentangled representation of actions.}
  \label{fig:pca_app}
  \vspace{-15pt}
\end{figure}

\end{document}

