\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage[lined,boxed,commentsnumbered]{algorithm2e}
%\usepackage{float}
%\newfloat{myalgo}{tbhp}{mya}
%\newenvironment{Algorithm}[1][tbh]%
%{\begin{myalgo}[#1]
%\centering
%\begin{minipage}{1.\linewidth}
%\begin{algorithm}} %[H]}%
%{\end{algorithm}
%\end{minipage}
%\end{myalgo}}

\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{makecell}



% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{3256} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%Addition to make captions closer to figures
\setlength{\textfloatsep}{6pt}

%%%%%%%%% TITLE
%\title{Deep Compositional Grammatical Architectures for Representation Learning (Confidential manuscript)}
\title{Learning Deep Compositional Grammatical Architectures for Visual Recognition}

\author{Xilai Li$^\dagger$, Tianfu Wu$^{\dagger,\ddagger}$\thanks{T. Wu is the corresponding author.}, and Xi Song\\
Department of ECE$^\dagger$ and the Visual Narrative Initiative$^\ddagger$, 
North Carolina State University\\
{\tt\small \{xli47, tianfu\_wu\}@ncsu.edu, xsong.lhi@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
    Neural architectures are the foundation for improving performance of deep neural networks (DNNs). This paper presents deep compositional grammatical architectures which harness the best of two worlds: grammar models and DNNs. The proposed architectures integrate compositionality and reconfigurability of the former and the capability of learning rich features of the latter in a principled way. They also show the platform-agnostic capability in deployment (e.g., cloud vs mobile).  We utilize AND-OR Grammars (AOG)~\cite{DisAOT-CVPR,Zhu_Grammar,Yuille_AndOr} in this paper and call the resulting networks \textbf{AOGNets}. An AOGNet consists of a number of stages each of which is composed of a number of AOG building blocks. An AOG building block splits its input feature map into $N$ groups along feature channels and then treat it as a sentence of $N$ words. It then jointly realizes a phrase structure grammar and a dependency grammar in bottom-up parsing the ``sentence" for better feature exploration and exploitation. It provides a unified framework for the  split-transform-aggregate heuristic widely used in neural architecture design. In experiments, AOGNets are tested on three highly competitive image classification benchmarks: CIFAR-10, CIFAR-100 and ImageNet-1K. AOGNets obtain better performance than ResNets~\cite{ResidualNet} and most of its variants, ResNeXts~\cite{ResNeXt}, DenseNets~\cite{DenseNet} and DualPathNets~\cite{DPN} when model sizes are comparable. AOGNets are also tested in object detection on the PASCAL VOC 2007 and 2012~\cite{VOC} using the vanilla Faster R-CNN~\cite{FasterRCNN} system. AOGNets use smaller models and obtain better performance by about $3\%$ mean average precision than the ResNets backbone. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}




%-------------------------------------------------------------------------
\subsection{Motivation and Objective} 
Recently, deep neural networks (DNNs)~\cite{LeCunCNN,AlexNet} improved prediction accuracy significantly in many vision tasks, and even obtained superhuman performance in image classification tasks~\cite{ResidualNet,InceptionNet,DenseNet,DPN}.
Much of these progress have been achieved mainly through engineering network architectures which can enjoy increasing representational power (by going either deeper or wider) without sacrificing the feasibility of optimization using back-propagation with stochastic gradient descent (i.e., handling the  vanishing and/or exploding gradient problem). 
The dramatic success does not necessarily speak to its sufficiency given the lack of theoretical underpinnings of deep neural networks at present~\cite{BoundsOfDL}. Different methodologies are worth exploring to enlarge the scope of neural architectures for seeking better DNNs. 
For example, Hinton recently pointed out a crucial drawback of  current convolutional neural networks: according to recent neuroscientific research, these artificial networks do not contain enough levels of structure~\cite{Hinton,capsules}.

As illustrated in Fig.~\ref{fig:existing-blocks} (a), neural architecture design and search can be posed as a combinatorial search problem in a product space comprising two sub-spaces: 
\begin{itemize}[leftmargin=*]
\itemsep0em
    \item The structure space which consists of all directed acyclic graphs (DAGs) with the start node representing input raw data and the end node representing task loss functions. DAGs are entailed for feasible computation.
    \item The node operation space which consists of all possible transformation functions for implementing nodes in a DAG, such as Convolution+BatchNorm~\cite{BatchNorm}+ReLU~\cite{AlexNet} and its bottleneck implementation~\cite{ResidualNet}. 
\end{itemize}

The structure space is almost unbounded, and the node operation space for a given structure is also combinatorial. Neural architecture design and search is an NP-hard problem due to the exponentially large space and the highly non-convex non-linear objective function to be optimized in the search. As illustrated in Fig.~\ref{fig:existing-blocks} (b), to mitigate the difficulty, neural architecture design and search have been simplified to design or search a building block structure, and then stack the same build block structure into a predefined number of stages. Fig.~\ref{fig:existing-blocks} (c) shows some examples of popular building blocks. In those hand-crafted building blocks, the best practices of finding a good building block adopt the so-called \emph{split-transform-aggregate} heuristic. The heuristic is motivated by the well-known Hebbian principle in neuroscience, i.e., neurons fire together, then wire tighter. Put in another word, the wisdom in designing better deep network architectures usually lies in finding network topologies which can support flexible information flows for both exploring new features and exploiting existing features in previous layers. More specifically, we observed the advantages of popular networks are:

\begin{figure}
    	\centering
    	\includegraphics[width = 0.5\textwidth]{./existing-blocks.pdf}
    	\caption{\small{ (a) Illustration of the space of neural architectures, (b) the building block based approaches to explored the space, and (c) examples of popular building blocks in GoogLeNets~\cite{InceptionNet}, ResNets~\cite{ResidualNet}, ResNeXts~\cite{ResNeXt}, DenseNets~\cite{DenseNet} and DualPathNets~\cite{DPN}.
    	See text for details. (Best viewed in color)} 
    	}
    	\label{fig:existing-blocks} 
\end{figure}
\begin{itemize}[leftmargin=*]
\itemsep0em
    \item InceptionNets or GoogLeNets~\cite{InceptionNet} embodies the split-transform-aggregate heuristic in a shallow feed-forward way. However, the filter numbers and sizes are tailored for each individual transformation, and the modules are customized stage-by-stage. Interleaved group convolutions~\cite{IGC1} share the similar spirit, but use simpler scheme. 
    \item ResNets~\cite{ResidualNet} provide a simple yet effective solution that enables networks to enjoy going either deeper or wider without sacrificing the feasibility of optimization using back-propagation with stochastic gradient descent (i.e., handling the vanishing and/or exploding gradient problems). From the perspective of representation learning, skip-connections within a ResNet~\cite{ResidualNet} contributes to effective features exploitation/reuse. They do not realize the split component.
    \item ResNeXts~\cite{ResNeXt} add the spit component in ResNets and address the drawbacks of the Inception modules using group convolutions in the transformation.
    \item Deep pyramid ResNets~\cite{DPRN} extend ResNets, which concentrate on the feature map dimension by increasing it gradually instead of by increasing it sharply at each residual unit with down-sampling.  
    \item DenseNets~\cite{DenseNet} explicitly differentiate between information that is added to the network (i.e., exploration via split-transform) and information that is preserved (i.e., exploitation via aggregation, especially residual connections). From the perspective of representation learning, dense connection with feature maps being concatenated together in DenseNets~\cite{DenseNet} leads to effective feature exploration. 
    \item Dual Path Networks (DPN)~\cite{DPN} utilize ResNet blocks and DenseNet blocks in parallel to balance feature exploitation and feature exploration. 
    \item Deep layer aggregation networks (DLA)~\cite{DLA} iteratively and hierarchically merge the feature hierarchy when stacking the building blocks such as the ResNet ones.  
\end{itemize}

%\setlength{\textfloatsep}{4pt}
\begin{figure}
    	\centering
    	\includegraphics[width = 0.5\textwidth]{./aognet_block.pdf}
    	\caption{\small{Illustration of the proposed AOG building block.
    	See text for details. (Best viewed in color)} 
    	}
    	\label{fig:AOG-block} 
\end{figure}

\begin{figure*}[t]
    	\centering
    	\includegraphics[width = 0.85\textwidth]{./aognet_model.pdf}
    	\caption{\small{Illustration of an AOGNet which has 3 stages, 1 AOG building bock in the first and third stage, and 2 AOG building blocks in the second stage. Note that different stages can use different AND-OR graphs. We show the same one for simplicity. The front-end can be either a vanilla convolution or convolution+MaxPooling.  
    	%{We also note that before entering the first AOG stage we can apply multiple steps of Conv-BatchNorm-ReLu or front-end stages from other networks such as residual nets. Thus, the proposed AOGNets can be integrated with many other networks.}
    	(Best viewed in color) } \vspace{-2mm}
    	}
    	\label{fig:model} %\vspace{-4mm}
\end{figure*}

On the one hand, this paper is motivated by the technical question: Can we unify all the best practices developed in the popular networks? On the other hand, \textit{Compositionality, reconfigurability and lateral connectivity} are well-known principles in cognitive science, neuroscience and pattern theory~\cite{Geman_CompositionSystems,Mumford_PT,Grenander_PT,RCN,ProbabilisticProgramInduction,RCN}. They are fundamental for the remarkable capabilities possessed by humans, of learning rich knowledge and adapting to different environments quickly, especially in vision and language. They have not been, however, fully and explicitly integrated in DNNs. This paper aims at addressing these issues. As illustrated in Fig.~\ref{fig:AOG-block}, \textit{this paper presents a method of deeply integrating hierarchical and compositional grammars and DNNs for harnessing the best of both worlds in representation learning.}  As David Mumford pointed out, ``Grammar in language is merely a recent extension of much older grammars that are built into the brains of all intelligent animals to analyze sensory input, to structure their actions and even formulate their thoughts."~\cite{Mumford}.    



Grammar models are well known in both natural language processing and computer vision. Image grammar~\cite{Zhu_Grammar,Pff_Grammar,Yuille_AndOr,Geman_CompositionSystems} was one of the dominant methods in computer vision before the recent resurgence in popularity of deep neural networks. With the recent resurgence, one fundamental puzzle arises that grammar models with more explicitly compositional structures and better analytic and theoretical potential, often perform worse than their neural network counterparts. The proposed method bridges the performance gap, which is motivated by, and aims at showing, the advantage of two nice properties of grammars which are desirable in network engineering: (i) The flexibility and simplicity of constructing different types of structural topologies based on a dictionary of primitives and a set of production rules in a principled way; and (ii) The highly expressive power and the parsimonious  compactness of their explicitly hierarchical and compositional structures. %Furthermore, the explainable rigor of grammar could be harnessed potentially to address the intepretability issue of deep neural networks (which is out of the scope of this paper). 




\subsection{Method Overview}
In this paper, we utilize AND-OR Grammars (AOG) in implementing our deep compositional grammatical architectures. We follow the general AOG framework~\cite{Zhu_Grammar,Yuille_AndOr} and specifically extend the method presented in~\cite{DisAOT-CVPR,TLP-PAMI} in constructing AOG building blocks. Fig.~\ref{fig:AOG-block} shows the proposed AOG building block. We call the resulting networks \textbf{AOGNets}. An AOGNet consists of a stack of AOG building blocks. Fig.~\ref{fig:model} illustrates a 3-stage AOGNet. 

 An AOG building block splits its input feature map into $N$ groups along feature channels, and treat it as a sentence of $N$ words. It then jointly realizes a phrase structure grammar (vertical composition)~\cite{Syntactic, Geman_CompositionSystems,DPM,Zhu_Grammar,Yuille_AndOr,DisAOT-CVPR} and a dependency grammar (horizontal connections in pink in Fig.~\ref{fig:AOG-block})~\cite{DependencyGrammar,Zhu_Grammar,RCN} in bottom-up parsing the ``sentence" for better feature exploration and exploitation, thus embodying Mumford's vision on grammars to analyze sensory inputs~\cite{Mumford} in visual recognition and Hinton's quest on more diverse neural architectures.  
 \begin{itemize}[leftmargin=*]
 \itemsep0em
     \item The phrase structure grammar component is a 1-D special case of the method presented in~\cite{DisAOT-CVPR, TLP-PAMI}. It can also be understood as a modified version of the well-known Cocke–-Younger–-Kasami (CYK) parsing algorithm in natural language processing according to a binary composition rule. 
     \item The dependency grammar component is integrated to capture lateral connections and improve the representational flexibility and power. 
 \end{itemize}

In an AOG building block, each node applies some basic operation $\mathcal{T}(\cdot)$ (e.g., Conv-BatchNorm-ReLU) to its input. There are three types of nodes: 
\begin{itemize}[leftmargin=*]
 \itemsep0em
    \item A \textit{Terminal-node} takes as input a channel-wise slice of the input feature map (i.e., a $k$-gram). 
    \item An \textit{AND-node} explores composition, whose input is computed by concatenating features of its syntactic child nodes, and adding the lateral connection if had;
    \item An \textit{OR-node} represents alternative ways of composition in the spirit of exploitation, whose input is the element-wise sum of features of its syntactic child nodes and the lateral connection if had.
\end{itemize}
%Note that the output feature map usually has smaller spatial sizes through the sub-sampling used in the Terminal-node operations and larger number of feature channels.

Our AOG building blocks unify the best practices developed in popular networks stated above in that,
\begin{itemize}[leftmargin=*]
 \itemsep0em
    \item Terminal-nodes implement the split-transform heuristic (or group convolutions) as done in GoogLeNets~\cite{InceptionNet} and ResNeXts~\cite{ResNeXt}, but at multiple levels (including overlapped group convolutions). They also implement the skip-connection at multiple levels. Unlike the cascade-based stacking scheme in ResNets, DenseNets and DPNs, Termninal-nodes can be computed in parallel to improve efficiency. Non-terminal nodes implement aggregation. 
    \item AND-nodes implement DenseNet-like aggregation (i.e., concatenation)~\cite{DenseNet} for feature exploration. 
    \item OR-nodes implement ResNet-like aggregation (i.e., summation)~\cite{ResidualNet} for feature exploitation.
    \item The hierarchy facilitates gradual increase of feature channels as in Deep Pyramid ResNets~\cite{DPRN}, and also leads to good balance between depth and width of networks. 
    \item The compositional structure provides much more flexible information flows than DPN ~\cite{DPN} and the DLA~\cite{DLA}.
    \item The lateral connections increase the depth of nodes on the flow without introducing extra parameters. 
\end{itemize}



 

In experiments, we test our AOGNets on three highly competitive and widely used image classification  benchmarks: the CIFAR-10 dataset and the CIFAR-100 dataset~\cite{CIFAR},  and the ImageNet-1K dataset~\cite{ImageNet}. Our AOGNets obtain better performance consistently than  ResNets~\cite{ResidualNet} and most variants, ResNeXts~\cite{ResNeXt}, DenseNets~\cite{DenseNet} and DualPathNets~\cite{DPN} when model sizes are comparable. We also test AOGNets in object detection on the PASCAL VOC 2007 and 2012~\cite{VOC}. We adopt the vanilla Faster R-CNN~\cite{FasterRCNN} system using AOGNets as backbone. We obtain better performance by about $3\%$ mean average precision than the one with larger ResNets as the backbones. %We also provide a detailed ablation study analyzing different aspects of our AOGNets. 




\section{Related Work and Our Contributions}
 Neural architectures are the foundation for improving performance of DNNs. 
The majority of existing methods are still based on hand-crafted architectures. A promising trend is to automatically learn better architectures with the long-term objective to have theoretical guarantee. So far, hand-crafted ones have better overall performance, especially on large-scale datasets such as the ImageNet benchmark~\cite{ImageNet}. We focus on hand-crafted architectures in this section. Related work on automatic neural architecture search is referred to the nice survey papers~\cite{NAS-survey1,NAS-survey2}.

\textbf{Hand-crafted network architectures.} 
After more than 20 years since the seminal work 5-layer LeNet5~\cite{LeCunCNN} was proposed, the recent resurgence in popularity of neural networks was triggered by the 8-layer AlexNet~\cite{AlexNet} with breakthrough performance on ImageNet~\cite{ImageNet} in 2012.
%The AlexNet presented two new insights in the operator space: the Rectified Linear Unit (ReLu) and the Dropout. 
Since then, a lot of efforts were devoted to learn deeper AlexNet-like networks with the intuition that deeper is better. The VGG Net~\cite{VGG} proposed a 19-layer network with insights on using multiple successive layers of small filters (e.g., $3\times 3$) %to obtain the receptive field by one layer with large filter and on adopting smaller stride in convolution to preserve information. 
A special case, $1\times 1$ convolution, was proposed in the network-in-network~\cite{NetInNet} for reducing or expanding feature dimensionality between consecutive layers, and have been widely used in many networks. %The VGG Net also increased computational cost and memory footprint significantly. To address these issues, 
The  22-layer GoogLeNet~\cite{GoogLeNet} introduced the first inception module and a bottleneck scheme implemented with $1\times 1$ convolution for reducing computational cost. The main obstacle of going deeper lies in the gradient vanishing issue in optimization, which is addressed with a new structural design, short-path or skip-connection, proposed in the Highway network~\cite{HighwayNet} and popularized by the ResNets~\cite{ResidualNet}, especially when combined with the batch normalization~\cite{BatchNorm}. More than 100 layers are popular design in the recent literature~\cite{ResidualNet,InceptionNet}, as well as even more than 1000 layers trained on large scale datasets such as ImageNet~\cite{StochasticResNet,PolyNet}. The Fractal Net~\cite{FractalNet} and deeply fused networks~\cite{DFN} provided an alternative way of implementing short path for training ultra-deep networks without residuals. Complementary to going deeper, width matters in ResNets and inception based networks too~\cite{WideResNet,ResNeXt,IGC1}. Going beyond the first-order  skip-connections in ResNets, DenseNets~\cite{DenseNet} proposed a densely connected network architecture with concatenation scheme for feature reuse and exploration, and DPNs~\cite{DPN} proposed to combine residuals and densely connections in an alternating way for more effective feature exploration and exploitation. DLA networks~\cite{DLA} further develop iterative and hierarchical  aggregation schema with very good performance obtained.  
%Both skip-connection and dense-connection adapt the sequential architecture to directed and acyclic graph (DAG) structured networks, which were explored earlier in the context of recurrent neural networks (RNN)~\cite{DAGRNN,DAGRNN1} and ConvNets~\cite{DAGCNN}.   
Most work focused on boosting spatial encoding and utilizing spatial dimensionality reduction. The squeeze-and-excitation module~\cite{SENet} is a recently proposed simple yet effective method focusing on channel-wise encoding. The Hourglass network~\cite{Hourglass} proposed a hourglass module consisting of both subsampling and upsampling to enjoy repeated bottom-up/top-down feature exploration.

Our AOGNet is created by intuitively simple yet principled grammars. It shares some spirit with the inception module~\cite{InceptionNet}, the deeply fused nets~\cite{DFN} and the DLA~\cite{DLA}. 

% \textbf{Learned network architectures.} There is less work on this direction. Even with very strong assumptions (e.g., limited number of stages and limited set of operators), the search space still grows exponentially due to the product space. Bayesian hyper-parameter optimization is one of the popular methods for network architecture  search in some restricted space~\cite{NetSearch_Bergstra,NetSearch_Mendoza}. More recently, by posing the structure and connectivity of a neural network as a variable-length string, the network architecture search work~\cite{RL4NetworkStructSearch} utilizes a recurrent network to generate a such string (i.e., network topology) under the reinforcement learning framework with the validation accuracy of intermediate models as reward. This automatic exploration of network topology entails very high demand on computing resource (e.g., 800 GPUs used in the experiments). Genetic algorithms are also explored in learning network structures~\cite{GeneticCNN}. In~\cite{AdaNet}, the AdaNet was proposed to learn directed acyclic network structures using a theoretical framework with some guarantee. % and a more comprehensive review on theoretical studies on structure learning was given.  


%The proposed AOG building block is potentially useful as a better heuristic in the search or to  facilitate theoretical analyses by taking advantage of the simple production rule in structure composition. 

\textbf{Grammar-like structures.} A general framework of image grammar was proposed in~\cite{Zhu_Grammar}. Object detection grammar was the dominant approaches for object detection~\cite{Pff_Grammar,Yuille_AndOr,DisAOT-CVPR,AOGShape,StochasticGrammar}, and has recently been integrated with DNNs~\cite{YingWu_CompositionalPose,YingWu_CompositionalModel}. Probabilistic program induction~\cite{ProbabilisticPrograming,lake15science,BuildMachineLikePeople} has been used successfully in many settings, but has not shown good performance in difficult visual understanding tasks such as large-scale image classification and object detection. More recently, recursive cortical networks~\cite{RCN} have been proposed with much more data efficiency in learning which adopts the AND-OR grammar framework~\cite{Zhu_Grammar}, showing great potential of grammar like structures in developing general AI systems. 

\textbf{Our contributions.} This paper makes two main contributions in the field of deep representation learning: 
\begin{itemize}[leftmargin=*]
\itemsep0em
    \item It presents a simple yet effective method of deeply integrating grammar models and DNNs, which facilitates both feature exploration and exploitation in a hierarchical and compositional way with nice balance between depth and width. In implementation, we adopt the AND-OR grammars (AOG)~\cite{DisAOT-CVPR,Zhu_Grammar,Yuille_AndOr}. To the best of our knowledge, it is the first work that utilizes grammar models in network engineering. %It sheds light on introducing more sophisticated structured knowledge representation~\cite{knowledgeRepresentation} in network architecture search. 
    \item It obtains better performance than state-of-the-art networks including ResNets~\cite{ResidualNet}, ResNeXts~\cite{ResNeXt}, DenseNets~\cite{DenseNet} and DualPathNets~\cite{DPN} in image classification, and also achieves better object detection performance than the ResNet backbone in Faster R-CNN~\cite{FasterRCNN} detection systems. 
\end{itemize}

%\textbf{Paper Organization.} The remainder  of the paper is organized as follows. Section~\ref{sec:AOGNets} presents details of our AOGNets. Section~\ref{sec:Exp} shows experimental results and comparisons, as well as ablation studies on different aspects of our AOGNets. Finally, Section~\ref{sec:conclusion} concludes this paper. %and discusses some on-going and future work. 




\section{AOGNets} \label{sec:AOGNets}
In this section, we first present details of constructing the structure of our AOGNets. Then, we define node operation functions for nodes in an AOGNet. We also propose a method of simplifying the full structure of an AOG building block which prunes syntactically symmetric nodes.  

\subsection{The Structure of an AOGNet} \label{sec:structure}
An AOGNet consists of a predefined number of stages each of which comprises one or more than one AOG building blocks.  Fig.~\ref{fig:model} shows a 3-stage AOGNet. 

As Fig.~\ref{fig:AOG-block} illustrates, an AOG building block maps an input feature map $F$ with the dimensions  $C\times H\times W$ (representing the number of channels, height and width respectively) to an output feature map $\mathbb{F}$ with the dimensions $\mathbb{C}\times \mathbb{H}\times \mathbb{W}$. \emph{We split the input feature map into $N$ groups along feature channels, and then treat it as a ``sentence of $N$ words".} Each word represents a primitive feature map with the dimensionality $c\times H \times W$ in the input, satisfying $C=N\times c$. In implementation, following a common  convention, we usually reduce the spatial size and increase the number of channels between consecutive stages for bigger receptive field and greater expressive power. %Within a stage, we usually keep the dimensions of input and output same for the AOG building blocks except for the first one. 
Our AOG building blocks integrates two grammars (see Algorithm~\ref{alg:AOG}). 

\textbf{The phrase structure grammar}~\cite{Syntactic,Geman_CompositionSystems,DPM,Zhu_Grammar,Yuille_AndOr,DisAOT-CVPR}. We consider the following three rules in unfolding the configurations of a sentence with $N$ words: 
\begin{align}
    S_{i,j}  \rightarrow & \quad t_{i,j}, \\
    S_{i,j}(m)  \rightarrow & \quad L_{i, i+m}\cdot R_{i+m+1, j}, \quad 0\leq m <k, \\
    S_{i,j} \rightarrow & \quad S_{i,j}(0) | S_{i,j}(1) | \cdots | S_{i,j}(j-i).
\end{align}
where $S_{i,j}$ represents a symbol for parsing the sub-sentence starting at the $i$-th word ($i\in [0, N-1]$) and ending at the $j$-th word ($j\in[0, N-1], j\geq i$) and its length equals $k=j-i+1$. 
\begin{itemize}[leftmargin=*]
\itemsep0em
    \item The first rule is a termination rule which grounds the non-terminal symbol $S_{i,j}$ directly to the corresponding sub-sentence $t_{i,j}$, i.e., a $k$-gram terminal symbol, which is represented by a \textbf{ Terminal-node}. 
    \item The second rule is a binary decomposition rule which decomposes the non-terminal symbol $S_{i,j}$ into two child symbols representing a left sub-sentence and a right sub-sentence respectively: $L_{i,i+m}$ and $R_{i+m+1,j}$, both of which are either a non-terminal symbol or a terminal symbol depending on $m$. It is represented by an \textbf{AND-node}, and entails \textbf{the concatenation scheme} in forward computation. 
    \item The third rule represents alternative ways of decomposing a symbol $S_{i,j}$, which is represented by an \textbf{OR-node}, and entails \textbf{summation scheme} in forward computation to ``integrate out" the decomposition structures. 
\end{itemize}

% \begin{Algorithm} %[H]
% \SetAlgoLined
% \KwIn{ (1) The number of groups, $N$ used to split the input feature map along feature channels ($N\geq 2$). (2) The number of branching factor, $k$ for AND-nodes ($k\geq 2$). (3) The flag, $lateral$ of whether using dependency grammar or not to add lateral connections ($true$ by default). (4) The flag, $pruning$ of whether pruning the syntactically symmetric nodes of OR-nodes ($true$ by default). %The min. and max. length of $k$-gram: $k_0$ and $k_1$.
% }
% \KwOut{The AND-OR Grammar building block graph $\mathcal{G}=<V, E>$}
% Initialization: Create an OR-node $O_{0,N-1}$ for the entire feature map, $V=\{O_{0,N-1}\}, E=\emptyset$, Breadth-First-Search (BFS) queue $Q=\{O_{0,N-1}\}$\;
% \While{$Q$ is not empty}{
% Pop a node $v_{i,j}$ out of $Q$ and let $n=j-i+1$\; 
% \uIf{$v_{i,j}$ is an OR-node}{
%       	i) Add a terminal-node $t_{i,j}$, and update $V=V\cup\{t_{i,j}\},\, E=E\cup\{<v_{i,j}, t_{i,j}>\}$\;
%       	ii) Create AND-nodes $A_{i,j}(m)$ for all valid splits with $2$ to $k$ child nodes. A split is indexed by $m$. $m=(c, s_1, \cdots, s_{c-1})$ where $c$ is the number of child nodes ($2\leq i\leq k$), and $i < s_1 < \cdots < s_{c-1} < j$\;
%       	For each $m$, set $Add=true$\;
%       	\If{$pruning=true$ and $A_{i,j}(m)$ is syntactically symmetric to an existing AND-node $A_{i,j}(m'),\, m'<m$}{
%       	    set $Add = false$
%       	}
%       	\If{$Add=true$}{
%       	$E=E\cup\{<v_{i,j}, A_{i,j}(m)>\}$\;
%       	\If{$A_{i,j}(m)\notin V$} {
%       		$V=V\cup\{ A_{i,j}(m) \}$\; 
%       		Push $A_{i,j}(m)$ to the back of $Q$\;
%       	}
%       	}
% }
% \ElseIf{$v_{i,j}$ is an AND-node with split index $m=(c, s_1, \cdots, s_{c-1})$} {
%       	Create $c$ OR-nodes $\mathcal{O}=\{O_{i,i+s_1}, \cdots, O_{i+s_{c-1}+1, j}\}$ for the $c$ sub-groups of feature maps respectively\;
%       	$E=E\cup\{<v_{i,j}, O_{i,i+s_{1}}>, \cdots,  <v_{i,j}, O_{i+s_{c-1}+1,j}>\}$\;
%       	For each $O\in \mathcal{O}$,\;
%       	\If{$O \notin V$} {
%       		$V=V\cup\{ O \}$\;
%       		Push $O$ to the back of $Q$\;
%       	}
% }  
% }
% \If{$lateral=true$}{
% Set $n=1$ \;
% \While{$n\leq N$}{
% i) Find all AND-nodes $A_{i,j}$ with the length $j-i+1=n$ and let $\mathcal{A}_n=\{A_{i,j}\,|\ j-i+1=n\}$\;
% \quad i.a) Sort the set $\mathcal{A}_n$ using the splits used by the AND-nodes.\;
% \quad i.b) Add lateral connections between successive AND-nodes in the sorted set $\mathcal{A}_n$.\;
% ii) Find all OR-nodes $O_{i,j}$ with the length $j-i+1=n$ and let $\mathcal{O}_n=\{O_{i,j}\,|\ j-i+1=n\}$\;
% \quad i.a) Sort the set $\mathcal{O}_n$ using the start index $i$'s of OR-nodes.\;
% \quad i.b) Add lateral connections between successive AND-nodes in the sorted set $\mathcal{O}_n$.\;
% iii) $n=n+1$
% }
% }

% \caption{Constructing the AND-OR Grammar (AOG) building block}\label{alg:AOG1} 
% \end{Algorithm}

\begin{algorithm} %[H]
\SetAlgoLined
\KwIn{The total length (or primitive size) $N$. %The min. and max. length of $k$-gram: $k_0$ and $k_1$.
}
\KwOut{The AND-OR Graph $\mathcal{G}=<V, E>$}
Initialization: Create an OR-node $O_{0,N-1}$ for the entire sentence, $V=\{O_{0,N-1}\}, E=\emptyset$, BFS queue $Q=\{O_{0,N-1}\}$\;
\While{$Q$ is not empty}{
Pop a node $v_{i,j}$ from the $Q$ and let $k=j-i+1$\; 
\uIf{$v_{i,j}$ is an OR-node}{
      	i) Add a terminal-node $t_{i,j}$, and update $V=V\cup\{t_{i,j}\},\, E=E\cup\{<v_{i,j}, t_{i,j}>\}$\;
      	ii) Create AND-nodes $A_{i,j}(m)$ for all valid splits $0\leq m<k$\;
      	$E=E\cup\{<v_{i,j}, A_{i,j}(m)>\}$\;
      	\If{$A_{i,j}(m)\notin V$} {
      		$V=V\cup\{ A_{i,j}(m) \}$\; 
      		Push $A_{i,j}(m)$ to the back of $Q$\;
      	}
}
\ElseIf{$v_{i,j}$ is an AND-node with split index $m$} {
      	Create two OR-nodes $O_{i,i+m}$ and $O_{i+m+1, j}$ for the two sub-sentence respectively\;
      	$E=E\cup\{<v_{i,j}(m), O_{i,i+m}>, <v_{i,j}(m), O_{i+m+1,j}>\}$\;
      	\If{$O_{i,i+m}\notin V$} {
      		$V=V\cup\{ O_{i,i+m} \}$\;
      		Push $O_{i,i+m}$ to the back of $Q$\;
      	}
      	\If{$O_{i+m+1,j}\notin V$} {
      		$V=V\cup\{ O_{i+m+1, j} \}$\;
      		Push $O_{i+m+1,j}$ to the back of $Q$\;
      	}
}  
}
Add lateral connections between between non-terminal nodes of the same type (AND-node or OR-node) with the same length.  
\caption{Constructing an AOG building block}\label{alg:AOG} 
\end{algorithm}

\textbf{The dependency grammar}~\cite{DependencyGrammar,RCN,Zhu_Grammar}. We introduce dependency grammar to model lateral connections between non-terminal nodes of the same type (AND-node or OR-node) with the same length (i.e., $k=j-i+1$ in the three rules). As illustrated by the arrows in pink in Fig.~\ref{fig:AOG-block}, we add lateral connections in a simple way: (i) For the set of OR-nodes with $k\in [1, N-1]$, we first sort them based on the starting index $i$; and (ii) For the set of AND-nodes with $k\in [2, N]$, we first sort them based on the lexical orders of the pairs of starting indexes of the two child nodes. Then, we add sequential lateral connections for nodes in the sorted set either from left to right or from right to left. We usually use opposite lateral connection directions for AND-nodes and OR-nodes to have globally consistent lateral flow from bottom to top in an AOG building block.  

%The AOG building block is constructed by the Algorithm~\ref{alg:AOG} using the breadth-first search (BFS) order.

% \begin{figure} [t]
%     	\centering
%     	\includegraphics[width = 0.5\textwidth]{./Fig/operator1.pdf}
%     	\caption{\small{Illustration of node operators in an AOGNet. \textit{Left}: a Terminal-node grounds on a chunk of the input feature map, \textit{Middle}: an AND-node concatenates the outputs of its two child nodes, and \textit{Right}: an OR-node sums up the outputs of its child nodes. The same type of operator $\mathcal{T}()$ is applied to compute the outputs.  %(Best viewed in color) 
%     	}
%     	}
%     	\label{fig:operator} 
% \end{figure}

\begin{figure} %[t]
    	\centering
    	\includegraphics[width = 0.48\textwidth]{./aognet_pruning.pdf}
    	\caption{\small{Illustration of simplifying the AOG building blocks by pruning syntactically symmetric child nodes of OR-nodes. \textit{Left:} An AOG building block with full structure consisting of  $10$ Terminal-nodes, $10$ AND-nodes and $10$ OR-nodes. Nodes and edges to be pruned are plotted in yellow. \textit{Right:} The simplified AOG building block consisting of  $8$ Terminal-nodes, $5$ AND-nodes and $8$ OR-nodes. Here, lateral connections are not shown for clarity. (Best viewed in color)}
    	}
    	\label{fig:pruning} %\vspace{-4mm}
\end{figure}


\subsection{Node Operations in an AOGNet}
In an AOG building bock, all nodes use the sample type of transofrmation function $\mathcal{T}(\cdot)$ (see Fig.~\ref{fig:AOG-block}). For a node $v_{i,j}$ with length $k=j-i+1$, denote by $f^v_{i,j}$ its input feature map,  and $\mathbf{f}^v_{i,j}$ its output feature map computed by $\mathbf{f}^v_{i,j}=\mathcal{T}(f^v_{i,j})$. We have, 
\begin{itemize}[leftmargin=*]
\itemsep0em
    \item For a Terminal-node $t_{i,j}$, denote by $F_{i,j}$ the corresponding $k$-gram chunk in the input feature map $F$. We have its input $f^t_{i,j}=F_{i,j}$ with the dimensionality $c^v_{i,j}\times H \times W$, and its output $\mathbf{f}^t_{i,j}=\mathcal{T}(F_{i,j})$ with the dimensionality $\mathbf{c}^v_{i,j}\times \mathbb{H} \times \mathbb{W}$, where $c^v_{i,j}=k\times c$ and $\mathbf{c}^v_{i,j}=k\times {\mathbb{C}\over N}$.  
    \item For an AND-node $A_{i,j}(m)$, its input is computed by the concatenation of the outputs of its two syntactic child nodes, $\mathbf{f}^L_{i,i+m}$ and $\mathbf{f}^R_{i+m+1, j}$. We have $f^A_{i,j}=[\mathbf{f}^L_{i,i+m}, \mathbf{f}^R_{i+m+1, j}]$. If it has a lateral child node whose output is denoted by $\mathbf{f}^A_{lateral}$, we add it to $f^A_{i,j}$, i.e.,  $f^A_{i,j}=[\mathbf{f}^L_{i,i+m}, \mathbf{f}^R_{i+m+1, j}]+\mathbf{f}^A_{lateral}$.   
    \item For an OR-node $O_{i,j}$, its input  is the summation of the outputs of its child nodes, $f^O_{i,j}=\sum_{u_{i,j}\in ch(O_{i,j})} \mathbf{f}^u_{i,j}$, where $ch(\cdot)$ be the set of child nodes. If it has a lateral child node whose output is denoted by $\mathbf{f}^O_{lateral}$, we add it to $f^O_{i,j}$, i.e.,  $f^O_{i,j}=\sum_{u_{i,j}\in ch(O_{i,j})} \mathbf{f}^u_{i,j}+\mathbf{f}^O_{lateral}$. 
\end{itemize}
Where the input and output of an AND-node or an OR-node, $v_{i,j}$, have the same dimensionalities: $\mathbf{c}^v_{i,j} \times \mathbb{H}\times \mathbb{W}$ and $\mathbf{c}^v_{i,j}=k\times {\mathbb{C}\over N}$.  
In learning and inference, we follow the depth-first search (DFS) order to compute nodes in an AOG building block, which ensures that all the child nodes have been computed when we compute a node $v$.   

% \begin{figure} [t]
%     	\centering
%     	\includegraphics[width = 0.4\textwidth]{./Fig/bottleneck.pdf}
%     	\caption{\small{Illustration of the bottleneck node operation. The 4-tuple of convolution represents (number of channels, kernel height, kernel width, stride). $\alpha$ is the bottleneck ratio (e.g., $\alpha=0.25$). The stride $s$ is determined by the spatial sizes of input and output feature maps. (Best viewed in color) }
%     	}
%     	\label{fig:bottleneck} 
% \end{figure}

\subsection{Simplifying AOG Building Blocks} 
The phrase structure grammar is syntactically redundant since it aims to explore all possible configurations w.r.t. the binary compositional rule. In representation learning, we usually want to increase the feature dimensions of different stages in a network for improving the representational power without increasing the total number of parameters too much. To balance the structural complexity and the feature dimensions in our AOG building blocks, we propose to simplify the structure of AOG building blocks by pruning. The pruning adopts a simple method: We follow the BFS order of nodes, and for each encountered OR-node we only keep the child nodes which do not have left-right syntactically symmetric counterparts in the current set of child nodes. For example, consider the four child nodes of the root OR-node in the left of Fig.~\ref{fig:pruning}, the fourth child node is removed since it is symmetric to the second one. Following the BFS, we can extract the pruned AOG building block. 







\section{Experiments}\label{sec:Exp}
In this section, we test our AOGNets on three highly competitive image classification benchmarks: CIFAR-10 and CIFAR-100~\cite{CIFAR}, and ImageNet-1K~\cite{ImageNet},  and on the PASCAL VOC 2007 and 2012 benchmarks~\cite{VOC}. We implemented our AOGNets using PyTorch. % (version 0.4.1).   

\subsection{Implementation Settings}
In our experiments, we use simplified AOG building blocks. We also use the same building block for all stages for simplicity. For node operations $\mathcal{T}()$'s, we use the bottleneck %(see Fig.~\ref{fig:bottleneck})
variant of Conv-BatchNorm-ReLU,  as done in ResNets~\cite{ResidualNet}, which adds one $1\times 1$ convolution before and after the operation to first reduce the dimensionality and then expand it back respectively. We notice that we can apply different AOG building blocks and node operations for different types of nodes as long as we can match the dimensions during the computation. We keep them simple in this paper. We leave the exploration of different operators in future work.   

{The depth of an AOGNet} is defined by the largest number of units which have learnable parameters along the paths from the final output to the input data following BFS order. \emph{E.g.}, the longest path in the simplified AOG building block in Fig.~\ref{fig:pruning} is $8$, and a bottleneck operation is counted as $3$ units, so the depth of the simplified AOG building block is counted as $24$. 
In comparison, to indicate the specifications, AOGNets will be written by AOGNet-\textit{PrimitiveSize-(\#AOG blocks per stage)}-[\textit{OutputFeatDim}]. \emph{E.g.},  AOGNet-4-(1,1,1,1)-256d represents a 4-stage AOGNet with 1 AOG building block per stage, primitive size being 4, and the final output feature dimension 256.

 \begin{table}[t]
        \centering 
        \small{
        \resizebox{0.48\textwidth}{!}{
        \begin{tabular}{c|ccc|cc}
        \hline 
        Method & Depth & \#Params & FLOPs & C10 & C100\\ \hline
        ResNet~\cite{ResidualNet} & 110 & 1.7M & 0.251G & 6.61 & - \\ \hline
        ResNet (reported by~\cite{StochasticResNet}) & 110 & 1.7M & 0.251G & 6.41 & 27.22 \\ \hline
        \multirow{2}{*}{ResNet (pre-activation)~\cite{ResNetPreAct}} & 164 & 1.7M & 0.251G & 5.46 & 24.33 \\ 
        & 1001 & 10.2M & - & 4.62 & 22.71 \\ \hline
        Wide ResNet~\cite{WideResNet} & 16 & 11.0M & -& 4.81 & 22.07 \\ \hline
        DenseNet-BC~\cite{DenseNet} ($k = 12$) & 100 & 0.8M & 0.292G & 4.51 & 22.27 \\ \hline
        \textbf{AOGNet-4-(1,1,1)-252d} & 74 & \textbf{0.78M} & \textbf{0.123G} & \textbf{4.37} & \textbf{20.95} \\ \Xhline{2\arrayrulewidth}
        
        DenseNet-BC~\cite{DenseNet} ($k = 24$) & 250 & 15.3M & 5.46G &  3.62 & 17.60\\ \hline
		\textbf{AOGNet-4-(1,1,1)-1152d} & 98 & 15.8M & \textbf{2.4G} & \textbf{3.42} & \textbf{16.93} \\ \Xhline{2\arrayrulewidth}

        Wide ResNet~\cite{WideResNet} & 28 & 36.5M & 5.24G & 4.17 & 20.50 \\ \hline
        FractalNet~\cite{FractalNet} & 21 & 38.6M &- & 5.22 & 23.30 \\
        with Dropout/DropPath & 21 & 38.6M &- & 4.60 & 23.73 \\ \hline
        ResNeXt-29, $8\times64$d~\cite{ResNeXt} & 29 & 34.4M & 3.01G & 3.65 & 17.77 \\ 
        ResNeXt-29, $16\times64$d~\cite{ResNeXt} & 29 & 68.1M & 5.59G & 3.58 & 17.31 \\ \hline
        %DenseNet~\cite{DenseNet} ($k = 12$)  & 40  & 1.0M & & 5.24 & 24.42 \\
		%DenseNet~\cite{DenseNet} ($k = 12$) & 100 & 7.0M & & 4.10 & 20.20 \\
		%DenseNet~\cite{DenseNet} ($k = 24$) & 100 & 27.2M & & 3.74 & 19.25 \\
		
		DenseNet-BC~\cite{DenseNet} ($k = 40$) & 190 & 25.6M & 9.35G & 3.46 & 17.18 \\ \hline
		\textbf{AOGNet-4-(1,1,1)-1444d} & 98 & \textbf{24.8M} & 3.7G & \textbf{3.27} & \textbf{16.63} \\ \hline 
		%AOGNet-4-(1,1,1) & 23 & 1.0M & & 5.29 & 25.98\\
		%AOGNet-4-(1,1,1) & 23 & 8.1M & & 4.02 & 20.64 \\
		%AOGNet-4-(1,1,1) & 23 & 16.0M & & 3.79 & 19.50 \\
		%AOGNet-4-(1,2,1) & 37 & 32.1M & & 3.74 & 18.89\\
		%AOGNet-BN-4-(1,1,1) & 65 & 1.0M & 0.151G & 4.74 & 22.81 \\ 
		%AOGNet-BN-4-(1,1,1) & 65 & 8.0M & 1.22G & 3.99 & 18.71 \\ 
		%AOGNet-BN-4-(1,2,1) & 86 & 16.0M & 2.70G & 3.78 & 17.82\\  
		%% TF: This ratio between improvement and complexity is so tiny and reviewers might attack us, especially someone wants to reject us.
		%AOGNet-BN-4-(1,2,1) & 86 & 32.5M & 5.50G & 3.66 & 17.74 \\ \hline
        \end{tabular} 
        } 
        }
        \\ [1ex]
        \caption{Error rates (\%) on the two CIFAR datasets~\cite{CIFAR}. \#Params uses the unit of Million. $k$ in DenseNet refers to the growth rate. %``+'' indicates standard data augmentation (translation and/or mirroring).
        }\label{table:cifar-results} 
    \end{table}
    
   
 

%After the AOG structure is specified, {the number of parameters} of an AOGNet is determined by the number of channels of input/output of each stage. Thus, for an $M$-stage AOGNet, we have an $(M+1)$-tuple specifying the number of channels. For example, we can specify the 4-tuple, $(16, 16, 32, 64)$ or $(16, 32, 64, 128)$ for a 3-stage AOGNet, resulting different number of parameters in total. 





    
\subsection{Experiments on CIFAR}
CIFAR-10 and CIFAR-100 datasets~\cite{CIFAR}, denoted by C10 and C100 respectively,  consist of $32\times32$ color images drawn from 10 and 100 classes. The training and test sets contains $50,000$ and $10,000$ images respectively. We adopt widely used standard data augmentation scheme, random cropping and mirroring, in preparing the training data. %We denote this data augmentation by a ``+'' mark ($e.g.$, CIFAR-10+). 

We train AOGNets with stochastic gradient descent (SGD) for $300$ epochs with random parameter initialization. The front-end (see Fig.~\ref{fig:model}) uses a single convolution layer. The initial learning rate is set to $0.1$, and is divided by $10$ at $150$ and $225$ epoch respectively. For CIFAR-10, we chose batch size $64$ with weight decay $1\times10^{-4}$, while batch size $128$ with weight decay $5\times10^{-4}$ is adopted for CIFAR-100. The momentum is set to $0.9$.  

   
\textbf{Results and Analyses.} We summarize the results in Table~\ref{table:cifar-results}. With smaller model sizes and much reduced computing complexity (FLOPs), our AOGNets obtain better performance than ResNets~\cite{ResidualNet} and some of the variants, ResNeXts~\cite{ResNeXt} and DenseNets~\cite{DenseNet} consistently on both datasets. Our small AOGNet ($0.78M$) already outperforms the ResNet~\cite{ResidualNet} ($10.2M$) and the WideResNet~\cite{WideResNet} ($11.0M$). Since the same node operation is used, the improvement must come from the AOG building block structure. Compared with the DenseNets, our AOGNets improve more on C100, and use less than half FLOPs for comparable model sizes. The reason for the reduced FLOPs is that DenseNets apply down-sampling after each Dense block, while our AOGNets sub-sample at Terminal-nodes.  

\subsection{Experiments on ImageNet-1K}

The ILSVRC 2012 classification dataset~\cite{ImageNet} consists of about $1.2$ million images for training, and $50,000$ for validation, from $1,000$ classes. We adopt the same data augmentation scheme (random crop and horizontal flip) for training images as done in~\cite{ResidualNet, ResNetPreAct, DenseNet}, and apply a single-crop with size $224\times224$ at test time. Following the common protocol,  we evaluate the top-1 and top-5 classification error rates on the validation set.

\begin{table}
    \centering
    \small{
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c}
    \hline 
    Method & \#Params & FLOPS & top-1 & top-5 \\ \hline
    ResNet-101~\cite{ResidualNet} & 44.5M & 8G & 23.6 & 7.1 \\ 
    ResNet-152~\cite{ResidualNet} & 60.2M & 11G & 23.0 & 6.7 \\ \hline
    ResNeXt-50~\cite{ResNeXt} & 25.03M & 4.2G & 22.2 & 5.6 \\ 
    ResNeXt-101 ($32\times 4d$)~\cite{ResNeXt} & 44M & 8.0G & 21.2 & 5.6 \\
    ResNeXt-101 ($64\times 4d$)~\cite{ResNeXt} & 83.9M & 16.0G & 20.4 & 5.3 \\ \hline
    DensetNet-161~\cite{DenseNet}  & 27.9M & 7.7G & 22.2 & - \\ 
    DensetNet-169~\cite{DenseNet}  & 13.5M & ~4G & 23.8 & 6.85 \\
    DensetNet-264~\cite{DenseNet}  & 33.4M & - & 22.2 & 6.1 \\ \hline
    ResNeXt-50+SE~\cite{SENet} & 25M & 4.3G & 21.1 & 5.49 \\
    ResNeXt-101+SE~\cite{SENet} & 44M & 8.0G & 20.70 & 5.01 \\ \hline
    DPN-68~\cite{DPN} & 12.8M & 2.5G & 23.57 & 6.93 \\ 
    DPN-92~\cite{DPN} & 38.0M & 6.5G & 20.73 & 5.37 \\
    DPN-98~\cite{DPN} & 61.6M & 11.7G & 20.15 & 5.15 \\ \hline
    {AOGNet-4-(1,1,2,1)-800d} & {11.7M} & {2.19G} & {22.34} & {6.16}  \\ 
    {AOGNet-4-(1,1,3,1)-1400d} & 40.3M & 7.56G & {20.08} & {5.07}  \\ 
    {AOGNet-4-(1,1,4,1)-1800d} & 60.2M & 12.69G & {19.73} & {4.91}  \\ \hline
    \end{tabular} 
    }
    }
    \\ [1ex]
    \caption{The top-1 and top-5 error rates (\%) on the ImageNet-1K validation set using single model and single-crop testing. 
    %We note that our AOGNet (40.31M) was trained with a tuned batch size (180) due to GPU memory limit, which potentially affected the performance (see text for analyses). 
    %Note that results of DualPathNet are taken from the author's Github (\url{https://github.com/cypw/DPNs}) which utilized an extra trick,  Max-Mean pooling, before the softmax with better performance obtained. 
    }\label{table:imagenet-results} 
\end{table}

\begin{table}
    \centering
    \small{
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c}
    \hline 
    Method & \#Params & FLOPS & top-1 & top-5 \\ \hline
    MobileNetV1~\cite{howard2017mobilenets} & 4.2M & 575M & 29.4 & 10.5 \\ 
    SqueezeNext~\cite{gholami2018squeezenext} & 4.4M & - & 30.92 & 10.6  \\
    ShuffleNet (1.5)~\cite{zhang2017shufflenet} & 3.4M & 292M & 28.5 & - \\ 
    ShuffleNet (x2)~\cite{zhang2017shufflenet} & 5.4M & 524M & 26.3 & - \\ 
    CondenseNet (G=C=4)~\cite{huang2017condensenet} & 4.8M & 529M & 26.2 & 8.3 \\ 
    MobileNetV2~\cite{sandler2018mobilenetv2} & 3.4M & 300M & 28.0 & 9.0 \\ 
    MobileNetV2 (1.4)~\cite{sandler2018mobilenetv2} & 6.9M & 585M & 25.3 & 7.5 \\ 
    NASNet-C (N=3)~\cite{zoph2017learning} & 4.9M & 558M & 27.5 & 9.0 \\ \hline
    {AOGNet-4-(1,1,2,1)-608d} & 4.6M & 546M & {26.5} & {8.4}  \\ \hline
    %AOGNet-v1 & 5.3M & 0.7G & 26.65 & 8.4 \\ \hline
    %% TF: This ratio between improvement and complexity is so tiny and reviewers might attack us, especially someone wants to reject us.
    %AOGNet-BN-4-(1,1,2,1) & 93.2M & 21.2 & 5.73 \\ \hline
    \end{tabular} }}
    \\ [1ex]
    \caption{The top-1 and top-5 error rates (\%) on the ImageNet-1K validation set using single model and single-crop testing. %Note that results of DualPathNet are taken from the author's Github (\url{https://github.com/cypw/DPNs}) which utilized an extra trick,  Max-Mean pooling, before the softmax with better performance obtained. 
    }\label{table:imagenet-results-small} 
\end{table}

   

\begin{table*}[!ht]
        \centering 
        \small{
        \resizebox{\textwidth}{!}{
        \begin{tabular}{c|c|c|c|*{13}{p{0.38cm}}*{3}{p{0.46cm}}*{4}{p{0.4cm}}}
        \hline 
        & Method & \#params & mAP & areo & bike & bird & boat & bottle & bus & car & cat & chair & cow & table & dog & horse & mbike & person & plant & sheep & sofa & train & tv  \\ \hline
        \multirow{3}{*}{07trainval/07test} 
        & ResNet-101$^*$ & 47.5M & 72.3 & 76.2 & 77.9 & 74.6 & 59.9 & 53.0 & 80.8 & 81.7 & 85.3 & 49.0 & 80.2 & 64.1 & 83.7 & 83.3 & 76.5 & 77.8 & 45.2 & 73.0 & 72.0 & 81.7 & 71.3 \\
        & {AOGNet-4-(1,1,2,1)-800d} & 13.6M & {72.1} & 76.0 & 77.4 & 72.6 & 59.2 & 55.6 & 80.4 & 83.7 & 85.2 & 50.1 & 77.1 & 63.7 & 83.8 & 82.8 & 78.7 & 77.8 & 44.7 & 71.4 & 72.5 & 79.1 & 70.6 \\
        & {AOGNet-4-(1,1,3,1)-1400d} & 43.2M & \textbf{75.8} & 77.2 & 84.4 & 78.0 & 64.5 & 60.6 & 83.5 & 87.2 & 85.8 & 55.4 & 85.1 & 65.7 & 86.0 & 84.5 & 80.3 & 78.9 & 52.4 & 75.5 & 72.6 & 82.6 & 75.9 \\\hline
        
        \multirow{3}{*}{07+12trainval/07test} 
        & ResNet-101~\cite{ResidualNet} & 47.5M & 76.4 & 79.8 & 80.7 & 76.2 & 68.3 & 55.9 & 85.1 & 85.3 & 89.8 & 56.7 & 87.8 & 69.4 & 88.3 & 88.9 & 80.9 & 78.4 & 41.7 & 78.6 & 79.8 & 85.3 & 72.0\\
        & ResNet-101$^*$ & 47.5M & 78.1 & 81.0 & 85.4 & 78.5 & 71.0 & 62.7 & 85.8 & 86.6 & 87.7 & 62.2 & 84.3 & 68.9 & 87.2 & 86.8 & 81.3 & 79.3 & 51.4 & 83.4 & 75.8 & 85.3 & 76.6 \\
        & {AOGNet-4-(1,1,2,1)-800d} & 13.6M & {78.3} & 80.0 & 83.1 & 78.4 & 71.9 & 66.0 & 84.6 & 87.2 & 87.5 & 57.6 & 84.0 & 71.6 & 86.4 & 87.0 & 84.5 & 81.1 & 51.3 & 80.1 & 78.2 & 87.2 & 77.6 \\ 
        & {AOGNet-4-(1,1,3,1)-1400d} & 43.2M & \textbf{81.0} & 87.0 & 86.3 & 81.0 & 72.2 & 70.9 & 87.5 & 88.3 & 89.3 & 64.5 & 85.4 & 74.0 & 89.1 & 88.1 & 84.6 & 83.7 & 55.5 & 85.9 & 81.4 & 87.2 & 78.8 \\\hline 
        %& AOGNet-BN-4-(1,1,2,1) & \\ \hline
        
        \multirow{2}{*}{07+12trainval/12test} & ResNet-101$^*$ & 47.5M & 75.1 & 86.1 & 82.5 & 77.6 & 63.4 & 54.5 & 80.6 & 79.9 & 91.0 & 55.8 & 79.9 & 56.0 & 89.5 & 82.6 & 83.3 & 83.1 & 53.9 & 79.8 & 67.4 & 86.3 & 69.5 \\
        & {AOGNet-4-(1,1,2,1)-800d} & 13.6M & {75.2} & 87.0 & 82.1 & 78.4 & 63.1 & 56.7 & 80.0 & 80.7 & 91.5 & 55.1 & 79.7 & 59.9 & 88.8 & 83.8 & 82.8 & 83.4 & 54.3 & 79.0 & 65.8 & 84.7 & 67.4 \\   
        & {AOGNet-4-(1,1,3,1)-1400d} & 43.2M & \textbf{78.0} & 88.9 & 82.8 & 81.3 & 66.9 & 62.4 & 82.6 & 83.0 & 92.5 & 59.6 & 82.5 & 61.9 & 90.9 & 86.1 & 84.5 & 84.5 & 56.1 & 83.3 & 69.7 & 87.6 & 71.5 \\\hline
        %& AOGNet-BN-4-(1,1,2,1) & \\ \hline
        \end{tabular} }}
        \\ [1ex]
        \caption{Performance comparisons using Average Precision (AP) at the intersection over union (IoU) threshold $0.5$ (AP@$0.5$) in the PASCAL VOC2007 / 2012 dataset.
        %(using the protocol, competition "comp4" trained with ImageNet pretrained models and using only 2007 trainval or 2007+2012 trainval). 
        $^*$ reported based on our re-implementation using the exactly same PyTorch implementation of Faster R-CNN and PyTorch pretrained  ResNet-101 backbone on ImageNet for fair comparisons. The reproduced results of ResNets are better than those reported in the original paper~\cite{ResidualNet}.  
        %The results evaluated by the VOC2012 test server can be viewed at \url{http://host.robots.ox.ac.uk:8080/anonymous/O9GMX1.html} (AOGNets) and \url{http://host.robots.ox.ac.uk:8080/anonymous/DFUPMK.html} (AOGNets) and \url{http://host.robots.ox.ac.uk:8080/anonymous/PKCXAJ.html} (ResNet-101).
        }\label{table:detection} \vspace{-4mm}
    \end{table*}
    


We train AOGNets with SGD for $120$ epochs and random parameter initialization. The front-end (see Fig.~\ref{fig:model}) uses three $3\times3$ Convolution+BatchNorm layers  (with stride $2$ for the first layer), followed by a $3\times3$ max pooling layer with stride $2$. We used 8 GPUs (NVIDIA V100) in training. The batch size is $128$ per GPU ($1024$ in total). The initial learning rate is  $0.4$, and the cosine learning rate scheduler~\cite{cosine_lr} is used with weight decay $1\times10^{-4}$ and momentum $0.9$. %For large AOGNets, we tune the batch size ($180$ used to fit the GPU memory limit) and the initial learning rate ($0.07$ used). Common practices in training DNNs on the ImageNet dataset usually use at least $8$ GPUs with batch size and initial learning rate being $256$ and $0.1$ respectively. So, potentially, the performance of our AOGNets can be improved.~\footnote{We are in the process of purchasing 8-GPU servers and will re-train AOGNets following the common practices to improve performance.}   

\textbf{Results and Analyses.} Table~\ref{table:imagenet-results} shows the results. Our AOGNets are the best among the models with comparable model sizes in comparison in terms of both accuracy. %and efficiency (FLOPs). 
Our small AOGNet ($11.97M$) even outperforms ResNets~\cite{ResidualNet} ($44.5M$ and $60.2M$) by $1.2\%$ and $0.6\%$ respectively. Similarly, we note that our AOGNets use the same bottleneck operation function as ResNets, so the improvement must be contributed by the AOG building block structure. Our AOGNet ($40.3M$) obtains better performance than ResNeXt~\cite{ResNeXt}+SE~\cite{SENet} ($44M$) which represents the most powerful and widely used combination in practice. It also obtains better performance than the best model, DPN~\cite{DPN} ($61.6M$), which indicates that the hierarchical and compositional integration of the DenseNet- and ResNet-aggregation in our AOG building blocks are more effective than the cascade-based integration in the DPN~\cite{DPN}. Our AOGNet ($60.2M$) achieves the best result. We note that the FLOPs of our AOGNets are slightly higher than DPNs since DPNs use ResNeXt operation (i.e., group convolutions). In our on-going experiments, we are testing AOGNets with ResNeXt operation for nodes.   

To further evaluate AOGNets for mobile platforms, we trained an AOGNet ($4.6M$) and Table~\ref{table:imagenet-results-small} shows the comparison results. We obtain performance on par to the popular networks specifically designed for mobile platforms such as the MobileNets~\cite{howard2017mobilenets,sandler2018mobilenetv2} and ShuffleNets~\cite{zhang2017shufflenet}. Our AOGNet also outperforms the auto-searched network, NASNet~\cite{zoph2017learning} (which used around 800 GPUs in search). We note that we used the same AOGNet structures, and thus show the platform-agnostic  capability of our AOGNets. This is potentially  important and useful for deploying DNNs to different platforms in practice since no extra efforts of hand-crafting or searching neural architectures are entailed. This will be also potentially useful for distilling a small model from a large model if they share the exactly same structure.  



\subsection{Object Detection on PASCAL VOC}
We test our AOGNets in object detection on the PASCAL VOC 2007 and 2012 datasets~\cite{VOC}. We adopt the vanilla Faster R-CNN system~\cite{FasterRCNN} and reuse the code in PyTorch~\footnote{\url{https://github.com/jwyang/faster-rcnn.pytorch}}. We only substitute the ConvNet backbone with our AOGNets in experiments and keep everything else unchanged for fair comparison. We finetue the AOGNets pretrained on ImageNet. We adopt the provided end-to-end training procedure to train the region proposal network (RPN) and R-CNN jiontly. The first three stages are shared by RPN and R-CNN, and the last stage is used as the head classifier for region-of-interest (RoI) prediction. We fix all parameters pretrained on ImageNet before stage 1 (inclusive) in training. We follow the standard evaluation metrics Average Precision (AP) and mean of AP (mAP) in evaluation~\cite{VOC}.  
Table~\ref{table:detection} shows the detection results and comparisons. Our AOGNets obtain better mAP than ResNet-101 by around $3\%$ consistently. When trained using the $07+12$ trainval dataset, our small AOGNet-backboned detector  ($13.6M$) already slightly outperforms the ResNet-backboned one ($47.5M$), which further shows the effectiveness of the AOG building blocks in object detection tasks.


 \begin{table}
    \centering
    \small{
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c}
    \hline 
    Method & \#Params & FLOPS & CIFAR10 & CIFAR100 \\ \hline
    AOGNet & 4.24M & 0.65G & 3.75 & 19.20 \\ 
    AOGNet+LC & 4.24M & 0.65G & 3.70 & 19.09 \\ 
    AOGNet+RS & 4.23M & 0.70G & 3.57 & 18.64 \\
    AOGNet+RS+LC & 4.23M & 0.70G & \textbf{3.52} & \textbf{17.99} \\ \hline
    \end{tabular} }}
    \\ [1ex]
    \caption{An ablation study of our AOGNets using the mean error rate across 5 runs. In the first two rows, the AOGNets use full structure, and the pruned structure in the last two rows. The feature dimensions of node operations are accordingly specified to keep model sizes comparable. %Note that results of DualPathNet are taken from the author's Github (\url{https://github.com/cypw/DPNs}) which utilized an extra trick,  Max-Mean pooling, before the softmax with better performance obtained. 
    }\label{table:ablation-cifar} 
\end{table}
\subsection{Ablation Study}
We conduct an ablation study which investigates the effects of \emph{(i) RS:} Removing Symmetric child nodes of OR-nodes in the pruned AOG building blocks, and of \emph{(ii) LC:} adding Lateral Connections for dependency grammars. As Table~\ref{table:ablation-cifar} shows, the two components, RS and LC, improve performance. The results are consistent with our design intuition and principles. The RS component facilitates higher feature dimensions due to the reduced structural complexity, and the LC component increases the effective depth of nodes on the lateral flows.   
 
%\vspace{-1mm}
\section{Conclusions} \label{sec:conclusion}
%\vspace{-2mm}
This paper presented a method of learning deep compositional grammatical architectures which are capable of harnessing the best of grammars and deep neural networks for visual recognition. AND-OR Grammars (AOG) comprising phrase structure grammars and dependency grammars are utilized to design building blocks. The resulting models are called AOGNets. An AOGNet consists of a number of stages of AOG building blocks. AOGNets are tested on three highly competitive and widely used image classification  benchmarks: the CIFAR-10 dataset and the CIFAR-100 dataset~\cite{CIFAR},  and the ImageNet-1K dataset~\cite{ImageNet}. Our AOGNets obtain better performance consistently than  ResNets~\cite{ResidualNet} and most variants, ResNeXts~\cite{ResNeXt}, DenseNets~\cite{DenseNet} and DualPathNets~\cite{DPN} when model sizes are comparable. AOGNets are also tested in object detection on the PASCAL VOC 2007 and 2012~\cite{VOC} using the vanilla Faster R-CNN~\cite{FasterRCNN} system, and obtain better performance by about $3\%$ mAP than ResNets~\cite{ResidualNet}.  

%\textbf{Discussions.} We hope this paper encourages further exploration in integrating compositional grammar models and other structured knowledge representation and deep neural networks end-to-end, especially to harness the explainable rigor of the former and the discriminative power of the latter. In implementation, some interesting aspects that worth investigating  include, but not limited to,  searching better hyper-parameters in learning AOGNets (e.g., learning rate and schedule, parameter initialization and batch size, etc),   conducting experiments with much larger AOGNets (deeper and wider with different primitive size per stage), extending binary composition rule in constructing AND-OR graphs, adopting sub-graph based concatenation scheme for AND-nodes with predefined growth rate as done in DenseNet and DPN, and using front-end stages from networks such as ResNet, DenseNet and DPN before the first stage to further boost the experssive power and to save memory footprint.    

\noindent\textbf{Acknowledgment.} This work is supported by ARO award W911NF1810295, ARO DURIP award W911NF1810209 and NSF IIS 1822477. Some early experiments in this work used the Extreme Science and Engineering Discovery Environment (XSEDE)~\cite{XSEDE} at the SDSC Comet GPU Nodes (Comet GPU) through allocation IRI180024 (XSEDE is supported by National Science Foundation grant number ACI-1548562).


{\small
\bibliographystyle{ieee}
\bibliography{references}
}

\end{document}
