
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}


% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% From paper
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage[pdftex]{graphicx}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}


% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.

% *** GRAPHICS RELATED PACKAGES ***
%
%\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
%\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
%\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
% \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
% \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%


\author{
   \fontsize{12pt}{0}\fontfamily{phv}\selectfont 
        ~ Patrick Helber\textsuperscript{1,2}
        ~ Benjamin Bischke\textsuperscript{1,2}
        ~ Andreas Dengel\textsuperscript{1,2} 
        ~ Damian Borth\textsuperscript{2} ~  \\ ~ \\
    \fontsize{10pt}{0}\fontfamily{phv}\selectfont 
    \hspace{-0.25em}
    $^1$TU Kaiserslautern, Germany ~~~~ $^2$German Research Center for Artificial Intelligence (DFKI), Germany  \\ 
    \fontsize{10pt}{0}\fontfamily{phv}\selectfont 
     \{Patrick.Helber, Benjamin.Bischke, Andreas.Dengel, Damian.Borth\}@dfki.de  \\ 
}


%\author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
%        John~Doe,~\IEEEmembership{Fellow,~OSA,}
%        and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
%\thanks{M. Shell was with the Department
%of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
%GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a %space
%\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%&\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
In this paper, we address the challenge of land use and land cover classification using Sentinel-2 satellite images. The Sentinel-2 satellite images are openly and freely accessible provided in the Earth observation program Copernicus. We present a novel dataset based on Sentinel-2 satellite images covering 13 spectral bands and consisting out of 10 classes with in total 27,000 labeled and geo-referenced images. We provide benchmarks for this novel dataset with its spectral bands using state-of-the-art deep Convolutional Neural Network (CNNs). With the proposed novel dataset, we achieved an overall classification accuracy of 98.57\%. The resulting classification system opens a gate towards a number of Earth observation applications. We demonstrate how this classification system can be used for detecting land use and land cover changes and how it can assist in improving geographical maps. The geo-referenced dataset EuroSAT is made publicly available at https://github.com/phelber/eurosat.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Remote Sensing, Earth Observation, Satellite Images, Satellite Image Classification, Land Use Classification, Land Cover Classification, Dataset, Machine Learning, Deep Learning, Deep Convolutional Neural Network
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{W}e are currently at the edge of having public and continuous access to satellite image data for Earth observation. Governmental programs such as ESA's Copernicus and NASA's Landsat are taking significant efforts to make such data freely available for commercial and non-commercial purpose with the intention to fuel innovation and entrepreneurship. With access to such data, applications in the domains of agriculture, disaster recovery, climate change, urban development, or environmental monitoring
can be realized~\cite{ponti2016precision, bischke2017multimediaSatelliteSolution, bischke2016contextual, bischke2017multimediaSatellite}. However, to fully utilize the data for the previously mentioned
domains, first satellite images must be processed and transformed into structured 
semantics~\cite{huang2018opensarship}. One type of such fundamental semantics is \textit{Land Use and Land Cover Classification}~\cite{basu2015deepsat, yang2010bag}. The aim of land use and land cover classification is to automatically provide labels describing the represented physical land type or how a land area is used (e.g., residential, industrial).

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.6]{overviewFirstPage_new.png}
	\caption{Land use and land cover classification based on Sentinel-2 satellite images. Patches are extracted with the purpose to identify the shown class. This visualization highlights the classes annual crop, river, highway, industrial buildings and residential buildings.}
	\label{fig:overview}
\end{figure}

As often in supervised machine learning, the performance of classification systems strongly depends on the availability of high-quality datasets with a suitable set of classes~\cite{ILSVRC15}. 
In particular when considering the recent success of deep \textit{Convolutional Neural 
Networks (CNN)}~\cite{krizhevsky2012imagenet}, it is crucial to have large quantities of training data available to train such a network. Unfortunately, current land use and land cover datasets are small-scale or rely on data sources which do not allow the mentioned domain applications. %and do not provide a sufficient number of land classes.

In this paper, we propose a novel satellite image dataset for the task of land use and land cover classification. The proposed EuroSAT dataset consists of 27,000 labeled images with 10 different land use and land cover classes. A significant difference to previous datasets is that the presented satellite image dataset is multi-spectral covering 13 spectral bands in the visible, near infrared and short wave infrared part of the spectrum. In addition, the proposed dataset is georeferenced and based on openly and freely accessible Earth observation data allowing a unique range of applications. The labeled dataset EuroSAT is made publicly available at https://github.com/phelber/eurosat. 
\begin{figure*}
	\centering
	\includegraphics[width=0.9\linewidth]{overview3.png}
	\caption{This illustration shows an overview of the patch-based land use and land cover classification process using satellite images. A satellite scans the Earth to acquire images of it. Patches extracted out of these images are used for classification. The aim is to automatically provide labels describing the represented physical land type or how the land is used. For this purpose, an image patch is feed into a classifier, in this illustration a neural network, and the classifier outputs the class shown on the image patch.}
	\label{fig:overview}
\end{figure*}
Further, we provide a full benchmark demonstrating a robust classification performance which is the basis for developing applications for the previously mentioned domains. We outline how the classification model can be used for detecting land use or land cover changes and how it can assist in improving geographical maps. 

We provide this work in the context of the recently published EuroSAT dataset, which can be used similar to~\cite{ni2015large} as a basis for a large-scale training of deep neural networks for the task of satellite image classification.

In this paper, we make the following \textbf{contributions}:
\begin{quote}
\begin{itemize}
\item We introduce the first large-scale patch-based land use and land cover classification dataset based on Sentinel-2 satellite images. Every image in the dataset is labeled and geo-referenced. We release the RGB and the multi-spectral (MS) version of the dataset.
\item We provide benchmarks for the proposed EuroSAT dataset using Convolutional Neural Networks (CNNs).
\item We evaluate the performance of each spectral band of the Sentinel-2 satellite for the task of patch-based land use and land cover classification.
\end{itemize}
\end{quote}



\section{Related Work}

In this section, we review previous studies in land use and land cover classification. In this context, we present remotely sensed aerial and satellite image datasets. Furthermore, we present state-of-the-art image classification methods for land use and land cover classification.

%\begin{figure*}[t!p]
%	\centering
%	\includegraphics[width=1.0\linewidth]{bandCombinations.png}
%	\caption{These images visualize different band combinations of the spectral bands provided by ESA's satellite Sentinel-2A. The left image shows a color-infrared image by combining the near-infrared (B08), red (B04) and green (B03) band. The middle image displays a combination of the shortwave-infrared 2 (B12), red edge 4 (B08A) and red band (B04). The right image is constructed using merely the near-infrared (B08) band.}
%		\label{fig:sentinel_2_combinations}
%\end{figure*}

\subsection{Classification Datasets}

The classification of remotely sensed images is a challenging task. The progress of classification in the remote sensing area has particularly been inhibited due to the lack of reliably labeled ground truth datasets. A popular and intensively studied~\cite{castelluccio2015land, nogueira2017towards, penatti2015deep, xia2016aid, yang2010bag} remotely sensed image classification dataset known as \textit{UC Merced} (UCM) land use dataset was introduced by Yang et al.~\cite{yang2010bag}. The dataset consists of 21 land use and land cover classes. Each class has 100 images and the contained images measure 256x256 pixels with a spatial resolution of about 30 cm per pixel. All images are in the RGB color space and were extracted from the USGS National Map Urban Area Imagery collection, i.e. the underlying images were acquired from an aircraft. Unfortunately, a dataset with 100 images per class is small-scale. Trying to enhance the dataset situation, various works used commercial Google Earth images to manually create novel datasets~\cite{sheng2012high, xia2016aid, xia2010structural, zhao2016feature} such as the two benchmark datasets PatternNet~\cite{zhou2018patternnet} and NWPU-RESISC45~\cite{cheng2017remote}. The datasets are based on very-high-resolution images with a spatial resolution of up to 30 cm per pixel. Since the creation of a labeled dataset is extremely time-consuming, these datasets consist likewise of only a few hundred images per class. One of the largest datasets is the \textit{Aerial Image Dataset (AID)}. AID consists of 30 classes with 200 to 400 images per class. The 600x600 high-resolution images were also extracted from Google Earth imagery. 


Compared to the EuroSAT dataset presented in this work, the previously listed datasets rely on commercial very-high-resolution and preprocessed images. The fact of using commercial and preprocessed very-high-resolution image data makes these datasets unsatisfying for real-world Sentinel-2 Earth observation applications as proposed in this work. Furthermore, while these datasets put a strong focus on strengthening the number of covered classes, the datasets suffer from a low number of images per class. The fact of a spatial resolution of up to 30 cm per pixel, with the possibility to identify and distinguish classes like churches, schools etc., make the presented datasets difficult to compare with the dataset proposed in this work.

% TODO: A dataset with 100 images per class is very small-scale ... And therefore difficult to train deep neural networks

A study closer to our work, provided by Penatti et al. ~\cite{penatti2015deep}, analyzed remotely sensed satellite images with a spatial resolution of 10 meters per pixel to classify coffee crops. Based on these images, Penatti et al.~\cite{penatti2015deep} introduced the \textit{Brazillian Coffee Scene (BCS) dataset}. The dataset covers the two classes coffee crop and non-coffee crop. Each class consists of 1,423 images. The images consist of a red, green and near-infrared band. 

Similar to the proposed EuroSAT dataset, Basu et al.~\cite{basu2015deepsat} introduced the \textit{SAT-6} dataset relying on aerial images. This dataset has been extracted from images with a spatial resolution of 1 meter per pixel. The image patches are created using images from the National Agriculture Imagery Program (NAIP). SAT-6 covers the 6 different classes: barren land, trees, grassland, roads, buildings and water bodies. The proposed patches have a size of 28x28 pixels per image and consist of a red, green, blue and a near-infrared band.


\begin{figure}[t!]
	\centering
	\includegraphics[width=.8\linewidth]{flow_chart.png}
	\caption{The diagram illustrates the EuroSAT dataset creation process.}
	\label{fig:flow_chart}
\end{figure}

% TODO: Ab hier weiterlesen
\subsection{Land Use and Land Cover Classification}

Convolutional Neural Networks (CNNs) are a type of Neural Networks~\cite{lecun1989backpropagation}, which became with the impressive results on image classification challenges ~\cite{krizhevsky2012imagenet, ILSVRC15, simonyan2014very} the state-of-the-art image classification method in computer vision and machine learning.

To classify remotely sensed images, various different feature extraction and classification methods (e.g., Random Forests) were evaluated on the introduced datasets. Yang et al. evaluated Bag-of-Visual-Words (BoVW) and spatial extension approaches on the UCM dataset ~\cite{yang2010bag}. Basu et al. analyzed deep belief networks, basic CNNs and stacked denoising autoencoders on the SAT-6 dataset~\cite{basu2015deepsat}. Basu et al. also presented an own framework for the land cover classes introduced in the SAT-6 dataset. The framework extracts features from the input images, normalizes the extracted features and used the normalized features as input to a deep belief network. Besides low-level color descriptors, Penatti et al. also evaluated deep CNNs on the UCM and BCS dataset~\cite{penatti2015deep}. In addition to deep CNNs, Castelluccio et al. intensively evaluated various machine learning methods (e.g., Bag-of-Visual-Words, spatial pyramid match kernels) for the classification of the UCM and BCS dataset. 

In the context of deep learning, the used deep CNNs have been trained from scratch or fine-tuned by using a pretrained network~\cite{castelluccio2015land, nogueira2017towards, ahmad2017cnn, cheng2017remote, ma2016satellite}. The networks were mainly pretrained on the ILSVRC-2012 image classification challenge~\cite{ILSVRC15} dataset. Even though these pretrained networks were trained on images from a totally different domain, the features generalized well. Therefore, the pretrained networks proved to be suitable for the classification of remotely sensed images ~\cite{marmanis2016deep}. The presented works extensively evaluated all proposed machine learning methods and concluded that that deep CNNs outperform non-deep learning approaches on the considered datasets~\cite{castelluccio2015land, marmanis2016deep, luus2015multiview, xia2016aid}. 


%\begin{figure}[h!]
%	\centering
%	\includegraphics[width=0.6\linewidth]{resolution_show.jpg}
%	\caption{This image shows a Sentinel-2 satellite image in the RGB color space generated by combining the red (B04), green (B03) and blue (B02) spectral band. The shown image gives an impression of the visible objects at a spatial resolution of 10 meters per pixel.}
%		\label{fig:sentinel_2_resolution}
%\end{figure}

\begin{figure*}
\centering
  \begin{subfigure}[b]{.19\linewidth}
    \centering
    \includegraphics[width=70px]{industrial.pdf}
    \caption{Industrial Buildings}\label{fig:1a}
  \end{subfigure}%   
  \begin{subfigure}[b]{.19\linewidth}
    \centering
    \includegraphics[width=70px]{residential.pdf}
    \caption{Residential Buildings}\label{fig:1b}
  \end{subfigure}%  
  \begin{subfigure}[b]{.19\linewidth}
    \centering
    \includegraphics[width=70px]{annual_crop.pdf}
    \caption{Annual Crop}\label{fig:1c}
  \end{subfigure}%  
  \begin{subfigure}[b]{.19\linewidth}
    \centering
    \includegraphics[width=70px]{permanent_crop.pdf}
    \caption{Permanent Crop}\label{fig:1d}
  \end{subfigure}%   
  \begin{subfigure}[b]{.19\linewidth}
    \centering
    \includegraphics[width=70px]{river.pdf}
    \caption{River}\label{fig:1i}
  \end{subfigure}%
    \vspace{+0.25cm}
  \begin{subfigure}[b]{.19\linewidth}
    \centering
    \includegraphics[width=70px]{sealake.pdf}
    \caption{Sea \& Lake}\label{fig:1j}
  \end{subfigure}%
  \begin{subfigure}[b]{.19\linewidth}
    \centering
    \includegraphics[width=70px]{herbaceous.pdf}
    \caption{Herbaceous Vegetation}\label{fig:1k}
  \end{subfigure}%
  \begin{subfigure}[b]{.19\linewidth}
    \centering
    \includegraphics[width=70px]{highway.pdf}
    \caption{Highway}\label{fig:1l}
  \end{subfigure}%  
  %\vspace{+0.25cm}
  \begin{subfigure}[b]{.19\linewidth}
    \centering
    \includegraphics[width=70px]{pastures.pdf}
    \caption{Pasture}\label{fig:1m}
  \end{subfigure}%
  \begin{subfigure}[b]{.19\linewidth}
    \centering
    \includegraphics[width=70px]{forest.pdf}
    \caption{Forest}\label{fig:1m}
  \end{subfigure}%
  \caption{This overview shows sample image patches of all 10 classes covered in the proposed EuroSAT dataset. The images measure 64x64 pixels. Each class contains 2,000 to 3,000 image. In total, the dataset has 27,000 geo-referenced images.}\label{fig:sentinel_2dataset_labels}
\end{figure*}

\section{Dataset Acquisition}

Besides NASA with its Landsat Mission, the European Space Agency (ESA) steps up efforts to improve Earth observation within its Copernicus program. Under this program, ESA operates a series of satellites known as Sentinels. 

In this paper, we use mutli-spectral image data provided by the Sentinel-2A satellite in order to address the challenge of land use and land cover classification. Sentinel-2A is one satellite in the two-satellite constellation of the identical land monitoring satellites Sentinel-2A and Sentinel-2B. The satellites were successfully launched in June 2015 (Sentinel-2A) and March 2017 (Sentinel-2B). Both sun-synchronous satellites capture the global Earth's land surface with a Multispectral Imager (MSI) covering the 13 different spectral bands listed in Table~\ref{table:1}. The three bands B01, B09 and B10 are intended to be used for the correction of atmospheric effects (e.g., aerosols, cirrus or water vapor). The remaining bands are primarily intended to identify and monitor land use and land cover classes. In addition to mainland, large islands as well as inland and coastal waters are covered by these two satellites. Each satellite will deliver imagery for at least 7 years with a spatial resolution of up to 10 meters per pixel. Both satellites carry fuel for up to 12 years of operation which allows for an extension of the operation. The two-satellite constellation generates a coverage of almost the entire Earth's land surface about every five days, i.e. the satellites capture each point in the covered area about every five days. This short repeat cycle as well as the future availability
of the Sentinel satellites allows a continuous monitoring of the Earth's land surface for about the next 20 - 30 years. Most importantly, the data is openly and freely accessible and can be used for any application (commercial or non-commercial use). 

%In order to show the spatial resolution of 10 meters per pixel, Figure~\ref{fig:sentinel_2_resolution} illustrates a sample scene originated from the combination of the red (B04), green (B03) and blue (B02) band. 

%In order to emphasize different optical aspects, several spectral bands can be combined. In remote sensing, these combinations are used to make different aspects visible, which are poorly visible or cannot be seen using RGB color space images. One example is the combination of shortwave-infrared, near-infrared and green light to identify floods. 

%An example of a color-infrared image, which results from the combination of the near-infrared (B08), red (B04) and green (B03) band, is shown in Figure~\ref{fig:sentinel_2_combinations}. A shortwave-infrared image arisen from the combination of the shortwave-infrared 2 (B12), red edge 4 (B08A) and red (B04) band is also shown in Figure~\ref{fig:sentinel_2_combinations}. Furthermore, Figure~\ref{fig:sentinel_2_combinations} shows an image consisting of merely the near-infrared (B08) band.

%the satellites scan the entire land surface every five days

We are convinced that the large volume of satellite data in combination with powerful machine learning methods will influence future research. Therefore, one of our key research aims is to make this large amount of data accessible for machine learning based applications. To construct an image classification dataset, we performed the following two steps:

%and evaluate the proposed classification approach, we conducted the following steps:

\begin{enumerate}
    \item Satellite Image Acquisition: We gathered satellite images of European cities distributed in over 34 countries as shown in Fig.~\ref{fig:distribution}.
    \item Dataset Creation: Based on the obtained satellite images, we created a dataset of 27,000 georeferenced and labeled image patches. The image patches measure 64x64 pixels and have been manually checked.
    %\item Land Use and Land Cover Classification: We evaluate deep CNNs on the proposed novel dataset and the introduced existing datasets.
\end{enumerate}

\subsection{Satellite Image Acquisition}
  
We have downloaded satellite images taken by the satellite Sentinel-2A via Amazon S3. We chose satellite images associated with the cities covered in the European Urban Atlas. The covered cities are distributed over the 34 European countries: Austria, Belarus, Belgium, Bulgaria, Cyprus, Czech Republic (Czechia), Denmark, Estonia, Finland, France, Germany, Greece, Hungary, Iceland, Ireland, Italy / Holy See, Latvia, Lithuania, Luxembourg, Macedonia, Malta, Republic of Moldova, Netherlands, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Sweden, Switzerland, Ukraine and United Kingdom.

In order to improve the chance of getting valuable image patches, we selected satellite images with a low cloud level. Besides the possibility to generate a cloud mask, ESA provides a cloud level value for each satellite image allowing to quickly select images with a low percentage of clouds covering the land scene. 


%Note that we use the
%term land-use to refer to this set of classes even though they
%contain some land-cover and possibly object classes


We aimed for the objective to cover as many countries as possible in the EuroSAT dataset in order to cover the high intra-class variance inherent to remotely sensed images. Furthermore, we have extracted images recorded all over the year to get a variance as high as possible inherent in the covered land use and land cover classes. Within one class of the EuroSAT dataset, different land types of this class are represented such as different types of forests in the forest class or different types of industrial buildings in the industrial building class. Between the classes, there is a low positive correlation. The classes most common to each other are the two presented agricultural classes and the two classes representing residential and industrial buildings. The composition of the individual classes and their relationships are specified in the mapping guide of the European Urban Atlas~\cite{mapEUA}. An overview diagram of the dataset creation process is shown in Fig.~\ref{fig:flow_chart}

\begin{table}[t!]
\vspace{+0.25cm}
\caption{All 13 bands covered by Sentinel-2's Multispectral Imager (MSI). The identification, the spatial resolution and the central wavelength is listed for each spectral band.}
\centering
\begin{tabular}{lccc}\toprule
\textbf{Band}	&\textbf{Spatial} &\textbf{Central}	\\
& \textbf{Resolution} & \textbf{Wavelength} \\
& \(m\)	& \(nm\)  \\\midrule

B01	- Aerosols	& 60	& 443	 \\
B02 - Blue 	& 10	& 490		\\
B03	- Green	& 10	& 560		\\
B04	- Red	& 10	& 665		\\
B05 - Red edge 1		& 20	& 705	 \\
B06 - Red edge 2	& 20	& 740	\\
B07	- Red edge 3	& 20	& 783	\\
B08 - NIR		& 10	& 842	\\
B08A - Red edge 4	& 20	& 865  \\
B09	- Water vapor	& 60	& 945 \\
B10	- Cirrus	& 60	& 1375 \\
B11	- SWIR 1	& 20	& 1610 \\
B12	- SWIR 2	& 20	& 2190 \\\bottomrule
\end{tabular}
\label{table:1}
\end{table}


\begin{table*}[ht!]
\centering
\caption{Classification accuracy (\%) of different training-test splits on the EuroSAT dataset.}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} 
    \hline
    \textbf{Method} & \textbf{10/90} & \textbf{20/80} & \textbf{30/70} & \textbf{40/60} & \textbf{50/50} & \textbf{60/40}& \textbf{70/30}& \textbf{80/20} & \textbf{90/10}\\
    \hline
    BoVW (SVM, SIFT, k = 10)& 54.54 & 56.13 & 56.77 & 57.06 & 57.22 & 57.47 & 57.71 & 58.55 & 58.44\\
    \hline
    BoVW (SVM, SIFT, k = 100)& 63.07 & 64.80 & 65.50 & 66.16 & 66.25 & 66.34 & 66.50 & 67.22 & 66.18\\
    \hline
    BoVW (SVM, SIFT, k = 500)& 65.62 & 67.26 & 68.01 & 68.52 & 68.61 & 68.74 & 69.07 & 70.05 & 69.54\\
    \hline
    CNN (two layers)& 75.88 & 79.84 & 81.29 & 83.04 & 84.48 & 85.77 & 87.24 & 87.96 & 88.66 \\
    \hline
    ResNet-50 & 75.06 & 88.53 & 93.75 & 94.01 & 94.45 & 95.26 & 95.32 & 96.43  & 96.37 \\ 
    \hline
    GoogleNet & 77.37 & 90.97 & 90.57 & 91.62 & 94.96 & 95.54 & 95.70 & 96.02 & 96.17 \\ 
    %\hline
    %ResNet-50 (pre-trained) & 96.60 & 96.28 & 98.08 & 98.09 & & & & & \\ 
    %\hline
    %GoogleNet (pre-trained) & 96.73 & 97.50 & 98.01 &  &  &  &  & &  \\ 
    %\hline
    %\hline
    %BoVW (RF, SIFT)& 50.07 & 50.94 & 51.71 & 51.43 & 52.49 & 52.32 & 52.61 & 53.61 & 52.62\\

    
%Own: InceptionV3: 0.9579, V1: 0.9393, eval/Accuracy[0.9818
%UCMerced V3: 0.9642, V1: 0.9732
%AID: %V3: 0.9384, V1: [0.93994
%Sat-6: V3: 97.64, V1: 0.9829
%Brazillian: V3_ eval/Accuracy[0.9131, V1: eval/Accuracy[0.92708331]
    
\hline
\end{tabular}
\label{table:dataset_accuracies_splits}
\end{table*}


\subsection{Dataset Creation}

The Sentinel-2 satellite constellation provides about 1.6 TB of compressed images per day. Unfortunately, supervised machine learning is restricted even with this amount of data by the lack of labeled ground truth data. The generation of the benchmarking EuroSAT dataset was motivated by the objective of making this open and free satellite data accessible to various Earth observation applications and the observation that existing benchmark datasets are not suitable for the intended applications with Sentinel-2 satellite images.
% as one part of our contribution.
The dataset consists of 10 different classes with 2,000 to 3,000 images per class. In total, the dataset has 27,000 images. The patches measure 64x64 pixels. We have chosen 10 different land use and land cover classes based on the principle that they showed to be visible at the resolution of 10 meters per pixel and are frequently enough covered by the European Urban Atlas to generate thousands of image patches. To differentiate between different agricultural land uses, the proposed dataset covers the classes annual crop, permanent crop (e.g., fruit orchards, vineyards or olive groves) and pastures. The dataset also discriminates built-up areas. It therefore covers the classes highway, residential buildings and industrial buildings. The residential class is created using the urban fabric classes described in the European Urban Atlas. Different water bodies appear in the classes river and sea \& lake. Furthermore, undeveloped environments such as forest and herbaceous vegetation are included. An overview of the covered classes with four samples per class is shown in Fig.~\ref{fig:sentinel_2dataset_labels}. %Please note that we refer to the term land use even thought the presented dataset contains some land cover classes.

We manually checked all 27,000 images multiple times and corrected the ground truth by sorting out mislabeled images as well as images full of snow or ice. Example images, which have been discarded, are shown in Fig.~\ref{fig:bad_patches}. The samples are intended to show industrial buildings. Clearly, no industrial building is visible. Please note, the proposed dataset has not received atmospheric correction. This can result in images with a color cast. Extreme cases are visualized in Fig.~\ref{fig:patches_color_cast}. With the intention to advocate the classifier to also learn these cases, we did not filter the respective samples and let them flow into the dataset.

Besides the 13 covered spectral bands, the new dataset has three further central innovations. Firstly, the dataset is not based on non-free satellite images like Google Earth imagery or relies on data sources which are not updated on a high-frequent basis (e.g., NAIP used in~\cite{basu2015deepsat}). Instead, an open and free Earth observation program whose satellites deliver images for the next 20 - 30 years is used allowing real-world Earth observation applications. Secondly, the dataset uses a 10 times lower spatial resolution than the benchmark dataset closest to our research but at once distinguishes 10 classes instead of 6. For instance, we split up the built-up class into a residential and an industrial class or distinguish between different agricultural land uses. Thirdly, we release the EuroSAT dataset in a georeferenced version.

With the release of the geo-referenced EuroSAT we aim to make the large amount of Sentinel-2 satellite imagery accessible for machine learning approaches. There effectiveness was successfully demonstrated in ~\cite{chen2018training, roy2018semantic, helber2018introducing}. 




%Chen et al. \cite{chen2018training} demonstrated the effectiveness of small neural networks for the task of satellite image classification as the EuroSAT. Roy et al. \cite{} demonstrated the effectiveness of Generative Adversarial Networks (GAN) in satellite image classification. 


%Therefore, one of our key research aims is to make this vast data source accessible for machine learning applications. To construct an image classification dataset, we conducted the following steps:
 

\begin{figure}[t!]
	\centering
	\includegraphics[width=1.0\linewidth]{distribution.png}
	\caption{EuroSAT dataset distribution. The georeferenced images are distributed all over Europe. The distribution is influenced by the number of represented cities per country in the European Urban Atlas.}
	\label{fig:distribution}
\end{figure}



% GoogLeNet, ResNet-50


%and evaluate the proposed classification approach, we conducted the following steps:
    %\item Land Use and Land Cover Classification: We evaluate deep CNNs on the proposed novel dataset and the introduced existing datasets.
    
% \subsection{Classification Using Deep Convolutional \\ Neural Networks}

%First, used networks pretrained on images in RGB color space on images in other color channels.

\section{Dataset Benchmarking}
As shown in previous work~\cite{castelluccio2015land, luus2015multiview, marmanis2016deep, nogueira2017towards}, deep CNNs have demonstrated to outperform non-deep learning approaches in land use and land cover image classification. Accordingly, we use the state-of-the-art deep CNN models GoogleNet~\cite{szegedy2015going} and ResNet-50~\cite{he2016deep, he2016identity} for the classification of the introduced land use and land cover classes. The networks make use of the inception module~\cite{szegedy2015going,szegedy2016rethinking, szegedy2016inception, lin2013network} and the residual unit~\cite{he2016deep, he2016identity}. For the proposed EuroSAT dataset, we also evaluated the performance of the 13 spectral bands with respect to the classification task. In this context, we evaluate the classification performance using single-band and band combination images.

%The used deep CNNs have been pretrained on the ILSVRC-2012 image classification dataset \cite{ILSVRC15}.

\begin{table}[!h]
\centering
\caption{Benchmarked classification accuracy (\%) of the two best performing classifiers GoogLeNet and ResNet-50 with a 80/20 training-test split. Both CNNs have been pretrained on the image classification dataset ILSVRC-2012 ~\cite{ILSVRC15}.}
\begin{tabular}{|c|c|c|c|c|c|c|} 
    \hline
    \textbf{Method} & \textbf{UCM} & \textbf{AID} & \textbf{SAT-6} & \textbf{BCS} & \textbf{EuroSAT}\\
    \hline
    ResNet-50 & 96.42 & 94.38 & 99.56 & 93.57 & 98.57\\
    \hline
    GoogLeNet & 97.32 & 93.99 & 98.29 & 92.70 & 98.18\\

    
    
%Own: InceptionV3: 0.9579, V1: 0.9393, eval/Accuracy[0.9818
%UCMerced V3: 0.9642, V1: 0.9732
%AID: %V3: 0.9384, V1: [0.93994
%Sat-6: V3: 97.64, V1: 0.9829
%Brazillian: V3_ eval/Accuracy[0.9131, V1: eval/Accuracy[0.92708331]
    
\hline
\end{tabular}
\label{table:dataset_accuracies}
\end{table}


\subsection{Comparative Evaluation}

We respectively split each dataset in a training and a test set (80/20 ratio). We ensured that the split is applied class-wise. While the red, green and blue bands are covered by almost all aerial and satellite image datasets, the proposed EuroSAT dataset consists of 13 spectral bands. For the comparative evaluation, we computed images in the RGB color space combining the bands red (B04), green (B03) and blue (B02). For benchmarking, we evaluated the performance of the Bag-of-Visual-Words (BoVW) approach using SIFT features and a trained SVM. In addition, we trained a shallow Convolutional Neural Network (CNN), a ResNet-50 and a GoogleNet model on the training set. We calculated the overall classification accuracy to evaluate the performance of the different models on the considered datasets. In Table~\ref{table:dataset_accuracies_splits} we show how the approaches perform in case of different training-test splits for the EuroSAT RGB dataset.

It can be seen that all CNN approaches outperform the BoVW method and, overall, deep CNNs perform better than shallow CNNs. Nevertheless, the shallow CNN classifies the EuroSAT classes with a classification accuracy of up to 89.03\%. Please note~\cite{castelluccio2015land, nogueira2017towards, sheng2012high} for the benchmarking performance of the other datasets on different training-test splits.

Table~\ref{table:dataset_accuracies} lists the achieved classification results for the two best performing CNN models GoogLeNet and ResNet-50. In this experiment, the GoogleNet and ResNet-50 CNN models were pretrained on the ILSVRC-2012 image classification dataset~\cite{ILSVRC15}. In all fine-tuning experiments, we first trained the last layer with a learning rate of 0.01. Afterwards, we fine-tuned through the entire network with a low learning rate between 0,001 and 0,0001. With a finetuned network we achieve a classification accuracy of about 2\% higher compared to randomly initialized versions of the networks which have been trained on the EuroSAT dataset with the same training-test split (see Table~\ref{table:dataset_accuracies_splits}).

The deep CNNs achieve state-of-the-art results on the UCM dataset and outperform previous results on the other three presented datasets by about 2-4\% (AID, SAT-6, BCS)~\cite{castelluccio2015land, nogueira2017towards, sheng2012high}. Table~\ref{table:dataset_accuracies} shows that the ResNet-50 architecture performs best on the introduced EuroSAT land use and land cover classes. In order to allow an evaluation on the class level, Fig.~\ref{fig:confusion_matrix} shows the confusion matrix of this best performing network. It is shown that the classifier sometimes confuses the agricultural land classes as well as the classes highway and river.

%In addition to the deep-learning approaches, we trained state-of-the-art non-deep-learning  image classification approaches. The approaches, Support Vector Machine (SVM) and Random Forest (RF) have been trained with a cross-validated hyperparameter tuning grid search. The results and the comparision of the CNN approaches and the SVM as well as RF show that the deep learning CNN approach clearly outperform the non-deep-learning approaches. Furthermore, the resutls show ...(TODO). 


%The results on various different datasets testify on "the effectiveness and wide applicability of the proposed solution"

\subsection{Band Evaluation}

%We were interested in verifying the hypothesis that possibly a single band is powerful enough to classify the different images and the RGB images are not needed for a similar performance.

In order to evaluate the performance of deep CNNs using single-band images as well shortwave-infrared and color-infrared band combinations, we used the pretrained ResNet-50 with a fixed training-test split to compare the performance of the different spectral bands. For the single-band image evaluation, we used images as input consisting of the information gathered from a single spectral band on all three input channels. We analyzed all spectral bands, even the bands not intended for land monitoring. Bands with a lower spatial resolution have been upsampled to 10 meters per pixel using cubic-spline interpolation~\cite{de1978practical}. Fig.~\ref{fig:band_accuracies} shows a comparison of the spectral band's performance. It is shown that the red, green and blue bands outperform all other bands. Interestingly, the bands red edge 1 (B05) and shortwave-infrared 2 (B12) with an original spatial resolution of merely 20 meters per pixel showed an impressive performance. The two bands even outperform the near-infrared band (B08) which has a spatial resolution of 10 meters per pixel. 


\begin{figure}[t]
	\centering
	\includegraphics[width=170px]{bad_patches.png}
	\caption{Four examples of bad image patches, which are intended to show industrial buildings. Clearly, no industrial building is shown due to clouds, mislabeling, dead pixels or ice/snow.}
	\label{fig:bad_patches}
\end{figure}
%Having a remote sensing satellite image dataset, the dataset also contains atmospheric effects, a classifier has to deal with. Furthermore, the dataset is based on images from various different countries covering a high intra-class variance. The proposed dataset, published with this paper, is available at: \url{www.anonymous.com}


\begin{figure}[t]
	\centering
	\includegraphics[width=120px]{patches_color_cast.png}
	\caption{Color cast due to atmospheric effects.}
	\label{fig:patches_color_cast}
\end{figure}

\begin{figure}[t!]
	\centering
	\includegraphics[width=0.8\linewidth]{confusion_matrix2.pdf}
	\caption{Confusion matrix of a fine-tuned ResNet-50 CNN on the proposed EuroSAT dataset using satellite images in the RGB color space.}
	\label{fig:confusion_matrix}
\end{figure}

\begin{table}[t!]
\centering
\vspace{+0.25cm}
\caption{Classification accuracy (\%) of a fine-tuned ResNet-50 CNN on the proposed EuroSAT dataset with the three different band combinations color-infrared (CI), shortwave-infrared (SWIR) and RGB as input.}

\begin{tabular}{|c|c|}
    \hline
    \textbf{Band Combination} & \textbf{Accuracy (ResNet-50)}\\
    \hline
    CI & 98.30\\
    \hline
    \textbf{RGB} & \textbf{98.57}\\ %InceptionV3: 0.9579, V1: 0.9393, eval/Accuracy[0.9818
    \hline
    SWIR & 97.05\\
    \hline
\end{tabular}
\label{table:combinations_accuracies}
\end{table}

In addition to the RGB band combination, we also analyzed the performance of the shortwave-infrared and color-infrared band combination. Table~\ref{table:combinations_accuracies} shows a comparison of the performance of these combinations. As shown, band combination images outperform single-band images. Furthermore, images in the RGB color space performed best on the introduced land use and land cover classes. Please note, networks pretrained on the ILSVRC-2012 image classification dataset have initially not been trained on images other than RGB images.

\begin{figure}[t!]
	\centering
	\includegraphics[width=0.7\linewidth]{band_accuracies.pdf}
	\caption{Overall classification accuracy (\%) of a fine-tuned ResNet-50 CNN on the given EuroSAT dataset using single-band images.}
	\label{fig:band_accuracies}
\end{figure}


%\begin{figure}[t!]
%	\centering
%	\includegraphics[width=1.0\linewidth]{10months.png}
%	\caption{The provided EuroSAT with its images }
%	\label{fig:time_series}
%\end{figure}


%\begin{figure}[t!]
%	\centering
%   \includegraphics[width=1.0\linewidth]{euroSATseg.png}
%	\caption{Pixel-wise land use and land cover classification (segmentation) on the EuroSAT images using the annotations provided in the European Urban Atlas.}
%	\label{fig:euroSATseg}
%\end{figure}



\section{Applications}
The openly and freely accessible satellite images allow a broad range of possible applications. In this section, we demonstrate that the novel dataset published with this paper allows real-world applications. The classification result with an overall accuracy of 98.57\% paves the way for these applications. We show land use and land cover change detection applications as well as how the the trained classifier can assist in keeping geographical maps up-to-date.

\subsection{Land Use and Land Cover Change Detection}

Since the Sentinel-2 satellite constellation will scan the Earth's land surface for about the next 20 - 30 years on a repeat cycle of about five days, a trained classifier can be used for monitoring land surfaces and detect changes in land use or land cover. To demonstrate land use and land cover change detection, we selected images from the same spatial region but from different points in time. Using the trained classifier, we analyzed 64x64 image regions. A change has taken place if the classifier delivers different classification results for patches taken from the same spatial 64x64 region. In the following, we show three examples of spotted changes. In the first example shown in Fig.~\ref{fig:changeDetection_industrial}, the classification system recognized that the land cover has changed in the highlighted area. The left image was acquired in the surroundings of Shanghai, China in December 2015 showing an area classified as industrial. The right image shows the same area in December 2016 revealing that the industrial buildings have been demolished. The second example is illustrated in Fig.~\ref{fig:changeDetection_residential}. The left image was acquired in the surroundings of Dallas, USA in August 2015 showing no dominant residential buildings in the highlighted area. The right image shows the same area in March 2017. The system has identified a change in the highlighted area revealing that residential buildings have been constructed. The third example presented in Fig.~\ref{fig:changeDetection_deforestation} shows that the system detected deforestation near Villamontes, Bolivia. The left image was acquired in October 2015. The right image shows the same region in September 2016 revealing that a large area has been deforested. The presented examples are particularly of interest in urban area development, nature protection or sustainable development. For instance, deforestation is a main contributor to climate change, therefore the detection of deforested land is of particular interest (e.g., to notice illegal clearing of forests).

%As we provide EuroSAT as geo-referenced dataset, the geo-reference for every single image patch is released. This allows to easily extract a segmentation mask for each image as shown in Figure~\ref{fig:euroSATseg} using existing land use and land cover maps. These segmentation masks allow to extend this work to a patch-based mulit-class as well as a pixel-wise land use and land cover classification. 


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{changeDetection_industrial2.png}
	\caption{The left image was acquired in the surroundings of Shanghai in December 2015 showing an area classified as industrial. The right image shows the same region in December 2016 revealing that the industrial buildings have been demolished.}
	
	\label{fig:changeDetection_industrial}
\end{figure}

% with the classification system emerged from the proposed dataset. 

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{changeDetection_residential.png}
	\caption{The left image was acquired in the surroundings of Dallas, USA in August 2015 showing no dominant residential buildings in the highlighted area. The right image shows the same area in March 2017 showing that residential buildings have been built up.}
	
	\label{fig:changeDetection_residential}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{changeDetection_deforestation.png}
	\caption{The left image was acquired near Villamontes, Bolivia in October 2015. The right image shows the same area in September 2016 revealing that a large land area has been deforested.}
	\label{fig:changeDetection_deforestation}
\end{figure}


\subsection{Assistance in Mapping}
While a classification system trained with 64x64 image patches does not allow a finely graduated per-pixel segmentation, it cannot only detect changes as shown in the previous examples, it can also facilitate keeping maps up-to-date. This is an extremely helpful assistance with maps created in a crowdsourced manner like OpenStreetMap (OSM). A possible system can verify already tagged areas, identify mistagged areas or bring large area tagging. The proposed system is based on the trained CNN classifier providing a classification result for each image patch created in a sliding windows based manner.

As shown in Fig.~\ref{fig:changeDetection_osm}, the industrial buildings seen in the left up-to-date satellite image are almost completely covered in the corresponding OSM mapping. The right up-to-date satellite image also shows industrial buildings. However, a major part of the industrial buildings is not covered in the corresponding map. Due to the high temporal availability of Sentinel-2 satellite images in the future, this work together with the published dataset can be used to build systems which assist in keeping maps up-to-date. A detailed analysis of the respective land area can then be provided using high-resolution satellite images and an advanced segmentation approach~\cite{bischke2017multitask_buildings, Kampffmeyer_2016_CVPR_Workshops}.

\begin{figure*}
	\centering
	\includegraphics[width=0.9\linewidth]{changeDetection_osm3.png}
	\caption{A patch-based classification system can verify already tagged areas, identify mistagged areas or bring large area tagging as shown in the above images and maps. The left Sentinel-2 satellite image was acquired in Australia in March 2017. The right satellite image was acquired in the surroundings of Shanghai, China in March 2017. The corresponding up-to-date OpenStreetMap (OSM) mapping images show that the industrial areas in the left satellite image are almost completely covered (colored gray). However, the industrial areas in the right satellite image are not properly covered.}
\label{fig:changeDetection_osm}
\end{figure*}

\section{Conclusion}
In this paper, we have addressed the challenge of land use and land cover classification. For this task, we presented a novel dataset based on remotely sensed satellite images. To obtain this dataset, we have used the openly and freely accessible Sentinel-2 satellite images provided in the Earth observation program Copernicus. The proposed dataset consists of 10 classes covering 13 different spectral bands with in total 27,000 labeled and geo-referenced images. We provided benchmarks for this dataset with its spectral bands using state-of-the-art deep Convolutional Neural Network (CNNs). For this novel dataset, we analyzed the performance of the 13 different spectral bands. As a result of this evaluation, the RGB band combination with an overall classification accuracy of 98.57\% outperformed the shortwave-infrared and the color-infrared band combination and leads to a better classification accuracy than all single-band evaluations. Overall, the available free Sentinel-2 satellite images offer a broad range of possible applications. This work is a first important step to make use of the large amount of available satellite data in machine learning allowing to monitor Earth's land surfaces on a large scale. The proposed dataset can be leveraged for multiple real-world Earth observation applications. Possible applications are land use and land cover change detection or the improvement of geographical maps.

\appendices
\section*{Acknowledgment}
This work was partially funded by the BMBF project DeFuseNN (01IW17002). The authors thank NVIDIA for the support within the NVIDIA AI Lab program.


% potential use cases of the underlying classification model. We demonstrate 

% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\bibitem{basu2015deepsat}
S.~Basu, S.~Ganguly, S.~Mukhopadhyay, R.~DiBiano, M.~Karki, and R.~Nemani.
\newblock Deepsat: a learning framework for satellite imagery.
\newblock In {\em Proceedings of the 23rd SIGSPATIAL International Conference
  on Advances in Geographic Information Systems}, page~37. ACM, 2015.

\bibitem{bischke2017multimediaSatelliteSolution}
B.~Bischke, P.~Bhardwaj, A.~Gautam, P.~Helber, D.~Borth, and A.~Dengel.
\newblock{Detection of Flooding Events in Social Multimedia and Satellite
  Imagery using Deep Neural Networks}.
\newblock In {\em MediaEval}, 2017.

\bibitem{bischke2016contextual}
B.~Bischke, D.~Borth, C.~Schulze, and A.~Dengel.
\newblock{Contextual Enrichment of Remote-Sensed Events with Social Media
  Streams}.
\newblock In {\em Proceedings of the 2016 ACM on Multimedia Conference}, pages
  1077--1081. ACM, 2016.

\bibitem{bischke2017multitask_buildings}
B.~Bischke, P.~Helber, J.~Folz, D.~Borth, and A.~Dengel.
\newblock{Multi-Task Learning for Segmentation of Buildings Footprints with
  Deep Neural Networks}.
\newblock In {\em arXiv preprint arXiv:1709.05932}, 2017.

\bibitem{bischke2017multimediaSatellite}
B.~Bischke, P.~Helber, C.~Schulze, V.~Srinivasan, and D.~Borth.
\newblock{The Multimedia Satellite Task: Emergency Response for Flooding
  Events}.
\newblock In {\em MediaEval}, 2017.

\bibitem{castelluccio2015land}
M.~Castelluccio, G.~Poggi, C.~Sansone, and L.~Verdoliva.
\newblock Land use classification in remote sensing images by convolutional
  neural networks.
\newblock{\em arXiv preprint arXiv:1508.00092}, 2015.

\bibitem{cheng2017remote}
G.~Cheng, J.~Han, and X.~Lu.
\newblock Remote sensing image scene classification: Benchmark and state of the
  art.
\newblock{\em Proceedings of the IEEE}, 2017.

\bibitem{de1978practical}
C.~De~Boor, C.~De~Boor, E.-U. Math{\'e}maticien, C.~De~Boor, and C.~De~Boor.
\newblock{\em A practical guide to splines}, volume~27.
\newblock Springer-Verlag New York, 1978.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 770--778, 2016.

\bibitem{he2016identity}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em European Conference on Computer Vision}, pages 630--645.
  Springer, 2016.

\bibitem{Kampffmeyer_2016_CVPR_Workshops}
M.~Kampffmeyer, A.-B. Salberg, and R.~Jenssen.
\newblock Semantic segmentation of small objects and modeling of uncertainty in
  urban remote sensing images using deep convolutional neural networks.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) Workshops}, June 2016.

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{lecun1989backpropagation}
Y.~LeCun, B.~Boser, J.~S. Denker, D.~Henderson, R.~E. Howard, W.~Hubbard, and
  L.~D. Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock{\em Neural computation}, 1(4):541--551, 1989.

\bibitem{lin2013network}
M.~Lin, Q.~Chen, and S.~Yan.
\newblock Network in network.
\newblock{\em arXiv preprint arXiv:1312.4400}, 2013.

\bibitem{luus2015multiview}
F.~P. Luus, B.~P. Salmon, F.~van~den Bergh, and B.~Maharaj.
\newblock Multiview deep learning for land-use classification.
\newblock{\em IEEE Geoscience and Remote Sensing Letters}, 12(12):2448--2452,
  2015.

\bibitem{ma2016satellite}
Z.~Ma, Z.~Wang, C.~Liu, and X.~Liu.
\newblock Satellite imagery classification based on deep convolution network.
\newblock{\em World Academy of Science, Engineering and Technology,
  International Journal of Computer, Electrical, Automation, Control and
  Information Engineering}, 10(6):1113--1117, 2016.

\bibitem{marmanis2016deep}
D.~Marmanis, M.~Datcu, T.~Esch, and U.~Stilla.
\newblock Deep learning earth observation classification using imagenet
  pretrained networks.
\newblock{\em IEEE Geoscience and Remote Sensing Letters}, 13(1):105--109,
  2016.

\bibitem{ni2015large}
K.~Ni, R.~Pearce, K.~Boakye, B.~Van~Essen, D.~Borth, B.~Chen, and E.~Wang.
\newblock Large-scale deep learning on the yfcc100m dataset.
\newblock{\em arXiv preprint arXiv:1502.03409}, 2015.

\bibitem{nogueira2017towards}
K.~Nogueira, O.~A. Penatti, and J.~A. dos Santos.
\newblock Towards better exploiting convolutional neural networks for remote
  sensing scene classification.
\newblock{\em Pattern Recognition}, 61:539--556, 2017.

\bibitem{penatti2015deep}
O.~A. Penatti, K.~Nogueira, and J.~A. dos Santos.
\newblock Do deep features generalize from everyday objects to remote sensing
  and aerial scenes domains?
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 44--51, 2015.

\bibitem{ILSVRC15}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, A.~C. Berg, and L.~Fei-Fei.
\newblock{ImageNet Large Scale Visual Recognition Challenge}.
\newblock{\em International Journal of Computer Vision (IJCV)},
  115(3):211--252, 2015.

\bibitem{sheng2012high}
G.~Sheng, W.~Yang, T.~Xu, and H.~Sun.
\newblock High-resolution satellite scene classification using a sparse coding
  based multiple feature combination.
\newblock{\em International journal of remote sensing}, 33(8):2395--2412,
  2012.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock{\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{szegedy2016inception}
C.~Szegedy, S.~Ioffe, V.~Vanhoucke, and A.~Alemi.
\newblock Inception-v4, inception-resnet and the impact of residual connections
  on learning.
\newblock{\em arXiv preprint arXiv:1602.07261}, 2016.

\bibitem{szegedy2015going}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich.
\newblock Going deeper with convolutions.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1--9, 2015.

\bibitem{szegedy2016rethinking}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2818--2826, 2016.

\bibitem{xia2016aid}
G.-S. Xia, J.~Hu, F.~Hu, B.~Shi, X.~Bai, Y.~Zhong, and L.~Zhang.
\newblock Aid: A benchmark dataset for performance evaluation of aerial scene
  classification.
\newblock{\em arXiv preprint arXiv:1608.05167}, 2016.

\bibitem{xia2010structural}
G.-S. Xia, W.~Yang, J.~Delon, Y.~Gousseau, H.~Sun, and H.~Ma{\^\i}tre.
\newblock Structural high-resolution satellite image indexing.
\newblock In {\em ISPRS TC VII Symposium-100 Years ISPRS}, volume~38, pages
  298--303, 2010.

\bibitem{yang2010bag}
Y.~Yang and S.~Newsam.
\newblock Bag-of-visual-words and spatial extensions for land-use
  classification.
\newblock In {\em Proceedings of the 18th SIGSPATIAL international conference
  on advances in geographic information systems}, pages 270--279. ACM, 2010.

\bibitem{zhao2016feature}
L.~Zhao, P.~Tang, and L.~Huo.
\newblock Feature significance-based multibag-of-visual-words model for remote
  sensing image scene classification.
\newblock{\em Journal of Applied Remote Sensing}, 10(3):035004--035004, 2016.

\bibitem{ahmad2017cnn}
Kashif Ahmad, Konstantin Pogorelov, Michael Riegler, Nicola Conci, and H~Pal.
\newblock Cnn and gan based satellite and social media data fusion for disaster
  detection.
\newblock In {\em Proc. of the MediaEval 2017 Workshop, Dublin, Ireland}, 2017.

\bibitem{chen2018training}
Guanzhou Chen, Xiaodong Zhang, Xiaoliang Tan, Yufeng Cheng, Fan Dai, Kun Zhu,
  Yuanfu Gong, and Qing Wang.
\newblock Training small networks for scene classification of remote sensing
  images via knowledge distillation.
\newblock{\em Remote Sensing}, 10(5):719, 2018.

\bibitem{roy2018semantic}
Subhankar Roy, Enver Sangineto, Nicu Sebe, and Beg{\"u}m Demir.
\newblock Semantic-fusion gans for semi-supervised satellite image
  classification.
\newblock In {\em 2018 25th IEEE International Conference on Image Processing
  (ICIP)}, pages 684--688. IEEE, 2018.
  
%\bibitem{chen2017change}
%Zhao Chen, Bin Yang, Bin Wang, Guohua Liu, and Wei Xia.
%\newblock Change detection in hyperspectral imagery based on
%  spectrally-spatially regularized low-rank matrix decomposition.
%\newblock In {\em Geoscience and Remote Sensing Symposium (IGARSS), 2017 IEEE
%  International}, pages 157--160. IEEE, 2017.
  
\bibitem{helber2018introducing}
Patrick Helber, Benjamin Bischke, Andreas Dengel, Damian Borth.
\newblock Introducing EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification
\newblock In {\em Geoscience and Remote Sensing Symposium (IGARSS), 2018 IEEE
  International}. IEEE, 2018.  
  
\bibitem{huang2018opensarship}
Lanqing Huang, Bin Liu, Boying Li, Weiwei Guo, Wenhao Yu, Zenghui Zhang, and
  Wenxian Yu.
\newblock Opensarship: A dataset dedicated to sentinel-1 ship interpretation.
\newblock{\em IEEE Journal of Selected Topics in Applied Earth Observations
  and Remote Sensing}, 11(1):195--208, 2018.
  
  
\bibitem{cheng2017remote}
Gong Cheng, Junwei Han, and Xiaoqiang Lu.
\newblock Remote sensing image scene classification: benchmark and state of the
  art.
\newblock{\em Proceedings of the IEEE}, 105(10):1865--1883, 2017.

\bibitem{ponti2016precision}
Moacir Ponti, Arthur~A Chaves, F{\'a}bio~R Jorge, Gabriel~BP Costa, Adimara
  Colturato, and Kalinka~RLJC Branco.
\newblock Precision agriculture: Using low-cost systems to acquire low-altitude
  images.
\newblock{\em IEEE computer graphics and applications}, 36(4):14--20, 2016.

\bibitem{zhou2018patternnet}
Weixun Zhou, Shawn Newsam, Congmin Li, and Zhenfeng Shao.
\newblock Patternnet: a benchmark dataset for performance evaluation of remote
  sensing image retrieval.
\newblock{\em ISPRS Journal of Photogrammetry and Remote Sensing}, 2018.


\bibitem{zhou2018patternnet}
Weixun Zhou, Shawn Newsam, Congmin Li, and Zhenfeng Shao.
\newblock Patternnet: a benchmark dataset for performance evaluation of remote
  sensing image retrieval.
\newblock{\em ISPRS Journal of Photogrammetry and Remote Sensing}, 2018.

\bibitem{mapEUA}
European Commission. Mapping guide for a European urban atlas. \url{https://ec.europa.eu/regional_policy/sources/tender/pdf/2012066/annexe2.pdf}, 2012.




  

\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


