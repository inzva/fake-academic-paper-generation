% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016
\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
%\usepackage{ruler}
\usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\usepackage[font=small,labelfont=bf]{caption}

\newcommand{\lp}{\left(}\newcommand{\rp}{\right)}\newcommand{\lb}{\left[}\newcommand{\rb}{\right]}\newcommand{\relmid}[1]{\mathrel{}\middle#1\mathrel{}}\newcommand{\norm}[1]{\|#1\|}\newcommand{\loglikelihood}{loglikelihood}\newcommand{\bishop}[1]{Bishop #1}\newcommand{\newword}[1]{{\bf #1}}\newcommand{\Data}{\mathcal{D}}\newcommand{\Model}{\mathcal{M}}\newcommand{\DataTrain}{\mathcal{D}_{{\text train}}}\newcommand{\DataTest}{\mathcal{D}_{{\text test}}}\newcommand{\N}{N}\newcommand{\Ntrain}{\N_{train}}\newcommand{\Ntest}{\N_{test}}\newcommand{\DataSize}{\N}\newcommand{\DataIndex}{n}\newcommand{\cind}{t}\newcommand{\pind}{\star}\newcommand{\eye}{{\bf I}}\newcommand{\Dim}{D}\newcommand{\DimIndex}{d}\newcommand{\DimOut}{K}\newcommand{\DimOutIndex}{k}\newcommand{\diagfrac}[2]{\,^{#1}\!\!\!\diagup\!\!\!_{#2}}\DeclareMathOperator{\E}{\mathbb{E}}\DeclareMathOperator*{\ED}{\mathbb{E}_{\Data}}\DeclareMathOperator{\V}{\mathbb{V}}\DeclareMathOperator{\LL}{\mathcal{L}}\DeclareMathOperator{\KL}{\mathcal{K\!\!L}}\DeclareMathOperator{\ReLU}{Re}\DeclareMathOperator\erf{erf}\DeclareMathOperator\trace{tr}\DeclareMathOperator\median{median}\DeclareMathOperator*{\argmax}{arg\,max}\DeclareMathOperator*{\argmin}{arg\,min}\newcommand{\expectation}{\E}\newcommand{\Dis}{\mathcal{D}}\newcommand{\Gen}{\mathcal{G}}\newcommand{\expectationdata}{\ED}\newcommand{\variance}{\V}\newcommand{\loss}{\LL}\newcommand{\ascalar}{a}\newcommand{\xscalar}{x}\newcommand{\xvec}{{\bf \xscalar}}\newcommand{\xvectest}{\xvec_{\star}}\newcommand{\Xmat}{{\bf \MakeUppercase\xscalar}}\newcommand{\tscalar}{t}\newcommand{\ttest}{\tscalar_{\star}}\newcommand{\tvec}{{\bf \tscalar}}\newcommand{\Tmat}{{\bf \MakeUppercase\tscalar}}\newcommand{\yscalar}{y}\newcommand{\yvec}{{\bf \yscalar}}\newcommand{\Ymat}{{\bf \MakeUppercase\yscalar}}\newcommand{\wscalar}{w}\newcommand{\wvec}{{\bf \wscalar}}\newcommand{\wvecML}{\wvec_{\text{MLE}}}\newcommand{\wvecMAP}{\wvec_{\text{MAP}}}\newcommand{\wvecs}{\wvec^{(s)}}\newcommand{\Wmat}{{\bf \MakeUppercase\wscalar}}\newcommand{\wbias}{\wscalar_0}\newcommand{\xn}{\xscalar_{\DataIndex}}\newcommand{\xvecn}{\xvec_{\DataIndex}}\newcommand{\tvecn}{\tvec_{\DataIndex}}\newcommand{\tn}{\tscalar_{\DataIndex}}\newcommand{\ti}{\tscalar_{i}}\newcommand{\yvecn}{\yvec_{\DataIndex}}\newcommand{\yn}{\yscalar_{\DataIndex}}\newcommand{\yfunc}{\yscalar}\newcommand{\yfunctest}{\yfunc_{\star}}\newcommand{\zerovec}{ {\bf 0}}\newcommand{\muvec}{\boldsymbol{\mu}}\newcommand{\muvecN}{\muvec_{\N}}\newcommand{\munotvec}{\boldsymbol{\mu}_0}\newcommand{\mnot}{{\bf m_0}}\newcommand{\mN}{{\bf m}_{\N}}\newcommand{\mNplus}{{\bf m}_{\N+1}}\newcommand{\thetavec}{\boldsymbol{\theta}}\newcommand{\Thetamat}{\boldsymbol{\Theta}}\newcommand{\thetav}{\thetavec}\newcommand{\phivec}{\boldsymbol{\phi}}\newcommand{\phivectest}{\phivec_{\star}}\newcommand{\Phimat}{\boldsymbol{\Phi}}\newcommand{\phivv}{\phivec}\newcommand{\phivecn}{\phivec_{\DataIndex}}\newcommand{\phiveci}{\phivec_{i}}\newcommand{\Sigmamat}{\boldsymbol{\Sigma}}\newcommand{\SigmamatN}{\Sigmamat_N}\newcommand{\Sigmamatnot}{\Sigmamat_0}\newcommand{\SigmamatInv}{\Sigmamat^{-1}}\newcommand{\Smat}{{\bf S}}\newcommand{\SmatN}{\Smat_{\N}}\newcommand{\Smatnot}{\Smat_0}\newcommand{\SmatInv}{\Smat^{-1}}\newcommand{\SmatNplus}{\Smat_{\N+1}}\newcommand{\SmatNplusinv}{\Smat_{\N+1}^{-1}}\newcommand{\betatest}{\beta_{\star}}\newcommand{\Amat}{{\bf A}}\newcommand{\Bmat}{{\bf B}}\newcommand{\Cmat}{{\bf C}}\newcommand{\Dmat}{{\bf D}}\newcommand{\Emat}{{\bf E}}\newcommand{\Fmat}{{\bf F}}\newcommand{\Gmat}{{\bf G}}\newcommand{\Hmat}{{\bf H}}\newcommand{\Imat}{{\bf I}}\newcommand{\Jmat}{{\bf J}}\newcommand{\Kmat}{{\bf K}}\newcommand{\Lmat}{{\bf L}}\newcommand{\Mmat}{{\bf M}}\newcommand{\Nmat}{{\bf N}}\newcommand{\Omat}{{\bf O}}\newcommand{\Pmat}{{\bf P}}\newcommand{\Qmat}{{\bf Q}}\newcommand{\Rmat}{{\bf R}}%\newcommand{\Smat}{{\bf S}}%\newcommand{\Tmat}{{\bf T}}\newcommand{\Umat}{{\bf U}}\newcommand{\Vmat}{{\bf V}}%\newcommand{\Wmat}{{\bf W}}%\newcommand{\Xmat}{{\bf X}}%\newcommand{\Ymat}{{\bf Y}}\newcommand{\Zmat}{{\bf Z}}\newcommand{\parf}[1]{\frac{\partial}{\partial #1}}\newcommand{\parft}[2]{\frac{\partial #1}{\partial #2}}\newcommand{\rvec}{{\bf r}}\newcommand{\dvec}{{\bf d}}\newcommand{\lvec}{{\bf l}}\newcommand{\mvec}{{\bf m}}\newcommand{\uvec}{{\bf u}}\newcommand{\vvec}{{\bf v}}\newcommand{\zvec}{{\bf z}}\newcommand{\xvecmean}{\bar{\xvec}}\newcommand{\xvecnest}{\tilde{\xvec}_n}\newcommand{\xvecestn}{\xvecnest}\newcommand{\class}{\mathcal{C}}\newcommand{\sigmoid}{\sigma}\newcommand{\ans}[1]{ 
  \begin{center}
    % \fbox{
      \begin{minipage}{.8\textwidth}
        {\sf#1}
      \end{minipage}
    % }
  \end{center} 
}
\usepackage{color}
\usepackage{colortbl}
\usepackage{subcaption,rotating,multirow}
\usepackage{sidecap}
\usepackage{pdflscape}

\makeatletter
\newcommand*{\centerfloat}{%
  \parindent \z@
  \leftskip \z@ \@plus 1fil \@minus \textwidth
  \rightskip\leftskip
  \parfillskip \z@skip}
\makeatother


%% FROM CVPR
\usepackage{xspace}
% Add a period to the end of an abbreviation unless there's one
% already, then \xspace.
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother
%%
\usepackage{hyperref}
\usepackage{url}

\graphicspath{{./figures/}}

\newcommand{\mail}[1]{\href{mailto:#1}{\texttt{#1}}}
\newcommand{\emailname}[2]{\href{mailto:#1}{\texttt{#2}}}
\newcommand{\myparagraph}[1]{\vspace{-2mm}\paragraph{\textbf{#1}}}
\newlength{\mylength}
\renewcommand*{\tableautorefname}{{\color{black} Tab.}}
\renewcommand*{\figureautorefname}{{\color{black} Fig.}}
\renewcommand*{\equationautorefname}{{\color{black} Eq.}}
\renewcommand*{\sectionautorefname}{{\color{black} Section}}
\renewcommand*{\subsectionautorefname}{{\color{black} Section}}
\renewcommand*{\subsubsectionautorefname}{{\color{black} Section}}

\newcommand{\todo}[1]{{\color{red} TODO: \emph{#1}}}
\newcommand{\ennl}[1]{{\color{red} \em [TL: #1]}}
\newcommand{\tm}[1]{{\color{red} {\bf TM:} #1}}
\newcommand{\checkvalue}[1]{{\color{red} {#1}}}
\newcommand{\citeneed}[0]{{\color{red}$^{[\text{citation needed}]}$}}
\let\wip\emph

\def\case#1#2{\def\mytemp{#1}\ifx\mytemp\isswitch\def\isdefault{1}#2\fi\ignorespaces}
\def\default#1{\if0\isdefault#1\fi\ignorespaces}
\def\isdefault{0}
\def\isswitch{0}

\def\wmad{$L_1^{\textrm{M}}$\xspace}
\protected\def\wip#1{\ignorespaces\def\isdefault{0}\def\isswitch{#1}\ignorespaces
    \case{Model 1}{Pix2pix} % Default Pix2pix
    \case{p2p}{Pix2pix} % Default Pix2pix
    \case{Model 2}{$\text{IG}$} % Iterative generator model    
    \case{Model 3}{$\text{IG}_{\textrm{+U}}$} % IG with independent intermediate discriminator
    \case{umodel}{$\text{IG}_{\textrm{+U}}$} % IG with independent intermediate discriminator
    \case{Model 4}{$\text{IG}_{\textrm{+S}}$} % IG with dependent intermediate discriminator
    \case{smodel}{$\text{IG}_{\textrm{+S}}$} % IG with dependent intermediate discriminator
    \case{Model 5}{$\text{IG}_{\textrm{DE}}$} % IG_D negative angle real examples
	\case{igmodel}{$\text{IG}$} % Iterative generator model            
    \case{mumodel}{$\text{IG}_{\textrm{M+U}}$} % Iterative generator model
    \case{ig}{$\text{IG}$} % Iterative generator model
    \case{igm}{$\text{IG}_{\textrm{M}}$} % Iterative generator model
    \case{igu}{$\text{IG}_{\textrm{+U}}$} % Iterative generator model
    \case{igs}{$\text{IG}_{\textrm{+S}}$} % Iterative generator model
	\case{ig6}{$\text{IG}^6$} % Iterative generator model        
    \case{ig6m}{$\text{IG}^6_{\textrm{M}}$} % Iterative generator model
    \case{ig6mu}{$\text{IG}^6_{\textrm{M+U}}$} % Iterative generator model
    \case{ig6u}{$\text{IG}^6_{\textrm{+U}}$} % Iterative generator model
    \case{ig6s}{$\text{IG}^6_{\textrm{+S}}$} % Iterative generator model
    \case{ig6ms}{$\text{IG}^6_{\textrm{M+S}}$} % Iterative generator model
    \case{stepm}{$\text{IG}^{k^a}_{\textrm{M+U}}$}
    \case{stepa}{$\text{IG}^{k^a}_{\textrm{M+U}}$}
    \case{stepi}{$\text{IG}^{k^i}_{\textrm{M+U}}$}
    \case{stepnu}{$\text{IG}^{k^i}_{\textrm{M}}$}
    \case{1x30}{Pix2pix} %\emph{p2p}}
    \case{v:1x30}{\emph{p2p}$^{1}$}
    \case{6x5}{\emph{p2p}$^{6}$}
    \case{6x?}{$\text{IG}^{6}$}
    \case{s6mmad}{$\text{IG}^6_{\textrm{M}}$}
    \case{v:6x?}{$\text{IG}^{6}$}
    \case{v:ss}{$\text{IG}^{6}_{\text{+U}}$}
    \case{5x?}{$\text{IG}^{5}$}
    \case{3x?}{$\text{IG}^{3}$}
    \case{kx?}{$\text{IG}^{k}$}
    \case{indep. sample}{$\text{IG}^{6}_{\text{+U}}$}
    \case{dep. sample}{$\text{IG}^{6}_{\text{+S}}$}
    \case{neg. example}{$\text{IG}^{6}_{\text{DE}}$}
    \case{indep. sample$\leftarrow$6x?}{\wip{indep. sample}\transGD\wip{6x?}}
    \case{dep. sample$\leftarrow$6x?}{\wip{dep. sample}\transGD\wip{6x?}}
    \case{ds6x}{\wip{dep. sample}\transGD\wip{6x?}}
    \case{neg. example$\leftarrow$6x?}{\wip{neg. example}\transD\wip{6x?}}
    \case{ne6x}{\wip{neg. example} \transD \wip{6x?}}
    \case{6x?$\leftarrow$neg. example}{\wip{6x?}\transGD\wip{neg. example}}
    \default{\emph{#1}}
    \xspace
}
\def\>{$\rightarrow$}
\def\<{$\Leftarrow$}
\def\transGD{\mbox{$\!\!\leftarrow\!\!\!\!\!\!\frac{\Dis}{\Gen}$}}
\def\transD{\mbox{$\!\!\leftarrow\!\!\!\!\!\!\frac{}{\Gen}$}}

\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter

%\def\ECCV18SubNumber{708}  % Insert your submission number here
\title{IterGANs: Iterative GANs to Learn and Control 3D Object Transformation}
\titlerunning{IterGANs to Learn 3D Object Transformation}
\authorrunning{Y. Galama \& T. Mensink}
\author{Ysbrand Galama \and Thomas Mensink}
\institute{Computer Vision Group\\University of Amsterdam\\{\scriptsize \texttt{\mail{ysbrand@ygalama.nl}, \mail{thomas.mensink@uva.nl}}}}
\maketitle

\begin{abstract}
We are interested in learning visual representations which allow for 3D manipulations of visual objects based on a single 2D image.
We cast this into an image-to-image transformation task, and propose Iterative Generative Adversarial Networks (IterGANs) which iteratively transform an input image into an output image. Our models learn a visual representation that can be used for objects seen in training, but also for never seen objects.
Since object manipulation requires a full understanding of the geometry and appearance of the object, our IterGANs learn an implicit 3D model and a full appearance model of the object, which are both inferred from a single (test) image.
Two advantages of IterGANs are that the intermediate generated images can be used for an additional supervision signal, even in an unsupervised fashion, and that the number of iterations can be used as a control signal to steer the transformation. 
Experiments on rotated objects and scenes show how IterGANs help with the generation process.
\end{abstract}

\section{Introduction}
In this paper we are interested in manipulating visual objects and scenes, without resorting to external provided (CAD) models or advanced (3D/depth) sensing techniques.
To be more specific, we focus on rotating objects and rotating the camera viewpoint of scene from a single 2D image.
Manipulating objects/scenes require an expectation about the appearance and the geometrical structure of the \emph{unseen} part of the object/scene. 
Humans clearly have such an expectation based on an understanding of the physics of the world, the continuity of objects, and previously seen (related) objects and scenes.
We aim to learn such an 3D understanding, which can be inferred from single 2D images.

In order to learn a representation for object manipulation, we cast this problem into an image-to-image transformation task, with the goal to transform an input image following a given 3D transformation to an target image.
For this kind of object manipulation, often either stereoscopic cameras~\cite{ko20072d,bruno20103d} or temporal data streams~\cite{pollefeys2008detailed,gibson2003interactive} have been used to infer depth cues, while our aim is to obtain the target image from a single input image only. 
Similarly as humans are able to do it with one eye closed~\cite{vishwanath2013seeing}, there has also been works that aim to reconstruct from a single image~\cite{saxena2009make3d,rematas2016novel}, however these typically require external provided 3D object models, \eg~\cite{vicente2013balloon}, or focus on a single class of objects only~\cite{park17cvpr}. 
Our aim, on the other hand, is to learn a general transformation model, which can transform many classes of objects, even objects never seen at train time (without provided 3D models), based on the fact that appearance and geometrical continuity are (mostly) not object/domain specific but general applicable.
For now, we focus on a specific instance of this general object manipulation problem: the object in the target image is a fixed rotation of the input image.

For this task, we propose the use of Iterative Generative Adversarial Networks (IterGANs), see \autoref{fig:pipeline}.
GANs have been used for many (image) generation tasks~\cite{reed2016generative,denton2015deep,radford2015unsupervised}, including image-to-image prediction~\cite{pix2pix2016,Zhu_2017_ICCV}. 
IterGANs are an extension of the image-to-image generator GANs~\cite{pix2pix2016}, where the input image is fed to the generator, and the output of the generator is again fed into the generator, this circle is repeated a predefined number of iterations, or used as a control mechanism to steer the image rotation. 

The iterative nature of IterGANs has some particular advantages over a single image-to-image GAN translation network:
(i) the generator only needs to learn a small image manipulation; and (ii) we can use the intermediately generated images to steer the learning process.
A fundamental difference between image manipulation and the image-to-image tasks explored in~\cite{pix2pix2016} is, that when translating a map into an aerial image there exists a one-to-one pixel relation between the input and the output image. In the case of object manipulation, however, pixels have long range dependencies, depending on the geometry and the appearance of the object and the required degree of rotation.
IterGANs break these long dependencies into a series of shorter dependencies. 
Moreover, IterGANs allow to use intermediate loss functions measuring the quality of the series of generated images to improve the overall transformation quality.

\begin{figure}[t]	
	\centering
	\subcaptionbox{IterGAN Pipeline\label{fig:pipeline}}{\includegraphics[height=20mm]{pipelineoverview}}
	\quad\quad
	\subcaptionbox{Mental rotation\label{fig:mental}}{\includegraphics[height=20mm]{mental}}
    \caption{
    \textbf{IterGAN} (\emph{left}): Illustration of a default 3D manipulation pipeline (\emph{blue}) versus an IterGAN pipeline (\emph{red}), the last of which learns an implicit image-to-image model for 3D object manipulation.
    \textbf{Mental rotation} (\emph{right}): Psychology research has shown that there exist a linear relation between the human reaction time to identify matching pairs of rotated objects and the degree of rotation~\cite{shepard1971mental}. Thus we postulate it is easier to train an IterGAN to iteratively rotate an object for a few degrees, than a GAN for a single (large) rotation.\newline\centerline {\footnotesize \emph{ How long does it take you to find the non-matching pair?}}}
    \label{fig:intro}    
    \label{fig:mm-mentalrot}    
    \vspace{-5mm}
\end{figure}

This paper is an extended version of our ICLR Workshop paper~\cite{galama18openreview} and the source-code is available on GitHub\footnote{Code available at: \url{https://github.com/tomaat/itergan}}.
Our paper is organised as follows. 
Next we will discuss some of the most related work in image-to-image object manipulation.
In \autoref{sec:mm-models} we introduce IterGANs and propose extended loss functions on the intermediate generated images.
We show extensive experimental results in \autoref{sec:ex}, on the ALOI dataset~\cite{geusebroek2005amsterdam} for object rotation, and the VKITTI dataset~\cite{Gaidon:Virtual:CVPR2016} for camera viewpoint scene rotation.
Finally, we conclude the paper in \autoref{sec:conc}.


\section{Related work}
There is vast amount of related work in the field of 3D reconstruction and image generation.
Here, we only highlight the most relevant methods with respect to our proposed models.
For a more in depth overview we refer to \cite{li2015overview} for 3D reconstruction and to \cite{Zhu_2017_ICCV} for image generation.

\myparagraph{3D reconstruction from 2D images}
There are several techniques for generating 3D environments from 2D data, differing in both the type of data, and the type of environment.
It is possible to create a point-cloud from video using Structure from Motion~\cite{koenderink1991affine,nister2005preemptive}, or to fit polygonal objects to images~\cite{rematas2016novel,suveg2004reconstruction} and deform them to fit other images~\cite{vicente2013balloon}.

In recent years, there have also been attempts to use deep learning instead.
These techniques also allow forgoing 3D models, thus using only 2D images to describe the 3D environment.
Such an approach has been used to classify objects~\cite{su2015multi}, generate different viewpoints from descriptors~\cite{dosovitskiy2015learning} or creating the frames for 3D movies~\cite{xie2016deep3d}.

In contrast to their work, we focus on changing the viewpoint of a given image.
Therefore our models need to not only constuct the view as Dosovitskiy et al.~\cite{dosovitskiy2015learning} does, but also perceive an input image, and capture more than a disparity map from Xie et al.~\cite{xie2016deep3d}.
Moreover, since the output is an image, we use GANs as a training method to transform the image.

\myparagraph{Image manipulation with GANs}
GANs~\cite{goodfellow2014generative,mirza2014conditional} have been shown to be useful for the generation of images.
These networks manage to generate visually pleasing images, as shown by many recent studies (\eg~\cite{reed2016generative,denton2015deep,radford2015unsupervised}).
Using a GAN conditioned on an input image to generate a related image has been proposed to transform in different settings as `Pix2pix'~\cite{pix2pix2016}.

The development of GANs has introduced multiple studies to manipulate images and the objects in images.
It is now possible to age a face from a single image~\cite{antipov2017face}, or provide it with glasses and a shave~\cite{shen2017learning}.
Alternatively images can be manipulated by changing the main object~\cite{liang2017generative} (\eg~cats to dogs).

\myparagraph{Novel viewpoint estimation}
A more specific form of image manipulation is \emph{Novel viewpoint estimation}, which has a slightly different goal than 3D reconstruction, since the output is again a 2D image.
A new viewpoint can be estimated using voxel projections~\cite{yan16nips}, or GANs to estimate the frontal view of a face~\cite{huang2017beyond}, or transforming a single image with viewpoint estimation~\cite{zhou16eccv}.
Quality of the generated image can be improved by multiple reconstructed images~\cite{zhou16eccv}, or using a second network to finish the image estimated by a flow-network~\cite{park17cvpr}. 

Contrary to previous work is the type of data and control signal used.
We use the amount of iterations of the generator as control, not a vector as input in the network.
Moreover, instead of synthetic data of a single object class, a heterogeneous dataset of real world images photographed under constraint conditions is used.
%%%%%%%%%%%%%%%%%%%%%%%%% Overview figure of the pipeline %%%%%%%%%%%%%%%%%%%%%%%%%\begin{figure}[t]
	\centering
	\subcaptionbox{IterGAN base\label{fig:itergan}}{\includegraphics[width=.3\textwidth]{stepmodelA}}
	\quad
	\subcaptionbox{Unsupervised IDL\label{fig:unsupervised}}{\includegraphics[width=.3\textwidth]{stepmodelB}}
	\quad
	\subcaptionbox{Supervised IDL\label{fig:supervised}}{\includegraphics[width=.3\textwidth]{stepmodelC}}		
	\caption{IterGAN framework and intermediate discriminator loss (IDL) functions: the iterative nature of IterGANs (\emph{left}) allow to define an unsupervised discriminator loss on the intermediately generated images (\emph{middle}), where the discriminator aims to tell apart generated images ($B^i$) from real images ($A$ or $T$), and to define a supervised discriminator loss function (\emph{right}) to tell apart an input-generated pair of images  of $(A,B^i)$ from a real pair of images $(A,T^i$).}
\end{figure}%%%\section{IterGANs}\label{sec:mm-models}
Iterative GANs are an extension of the image-to-image GAN networks of Isola \etal~\cite{pix2pix2016}, where the generator is called iteratively.
This allows for both, learning with more details an image-to-image translation task by making $k$ small iterative steps, and for controlling object manipulation by calling the generator for $k$ iterations, where $k$ depends on the desired manipulation. 
The underlying assumption of Pix2Pix, \ie a one-to-one correspondence between pixels in the input and output image, is (in part) restored by calling the generator iteratively.
Thus, the final image $B$ generated from input image $A$ is given by, iteratively calling the generator of the GAN network $k$ times:
\begin{equation}
	B^k = \Gen_\theta(\Gen_\theta(\Gen_\theta(\Gen_\theta(\Gen_\theta(\Gen_\theta(A)))))) = \Gen^k_{\theta}(A).
	\label{eq:gen}
\end{equation}
Each generator $\Gen_\theta$ now has to rotate its input image only by a small fraction, until the final rotation has been reached $B^k$.
The iterative nature of the IterGAN is illustrated in~\autoref{fig:itergan}, we refer to the IterGAN network as \text{\wip{ig}}.


Our proposed network has the same number of parameters as a Pix2Pix network~\cite{pix2pix2016} with a single generator, and for learning we can use the same generator and discriminator losses as in~\cite{pix2pix2016}:
\begin{align}
	\LL_\Gen^{(\text{\wip{ig}})} &= H[\Dis_\phi(A, B^k), 1] + \lambda_{L_1} \ L_1(B^k, T)\label{eq:mm-lgen-m1},\\
	\LL_\Dis^{(\text{\wip{ig}})} &= H[\Dis_\phi(A, T), 1] + H[\Dis_\phi(A, B^k), 0],\label{eq:mm-ldis-m1}
\end{align}
where the cross-entropy loss ($H$) between the input image $A$ and the generated image $B^k = \Gen^k_{\theta}(A)$ is used,
as well as the $L_1$ loss between the generated image $B^k$ and the target image $T$.

\myparagraph{Object mask specific reconstruction loss}
The ALOI dataset~\cite{geusebroek2005amsterdam} we use for most of our experiments contains (rotated) objects against a black background.
In order to focus the reconstruction mostly on the objects, we use a variant of the L1-loss, which uses the provided binary mask $M$:
\begin{align}
	L_1^{\textrm{M}} &= \frac{2}{|M|} \sum_{xy} M_{xy} Z_{xy} + \frac{1}{|\neg M|} \sum_{xy} \neg M_{xy} Z_{xy}, \label{eq:L1m}\\ \textrm{with } Z_{xy} &= \sum_c |T_{xyc}-B^k_{xyc}|, \nonumber	
\end{align}
where $M \in \{0,1\}$ assigns pixels to the object (1) or background (0). 
This variant of the L1-loss weights the object twice as important as the background, which is relevant for our task of object transformations.
We refer to this as \wip{igm} model.

%%%%%%%%%%%%%%%%%%%%%%%%% IterGAN Artefacts                       %%%%%%%%%%%%%%%%%%%%%%%%%\begin{SCfigure}[10][t]
  \centering  
  \caption{Examples of generated artefacts in intermediate images (\emph{top row}) that are not present in the final generated image (\emph{bottom row}).
  		Artefacts are independent of specific value for $k = \{3,5,6\}$, examples from different iterations $i$.
        }  
    \includegraphics[width=.5\textwidth]{method_artifacts}    
	\label{fig:artefacts}        
\end{SCfigure}%%%\myparagraph{Intermediate artefacts}
While the iterative generator in general leads to more realistic final images compared to the baseline \wip{Model 1} model, the intermediate generated images show different types of artefacts.
In \autoref{fig:artefacts}, we show examples of generated intermediate images from experiments using $k=\{3,5,6\}$ and each of these have artefacts, like switching colours, adding noise, or adding other patterns. Interestingly, these are gone in the final generated image, so repeatedly applying the same generator removes the introduced artefacts.
To overcome these artefacts and to improve the general generation quality, we introduce a set of intermediate loss functions.

\subsection{Intermediate Discriminator Loss Functions}
In this section we present two classes of intermediate discriminator loss (IDL) functions, where we include an additional discriminator in the learning process, which is fed by the intermediate images.
While IterGANs produce intermediate generated images, so far it was an implicit assumption that these would be realistic images as well.  
The additional IDLs will enforce to learn to generate realistic intermediate images, either in a \emph{unsupervised} setting, \ie which only requires real images $A$ and $T$, or in a \emph{supervised} setting, \ie where we require a sequence of images $A$ and $\{T^i\}_{i=1}^{k}$, see \autoref{fig:unsupervised} and \autoref{fig:supervised}.

\myparagraph{Unsupervised Intermediate Discriminator Loss}
In the Unsupervised IDL model we include a discriminator, with the goal to tell apart image A or T from any of the generated images $\{B^i\}_{i=1}^k$,  unconditioned on the original input image.
The generator, on the other hand, aims to fool this discriminator (as well as the main discriminator).
This results in the following loss functions:
\begin{align}
	\LL_\Gen^{(\text{\wip{igu}})} &= \LL_\Gen^{(\text{\wip{ig}})} + \lambda_u \ H[\Dis_\mu(B^i), 1]\label{eq:mm-lgen-m3} \\
	\LL_\Dis^{(\text{\wip{igu}})} &= \LL_\Dis^{(\text{\wip{ig}})} + \lambda_u \ \big( H[\Dis_\mu(B^i), 0] + H[\Dis_\mu(A \lor T), 1] \big) \label{eq:mm-ldis-m3}
\end{align}
where $B^i$ is uniformly sampled from $\{B^i\}_{i=1}^k$, and either $A$ or $T$ is used, $\lambda_u$ is an additional hyper-parameter.
Since no additional labeled data is required, this is an unsupervised IDL and we coin this model \wip{igu}, see \autoref{fig:unsupervised}.

\myparagraph{Supervised Intermediate Discriminator Loss}
The second extension also adds a discriminator, but conditioned on the input image $A$. 
It aims to discriminate whether $\{B^i\}_{i=1}^k$ is a real or a generated rotation from image $A$, and uses the intermediate target images $\{T^i\}_{i=1}^k$ for supervision. 
While this discriminator is also conditioned and therefore behaves more similar to the conditional discriminator in the Pix2Pix/\wip{igmodel}, the difference is that the goal of the main discriminator is to detect if the output is a real $R^o$ rotation of the input $A$ (R=30 in most of our experiments), and the aim of the new discriminator is to accepts an arbitrarily rotation of image $A$.

The loss functions for this model are:
\begin{align}
    \LL_\Gen^{(\text{\wip{Model 4}})} &= \LL_\Gen^{(\text{\wip{Model 2}})} + \lambda_s \ H[\Dis_\nu(A,B^i), 1] \label{eq:mm-lgen-m4}\\
    \LL_\Dis^{(\text{\wip{Model 4}})} &= \LL_\Dis^{(\text{\wip{Model 2}})} + \lambda_s \left( H[\Dis_\nu(A,B^i), 0] + H[\Dis_\nu(A,T^i), 1] \right)\label{eq:mm-ldis-m4}
\end{align}
where per image pair ($A, T$) randomly a single pair ($B^i,T^i$) is sampled, with $i= \{1,\ldots,k-1\}$.
In order to train this losses, we need additional supervision in the form of the intermediate real images $\{T^i\}_{i=1}^k$, therefore this is a supervised IDL, and we coin this method \wip{smodel} model, see \autoref{fig:supervised}.

\subsection{Training IterGANs to Control Object Manipulation}\label{sec:stepwise}
In this section we aim to use IterGANs to control object rotation, where the number of iterations ($k$) is determined by the desired rotation of the object.
We see this as a explicit alternative to methods which learn implicit control, by adding a signal to representation of the generator network, see \eg~\cite{park17cvpr}. 

In fact, the supervised IDL method already uses sequences of images of a transforming (rotating) object to train, however the learning objective aims for a fixed rotation and a fixed number of iterations $k$.
Here, we control object rotation, by varying the number of iterations ($k$) depending on the desired rotation of the object.
Therefore, instead of training on input-target pairs with a fixed rotation, we sample a value of $k \in \{1,\ldots,36\}$ and select an input/target pair with the according different degrees of rotation $(5,10,\ldots,180)$.
Each train step, the generator is repeated $k$ times and aims to generate an image of $d=5^\circ\times k$~degrees rotation.
When $k>1$ the IDL can still be used to discriminate the intermediate results sampled from $\{B^i\}_{i=1}^{k-1}$.

\myparagraph{Stepwise learning to rotate objects}
To learn optimally for controlling manipulation, we combine the insight that learning small rotations are easier than larger rotations, and that the intermediate images in IterGANs might produce artefacts.
Therefore we adhere to a training procedure, which slowly increase the degree of rotation. In practice we start with $k=1$ for a few epochs, then in 3 steps increases the range of $k$ to the full range ($k \in \{1,\ldots,36\}$).

% !TEX root =  itergan18eccv_arxiv.tex\section{Experiments}\label{sec:ex}\myparagraph{Experimental Setup and Evaluation Measures}
For most of our experiments we use the data of the Amsterdam Library of Object Images (ALOI)~\cite{geusebroek2005amsterdam}.
This set contains images of 1000 household objects, photographed under constrained lighting conditions, from different viewing directions using a turntable setup.
Each object is rotated in steps of $5^\circ$, resulting in 72 images per object, which were padded and scaled to fit the $256\times256$ pixel size of the networks.

For training and testing, we split this data into three different sets.
First, for training we use a subset of the data containing 800 objects in 36 pairs of pictures with an angle of $30^\circ$, resulting in $28.8$k images.
For testing we use two different sets, the first contains 100 objects from the train set, but with different start (and thus target) rotations, resulting in $3.6$k images, referred to as \emph{Seen objects}.
The other test set, also contains a set of 100 objects, albeit not present in the train set, \ie$\text{Train objects} \cup \text{Test objects} = \emptyset$.
This set is referred to as \emph{Unseen objects} and it also contains $3.6$k images.

We train all models for 20 epochs, with the hyper parameters from~\cite{pix2pix2016} and (if used) $\lambda_u = \lambda_s =0.1$.
To counteract bad initialisation of the models, each was trained multiple times (3), and the best were used for comparison.
%Source code, the trained models, and the train and test splits used will be made available upon acceptance.
Source code, the trained models, and the train and test splits used are available on GitHub.

Evaluating the quality of generated images is hard by itself~\cite{wang2002image,salimans16nips}, therefore we use different evaluation measures.
Pixels in target and output generated images are scaled to $(-1,1)$ by using a tanh activation function in the last layer of the neural net, we then evaluate using:
\begin{enumerate}
	\item the mean absolute distance between pixels ($L_1$ loss, also used in~\cite{pix2pix2016});
	\item the object specific mask loss (\wmad loss, \autoref{eq:L1m}); and
	\item the Kullback-Leibner Label divergence:
		\vspace{-2mm}\begin{equation}D_{\textrm{KL}}(p(y|B^k) || p(y|T)),\vspace{-2mm}\end{equation}
		to measure the similarity of the label distributions $p(y|\cdot)$, obtained from a pre-trained VGG16, between the generated image $B$ and the target image $T$. 
		This measure is inspired on the KL measure used to measure specificity and diversity in~\cite{salimans16nips}, however we aim that the generated image is realistic and therefore has a similar label distribution to the target image.
\end{enumerate}
Preliminary experiments with other evaluation measures, including SSIM~\cite{wang2004image} and VIFp~\cite{sheikh2006image}, showed similar order of results as the \wmad-loss, and are therefore not included in the overview.

%%%%%%%%%%%%%%%%%%%%%% Figure 2%%%%%%%%%%%%%%%%%%%%\begin{figure}[t]
	\centering
	\subcaptionbox{Baseline Comparison\label{tab:overview}}{\resizebox*{!}{31.5mm}{\input{experiments_table1}}}
	\subcaptionbox{Cumulative plot of \wmad\label{fig:cumulative}}{\vspace{-3mm}\includegraphics[width=.5\textwidth]{experiments_mmad_scores}}
    \caption{%
		Model comparison: evaluation of different models (\emph{left}) and a cumulative plot of \wmad measure of the first four models (\emph{right}).
        In a cumulative plot, each line indicates the data percentage with that score or lower, thus the earlier a line reaches a value of $1$, the better the model.
    }    
    \label{fig:exp}
\end{figure}%%%%%%%%%%%%%%%%%%%%\vspace{-3mm}\subsection{IterGANs on ALOI}\vspace{-2mm}
In the first experiment, we compare different \wip{ig} models, to three baselines: 
Pix2Pix, which is identical/similar as using k=1, and two non-learning transformations, the identity projection (B=A) and the projective transformation, which rotates the image plane assuming a pinhole camera to compute point-pairs for calculating the transformation matrix.
To compare we introduce the IG models (\wip{ig6}), with both IDL (\wip{ig6u}, \wip{ig6s}), and the \wmad-loss (\wip{ig6m}, \wip{ig6mu}, \wip{ig6ms}).
As reduction of hyper-parameters, we use $k=6$ for these models, since the targets are available at $5^\circ$ intervals.
For different values of $k$ we already showed the existance of artefacts in intermediate images (see \autoref{fig:artefacts}).

From the results in \autoref{fig:exp} (\emph{left}) we observe that for $L_1$ and $L_1^\textrm{M}$ the learning methods outperform the non-learning baselines, while for $D_{\textrm{KL}}$ the later are very strong.
This is probably because VGG16 has learned to be invariant versus object viewpoint, while subtle differences in local image statistics can have a large impact. 
The \wip{ig6} model improves the \wip{p2p} model on \wmad and $D_\textrm{KL}$, while small, it is significant according to the non-parametric Friedman test where each image judges the two models on the results of all images. 

\begin{figure}[b]
    \centering
    \vspace{-5pt}
    \includegraphics[height=90pt]{exp_scores_exp1}
    \vspace{-4mm}
    \caption{\small
        Cumulative scores showing the differences between \wip{1x30} and \wip{6x?} in detail.
        Shown is that for both seen (solid) and unseen (dashed) data, \wip{6x?} scores slightly better than \wip{1x30}.
    }
    \label{fig:ppvsig}
\end{figure}\begin{figure}[t] 
    \centering
    \includegraphics[width=\textwidth]{exp_id_vs_angle1}
    \vspace{-5mm}
    \caption{
        Difference in \wmad of \wip{Model 1} and \wip{Model 2} split in input angle and object ID.
        The y-axis indicates the \wmad score, where the x-axis indicates either the input angle from the camera (\emph{left}) or the ID of the object (\emph{right}).
        *To increase readability, the IDs are sorted by the scores of our model.
    }
    \label{fig:idangle}
    \vspace{-5mm}
\end{figure}\myparagraph{Detailed comparison}
In order to gain more insight in the difference between \wip{1x30} and \wip{6x?} models, we compare these two in different ways.
First, in \autoref{fig:ppvsig} we show the a cumulative histogram of the \wmad score for both models.
This plot shows that for both seen and unseen objects, \wip{6x?} scores better than \wip{1x30}.
An independent T-test of both populations supports this by indicating a significant difference between the two models. 


Second, in \autoref{fig:idangle}, we compare \wip{Model 1}  and \wip{6x?}  where the objects are sorted by viewing angle of the input (\emph{left}) or based on object identity (\emph{right}), and then averaged over all test examples of the same angle/ID.
From this figure, we can draw the conclusions that the viewing angle of the input image is not significantly influencing the performance.
On the other hand, there is a clear relation between object and \wmad performance.
Apparently, for both models, some objects are more difficult to turn around than others.

The object identity figure also show a preference for the \wip{6x?} model, given that \wip{6x?} outperforms (\ie has a lower \wmad score than) \wip{1x30} for every angle and almost every object.

\begin{figure}[b]
	\centering
    \vspace{-5pt}
	\includegraphics[width=\textwidth]{experiments_mmad_scores2}
    \vspace{-8mm}
    \caption{%
		Evaluation of the performance of adding IDL (\emph{left}) and \wmad-loss (\emph{right}) to the training.
        \wmad-loss increases performance on the generated image, and IDL only when used in combination.
    }    
    \label{fig:exp2}
\end{figure}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Page of Illustrations                      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\begin{figure*}[p] 
    \centering
    \centerline{\includegraphics[width=1.1\textwidth]{experiments_illustration_qualitative}}
    \caption{
        Qualitative comparison of the models (\emph{columns}). The top three rows show a $30^\circ$ rotation of seen objects, and the bottom three for unseen objects.
        The green rectangle is magnified to better compare details.
    }
    \label{fig:examples}
\end{figure*}\begin{figure*}[p] % between rotate
    \centering
    \centerline{\hspace{-15pt}\includegraphics[width=1.1\textwidth]{experiments_illustration_rotation}}
    \caption{
        Inter- and extrapolation of the different models (\emph{rows}) with the ground truth at top.
        The columns show the rotation from the input.        
        The iterative GANs show more realistic generated images for a wide range of angles.
        \emph{(Best viewed in colour, zoom in for details)}
    }
    \label{fig:ex-between4}
\end{figure*}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\myparagraph{IterGAN extentions}
The scores in \autoref{fig:exp2} show the performance of the introduced extentions: IDL and \wmad-loss.
The IDLs were introduced to supress the artefacts from the intermediate results.
These models (\wip{igu} and \wip{igs}) score lower on the performance of the final image.
However, qualitative examples show a reduction of artefacts in the intermediate images $\{B^i\}_{i=1}^{k-1}$.
See \autoref{fig:ex-between4} for these examples.
The fact that the final results of these models is lower than \wip{6x?} could be caused because finding the solution satisfying both intermediate and overall quality is much harder than any solution which satisfy only final quality, thus making it harder to find a good solution during training.

Learning with the \wmad-loss shows a clear increase in performance of the models.
This result is not unsuprisingly, since the models are now also trained on the test score, and the large background is taken into account during training.
The fact that the IDL models now score better could be that the \wmad-loss helps the generator more to find a solution that fools the discriminators.
The small difference in performance of the unsupervised and supervised IDL, where the second one has more target-data available during trainig, could either be that the models already perform at their best, or that the extra data is underused by the current training method.

We therefore conclude that the introduction of \wmad-loss clearly outperforms the other methods, and that adding the unsupervised IDL (\wip{ig6mu}) further improves the quality of the image, without the need of extra train data.

\subsection{Control object rotation}
Since IterGANs generate the final image in steps, each step could be interpreted as a partial rotation.
In this experiment we show what happens when the amount of iterations $k$ is used to control the amount of rotation, \ie~inter- and extrapolate the generator to generate different angles.
Furthermore, we show how using variable $k$ during training (as explained in \autoref{sec:stepwise}) influences the performance of the partial rotations.

As baseline Pix2pix was used in our default setting of $30^\circ$ (which is only able to generate images rotated a multiple of $30^\circ$), and one trained on $5^\circ$ image pairs (Pix2pix$^{5^\circ}$).
The three models with variable $k$ during training are \wip{stepa}, \wip{stepi} and \wip{stepnu}, where $k^a$ indicates the full range of $k$ used for each epoch, and $k^i$ indicates stepwise increase from $k\in\{1\}$ to the full range in three steps of five epochs.
Furthermore, the best model with fixed $k$ during trainig (\wip{ig6mu}) is also used for comparison.

The graph in \autoref{fig:rot} shows the performance of each model for different angles (thus different values of $k$).
The baseline of Pix2pix$^{5^\circ}$ shows an upper bound of quality, and Pix2pix shows the inflexibility of interpolated angles.
A clear phenomenon is the periodic patern of \wip{ig6mu}, with phase two it is one of the best scoring models, or the worst, showing the effect of the artefacts in the intermediate images, as also seen in the examples in \autoref{fig:ex-between4}.
This phenomenon shows that even with IDL, it is hard to train the generator to produce good intermediate images because of the complex parameter space.
The models with variable $k$ do not show this behaviour, each iteration of the generator creates a slightly rotated image.
The graph also shows that training with incrementing $k$ outperforms training all with values for $k$ from the start, indicating that smaller angles first helps the training process.
Lastly, the IDL no longer seems to have any effect, probably because the generator is already forced to produce good intermediate images since it was also trained on smaller values of $k$.
The use of IDLs could be benificial again if not all data of the intermediate images is available, \eg~only $k\in\{1,6,10\}$ has targets to train on.

\begin{figure}[t]
    \centering
    \includegraphics[width=.9\textwidth]{figures/rotated.pdf}
    \caption{
        Performance on control of the rotation. Each bar indicates the mean \wmad-score of that model on images generated of the angle indicated by the x-axis.
        Seen objects (\emph{top}) and unseen objects (\emph{bottom}) are rotated by repeating the trained generator.
        Note that since Pix2pix rotates in steps of $30^\circ$, most angles cannot be generated by this model, and has no bar indicating the score for those angles.
    }
    \label{fig:rot}
    \vspace{-5mm}
\end{figure}\begin{figure}
    \centering
    \raisebox{16mm}{\resizebox*{!}{32.5mm}{\input{experiments_table3}}}
    \includegraphics[width=.5\textwidth]{experiments_unseen}
    \vspace{-3mm}    
    \caption{
        Scores comparing the models on {\em unseen objects}. Right shows the mean$\pm$std.\ \wmad-scores on both \emph{seen} and \emph{unseen} datasets, and left a cumulative plot of four of these models for visual comparison. \wip{ig6mu} performs best on seen data, however \wip{stepi} and \wip{stepnu} score almost as good on unseen data.
    }
    \label{fig:unseen}
\end{figure}\vspace{-5pt}\subsection{Unseen objects}
In this final experiment we compare the results on the two test sets: the \emph{seen} and \emph{unseen} objects.
The results are shown in \autoref{fig:unseen}.
We observe, as is expected, that the performance on the seen objects is in general better than the unseen objects, but also that training for the \wmad metric does not improve the results noticeably in this case.
Moreover, as seen from the qualitative results in \autoref{fig:examples}, the \wip{s6mmad} model also introduces artefacts in background of the final image.
However, the models trained with variable $k$ show they are less affected by unseen objects, the drop in performance is noticabily less.
This last indicates that these models can rotate objects in images, without the object necessarily part of the training procedure.
\Ie~the models can use the depth cues in a single image to understand the 3D properties of any object.

\begin{figure}[b]
    \centering
    \vspace{-2mm}
    \let\temp\tabcolsep
    \setlength{\tabcolsep}{5pt}{\scriptsize
    \begin{tabular}{l|cc}
    & seen & unseen \\ \hline
    Pix2pix    & $0.077\pm0.017$ & $0.216\pm0.019$ \\
    \wip{ig6}  & $0.073\pm0.015$ & $0.209\pm0.020$ \\
    \wip{ig6u} & $0.073\pm0.015$ & $0.201\pm0.024$
    \end{tabular}
    }
    \setlength{\tabcolsep}{\temp}
    \caption{
        Table of $L_1$-scores on VKITTI trained models.
    }
    \label{fig:vkscore}
\end{figure}\subsection{Camera rotation on VKITTI}
In our final experiment, we explore a different rotation problem: camera rotation.
The task is to generate a scene from a different camera viewpoint. 
For this task we use the Virtual-KITTI dataset~\cite{Gaidon:Virtual:CVPR2016}, which is a synthetic version of KITTI~\cite{geiger12cvpr} that contains a side view ($30^\circ$ rotated from the main camera).
Now the task is to generate this side view from the main camera.
Since the proposed models are based on image pairs, we use all frames independently and not as a video stream.

We use pairs of $728\times256$ pixels images, and select four sequences for training.
Since the models are fully convolutional, a three times as wide input image will only change the output of each layer, not the amount of parameters.
Yet, given that the dataset is smaller than ALOI (and that the intermediate rotations were not available), we evaluate only \wip{Model 1}, \wip{Model 2} and \wip{Model 3}, trained for 50 epochs.
Also note that since there are no foreground masks, the $L_1$-measure was used throughout both training and testing.

\begin{figure}
    \centering
    \includegraphics{vkitti.pdf}
    \caption{
        Illustration of generated images from the VKITTI dataset. The \wip{v:ss} model iteratively rotates and add details.
        \emph{(Best viewed in colour, zoom in for details)}
    }
    \label{fig:er-vk-between}
\end{figure}%1 paragraaf tof resultaat
In \autoref{fig:er-vk-between} we show examples of the trained models.
For both \wip{Model 2} and \wip{Model 3} we show for an given input the intermediate and final generated image. 
Similarly as in the ALOI experiments, the \wip{Model 2} has periodic artefacts in the intermediate steps, which are reduced by the supervision of the second discriminator in \wip{Model 2}.
The intermediate images from \wip{Model 3} seem to add details in every step, besides rotating the scene.

The final performance is rather similar for all three models (see \autoref{fig:vkscore}).
On test images from the train sequences the $L_1$-measure is around $.075$, while for the unseen test sequences the performance is around $.21$, with \wip{Model 2} being marginally better in both scenarios. 
Therefore, we conclude that Iterative GANs are suitable for generating rotated images.



\vspace{-3mm}
\section{Conclusion \& Outlook}
\label{sec:conc}
\vspace{-3mm}
In this paper we have introduced IterGANs, a model to iteratively transform an image into a target image whereby the generator has to learn only a small transformation.
IterGANs are in part inspired on the Shepard and Metzler\cite{shepard1971mental} mental rotation experiment\footnote{In \autoref{fig:mental} the middle objects are not only rotated, but also mirrored.}. Our experiments have shown that IterGANs outperform a direct transformation GAN.

Moreover the intermediate generated images allow for extending loss functions, by using intermediate discriminators, either supervised or unsupervised.
Surprisingly the unsupervised loss functions outperform the supervised ones.

Finally the IterGANs allow for controlling the image manipulation by calling the generator different number of times.
Learning with increasing number of iterations, \ie~sample from an increasing amount of possible $k$, shows an increased quality of the final results.
Our experiments on rotating objects never seen at train time, show that our proposed models learn a generic object appearance and transformation generator. 
Future research could investigate extension of the control signal beyond rotation to a full 3D transformation of both objects and~scenes.

\section*{Acknoledgements}
\vspace{-3mm}
This research is supported in part by the NWO VENI What\&Where project.

\clearpage
\bibliography{bib}
\bibliographystyle{splncs}

\clearpage
\begin{appendix}
\section{Projective transformation}
One of the baselines introduced in the paper is a projective baseline: approximate the rotation with a projective transformation matrix.
In this supplementary, a more detailed explanation of the computation, and several examples of qualitative results.

When assuming the object to be a plane, and using the assumption of a pinhole camera, a projective transformation can be performed to transform the images.
Because the position of the camera with respect to the object is known for the ALOI dataset, the computation is relatively straightforward.

The rotation matrix for a $30^\circ$ rotation ($\tfrac{\pi}{6}$rad) around the $Z$ axis is:
\begin{equation} \label{eq:ex-bp-rot}
\begin{pmatrix} \hat{x}\\\hat{y}\\\hat{z} \end{pmatrix} = 
\begin{pmatrix} \cos\frac\pi6 & \sin\frac\pi6 & 0 \\ -\sin\frac\pi6 & \cos\frac\pi6 & 0 \\ 0 & 0 & 1 \end{pmatrix}
\begin{pmatrix} x\\y\\z \end{pmatrix}
\end{equation}

When using the pinhole camera model to compute where points on this rotated plane and there origin are projected on the image, we need to define the origin.
Let's assume the origin of the 3D orthogonal basis is fixed at the base of the object, the position of the camera is at $(0,124.5,30)$ cm in (X,Y,Z), then we obtain:
\begin{equation} \label{eq:ex-bp-phole}
\begin{pmatrix} \lambda i \\ \lambda j \\ \lambda \end{pmatrix} =
\underbrace{\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & \frac1f \end{pmatrix}}_\text{Internal matrix}
\underbrace{\begin{pmatrix} 1 & 0 & 0 & 0\\ 0 & 1 & 0 & 30\\ -1 0 & 0 & -124.5 \end{pmatrix}}_\text{External matrix}
\begin{pmatrix} x\\y\\z \end{pmatrix},
\end{equation}
where $x,y,z$ are real world coordinates, and $i,j$ are pixel coordinates, with $\lambda$ being a scaling factor.
Therefor $i,j$ can be computed from $x,y,z$ and for the rotated version $\hat{\i},\hat{\j}$ can be computed from $\hat{x},\hat{y},\hat{z}$.

The pixel to rotated-pixel transformation matrix is defined as:
\begin{equation} \label{eq:ex-pb-trans}
\begin{pmatrix} \lambda \hat{\i} \\ \lambda \hat{\j} \\ \lambda \end{pmatrix} =
\begin{pmatrix} T_{11} & T_{12} & T_{13} \\ T_{21} & T_{22} & T_{23} \\ T_{31} & T_{32} & T_{33} \end{pmatrix}
\begin{pmatrix} i \\ j \\ 1 \end{pmatrix}
\end{equation}
To solve for the $T$s we need at least 4 pairs of $i,j$ and $\hat{\i},\hat{\j}$. 
Thus, we use the four points on the $XZ$-plane (\ie a unit square) to compute $T$.
With this transformation matrix $T$ new images can be created from the images of the dataset using bilinear interpolation with \autoref{eq:ex-pb-trans} for all pixels.
Examples of transformed images can be seen in \autoref{fig:ex-seen0}.

\begin{figure}[hbtp] % proj examples
    \centering
    \includegraphics{supp_00.pdf}
    \caption{
        Several examples of the projective baseline.
        The columns show input, output, target, pixel-wise difference between output and target, and $L_1^\textrm{M}$ score.
        For illustration purposes grey values indicate the out-of-plane pixels, when computing $L_1^\textrm{M}$, we use the background colour (black).
    }
    \label{fig:ex-seen0}
\end{figure}


\clearpage
\begin{landscape}
\begin{figure}[h!] % examples of different models
    \section{Additional qualitative examples}
    \vspace{-4mm}
    \centering
    \includegraphics{supp_0.pdf}
    \vspace{-5mm}
    \caption{
        Examples of \emph{seen objects} as rotated by the different models (columns). Each row shows a $30^\circ$ rotation of the input (leftmost), and the ground truth is given as rightmost.
        The green rectangle is magnified to better compare details.
    }
    \label{fig:examplesa}
\end{figure}

\begin{figure} % examples of different models
    \centering
    \includegraphics{supp_1.pdf}
    \caption{
        Examples of \emph{unseen objects}, similar to \autoref{fig:examplesa}.
    }
    \label{fig:examplesb}
\end{figure}
\end{landscape}


\begin{figure}[p] % between rotate
    \centering
    \centerfloat
    \includegraphics{supp_2.pdf}
    \caption{
        Rotating a \emph{seen object} with the different models (rows, first is ground truth) over different angles (columns).
    }
    \label{fig:ex-between4a}
\end{figure}

\begin{figure}[p] % between rotate
    \centering
    \centerfloat
    \includegraphics{supp_23.pdf}
    \caption{
        Another \emph{seen} example like \autoref{fig:ex-between4a}
    }
    \label{fig:ex-between4b}
\end{figure}

\begin{figure}[p] % between rotate
    \centering
    \centerfloat
    \includegraphics{supp_3.pdf}
    \caption{
        Rotating an \emph{unseen object} with the different models (rows, first is ground truth) over different angles (columns).
        Similar to \autoref{fig:ex-between4a} and \ref{fig:ex-between4b}, only now for objects never seen during training.
    }
    \label{fig:ex-between5a}
\end{figure}

\begin{figure}[p] % between rotate
    \centering
    \centerfloat
    \includegraphics{supp_4.pdf}
    \caption{
        Another \emph{unseen} example like \autoref{fig:ex-between5a}.
    }
    \label{fig:ex-between5b}
\end{figure}

\end{appendix}

\end{document}