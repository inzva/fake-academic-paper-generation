
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt, a4paper, conference]{IEEEtran}
% Add the compsocconf option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{epstopdf}
\usepackage{multirow}
\usepackage{CJK}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2
\end{tabular}}
\usepackage{subfig}
\usepackage[colorlinks,linkcolor=red]{hyperref}
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Large-scale Isolated Gesture Recognition Using Convolutional Neural Networks}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{Pichao Wang$^{\rm 1}$, Wanqing Li$^{\rm 1}$, Song Liu$^{\rm 1}$, Zhimin Gao$^{\rm 1}$, Chang Tang$^{\rm 2}$ and Philip Ogunbona$^{\rm 1}$\\
\\
$^{\rm 1}$Advanced Multimedia Research Lab, University of Wollongong, Australia\\
$^{\rm 2}$School of Information Science and Engineering, Wuhan University of Science and Technology, Wuhan, China\\
\\
{ pw212@uowmail.edu.au, \{wanqing, songl\}@uow.edu.au, zg126@uowmail.edu.au}\\
{ tangchang@wust.edu.cn, philipo@uow.edu.au}
}






% make the title area
\maketitle


\begin{abstract}
%In this paper we proposed three simple, compact yet effective representations of depth sequences for gesture recognition in the context of convolutional Neural networks (ConvNets). The three representations are called Dynamic Depth Image (DDI), Dynamic Depth Normal Image (DDNI) and Dynamic Depth Motion Normal Image (DDMNI). They are all based on bidirectional rank pooling method converting the depth sequences into images. Such representations enables the use of existing ConvNets models directly on video data with fine-tuning without introducing large parameters to learn. The three representations represent the posture and motion in different levels and they are complementary to each other and improve the recognition accuracy largely. The proposed method is evaluated on the Large-scale Isolated Gesture Recognition at the ChaLearn Looking at People (LAP) challenge 2016. Our method obtains the performance of 62.25\% and ranks the $1^{st}$ place in this challenge.
This paper proposes three simple, compact yet effective representations of depth sequences, referred to respectively as Dynamic Depth Images (DDI), Dynamic Depth Normal Images (DDNI) and Dynamic Depth Motion Normal Images (DDMNI). These dynamic images are constructed from a sequence of depth maps using bidirectional rank pooling to effectively capture the spatial-temporal information. Such image-based representations enable us to fine-tune the existing ConvNets models trained on image data for classification of depth sequences, without introducing large parameters to learn. Upon the proposed representations, a convolutional Neural networks (ConvNets) based method is developed for gesture recognition and evaluated on the Large-scale Isolated Gesture Recognition at the ChaLearn Looking at People (LAP) challenge 2016. The method achieved 55.57\% classification accuracy and ranked $2^{nd}$ place in this challenge but was very close to the best performance even though we only used depth data.

\end{abstract}

\begin{IEEEkeywords}
gesture recognition; depth map sequences; Convolutional Neural Networks

\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\begin{figure*}[t]
\begin{center}{\includegraphics[height = 130mm, width = 180mm]{framework}}
\end{center}
\caption{The framework for proposed method.}
\label{fig:framework}
\end{figure*}


\section{Introduction}
%Gestures are naturally performed by humans, produced as part of deliberate actions, signs or signals, or subconsciously revealing intentions or attitude~\cite{escalera2016challenges}. They may involve the motion of all parts of the body. The studies in research usually focus on arms and hands which are essential for action and communication. Gesture recognition has attracted researchers' eyes due to its indubitable importance in human activities, such as Human Computer Interaction (HCI), Human Robot Interaction (HRI) and security. It also has an important social impact in assistive technologies for the handicapped and the elderly.

Gestures are naturally performed by humans, produced as part of deliberate actions, signs or signals, or subconsciously revealing intentions or attitude~\cite{escalera2016challenges}. While they may involve the motion of all parts of the body, the studies of gestures usually focus on arms and hands which are essential in gesture communication. Recognition of gestures has recently attracted increasing attention due to its indubitable importance in many applications such as Human Computer Interaction (HCI), Human Robot Interaction (HRI) and assistive technologies for the handicapped and the elderly.

Gestures are one type of actions and many action recognition methods can be applied to gesture recognition. Recognition of human actions from depth/skeleton data is one of the most active research topics in multimedia signal processing in recent years due to the advantages of depth information over conventional RGB video, e.g. being insensitive to illumination changes. Since the first work of such a type~\cite{li2010action} reported in 2010, many methods~\cite{wang2012mining,Yang2012a,Oreifej2013,Gowayyed2013_HOD,yangsuper,rahmani2014hopc,
pichao2014,lurange,escalante2015principal,Vemulapallia2016,zhang2016rgb} have been proposed based on specifical hand-crafted feature descriptors extracted from depth/skeleton. With the recent development of deep learning, a few methods have been developed based on Convolutional Neural Networks (ConvNets)~\cite{pichao2015,pichaoTHMS,pichao2016,pichaoicprwa,pichaocsvt2016} and Recurrent Neural Networks (RNNs)~\cite{du2015hierarchical,veeriah2015differential,zhu2015co,shahroudy2016ntu}.  However, it remains unclear how video could be effectively represented and fed to deep neural networks for classification. For example, one can conventionally consider a video as a sequence of still images with some form of temporal smoothness, or as a subspace of images or image features, or as the output of a neural network encoder. Which one among these and other possibilities would result in the best representation in the context of gesture recognition is not well understood. 

% In this paper, we proposed three simple, compact and effective representations of depth sequences especially when ConvNets are used for gesture recognition, inspired by~\cite{pichao2015,pichaoTHMS,pichao2016,bilen2016dynamic} where they represented a short video sequence with one image. Such representation makes it possible to use a standard ConvNets architecture to learn suitable "dynamic" features from the sequences without introducing millions of parameters to learn especially in the context of lack of annotated training video data. Even the large-scale isolated gesture recognition challenge~\cite{wanchalearn} aims to provide large annotated training data, it is still small compared to the millions of parameters to be trained (average 144 video clips per class for this challenge while 1200 images per class for ImageNet).

Inspired by the recent work in~\cite{pichao2015,pichaoTHMS,pichao2016,bilen2016dynamic}, this paper proposes for gesture recognition three simple, compact and effective representations of depth sequences which effectively decribe a short depth sequence with images. Such representations make it possible to use a standard ConvNet architecture to learn suitable ``dynamic" features from the sequences by utilizing the ConvNet models trained from image data. Consequently, it avoids training millions of parameters from scratch and is especially valuable in the cases that lack sufficient annotated training video data. For instance, the large-scale isolated gesture recognition challenge~\cite{wanchalearn} has on average only 144 video clips per class compared to 1200 images per class in ImageNet.

%There are two main factors for gesture recognition, posture and motion. The proposed three representations are called Dynamic Depth Image (DDI), Dynamic Depth Normal Image (DDNI) and Dynamic Depth Motion Normal Image (DDMNI). They are all based on bidirectional rank pooling method converting the depth sequences into images.The three representations represent the posture and motion in different levels and they are complementary to each other and improve the recognition accuracy largely.

The proposed three representations are Dynamic Depth Image (DDI), Dynamic Depth Normal Image (DDNI) and Dynamic Depth Motion Normal Image (DDMNI). They are all constructed from a sequence of depth maps based on bidirectional rank pooling to encode the spatial (i.e. posture) and temporal (i.e. motion) information at different levels and are complementary to each other. Experimental results have shown that the three representations can improve the recognition accuracy substantially. 

The rest of this paper is organized as follows. Section II briefly reviews the related works on gesture/action recognition based on depth and deep learning. Details of the proposed method are described in Section III. Experimental results are presented in Section IV. Section V concludes the paper.

\section{Related Work}
\subsection{Depth Based Action Recognition}
With Microsoft Kinect Sensors researchers have developed methods for depth map-based action recognition. Li et al. \cite{li2010action} sampled points 
from a depth map to obtain a bag of 3D points to encode spatial information and 
employ an expandable graphical model to encode temporal information 
\cite{li2008}. Yang et al. \cite{Yang2012a} stacked differences between projected depth maps as 
a depth motion map (DMM) and then used HOG to extract relevant features from the 
DMM. This method transforms the problem of action recognition from 
spatio-temporal space to spatial space.  In \cite{Oreifej2013}, a feature called Histogram of Oriented 4D 
Normals (HON4D) was proposed; surface normal is extended to 4D space and 
quantized by regular polychorons. Following this method, Yang and Tian 
\cite{yangsuper} cluster hypersurface normals and form the polynormal which can 
be used to jointly capture the local motion and geometry information. Super 
Normal Vector (SNV) is generated by aggregating the low-level polynormals. In 
\cite{lurange}, a fast binary range-sample feature was proposed based on a test 
statistic by carefully designing the sampling scheme to exclude most pixels that 
fall into the background and to incorporate spatio-temporal cues. 


\subsection{Deep Leaning Based Recognition}
Exiting deep learning approach can be generally divided into four categories based on how the video is represented and fed to a deep neural network. The first category views a video either as a set of still images~\cite{yue2015beyond} or as a short and smooth transition between similar frames~\cite{simonyan2014two}, and each color channel of the images is fed to one channel of a ConvNet. Although obviously suboptimal, considering the video as a bag of static frames performs reasonably well. The second category is to represent a video as a volume and extends ConvNets to a third, temporal dimension~\cite{ji20133d,tran2015learning} replacing 2D filters with 3D ones. So far, this approach has produced little benefits, probably due to the lack of annotated training data. The third category is to treat a video as a sequence of images and feed the sequence to a RNN~\cite{donahue2015long,du2015hierarchical,veeriah2015differential,zhu2015co,shahroudy2016ntu}. A RNN is typically considered as memory cells, which are sensitive to both short as well as long term patterns. It parses the video frames sequentially and encode the frame-level information in their memory. However, using RNNs did not give an improvement over temporal pooling of convolutional features~\cite{yue2015beyond} or over hand-crafted features. The last category is to represent a video in one or multiple compact images and adopt available trained ConvNet architectures for fine-tuning~\cite{pichao2015,pichaoTHMS,pichao2016,bilen2016dynamic}. This category has achieved state-of-the-art results of action recognition on many RGB and depth/skeleton datasets. The proposed method in this paper falls into the last category.

\section{Proposed Method}

The proposed method consists of three stages: construction of the three sets of dynamic images, ConvNets training and score fusion for classification, as illustrated in Fig.~\ref{fig:framework}. Details are presented in the rest of this section. 

\subsection{Construction of Dynamic Images}

The three sets of dynamic images, Dynamic Depth Images (DDIs), Dynamic Depth Normal Images (DDNIs) and Dynamic Depth Motion Normal Images (DDMNIs) are constructed from a sequence of depth maps through rank pooling~\cite{bilen2016dynamic}. They aim to capture both posture and motion information for gesture recognition.

\subsubsection{Rank Pooling}

Let $I_{1},...,I_{T}$ denote the frames in a sequence of depth maps, and $\varphi(I_{t}) \in \mathbb{R}^{d}$ be a representation or feature vector extracted from each individual frame $I_{t}$. Let $V_{t} = \dfrac{1}{t}\sum_{\tau=1}^{t}\varphi(I_{t})$ be time average of these features up to time $t$. The ranking function associates to each time $t$ a score $S(t|\textbf{d}) = <\textbf{d}, V_{t}>$, where $\textbf{d} \in \mathbb{R}^{d}$ is a vector of parameters. The function parameters $\textbf{d}$ are learned so that the scores reflect the rank of the frames in the video. In general, later times are associated with larger scores, $ i.e. ~q > t \Rightarrow S(q|\textbf{d}) > S(t|\textbf{d})$. Learning $\textbf{d}$ is formulated as a convex optimization problem using RankSVM~\cite{smola2004tutorial}:

\begin{equation}
\begin{aligned}
\textbf{d}^{*} &=\rho(I_{1},...,I_{T};\varphi) = \mathop{\arg\min}_{\textbf{d}} E(\textbf{d}),\\
  E(\textbf{d}) &= \dfrac{\lambda}{2}\parallel \textbf{d} \parallel^{2} +\\
  & \dfrac{2}{T(T-1)}\times\sum\limits_{q>t}max\{{0,1-S(q|\textbf{d}) + S(t|\textbf{d})}\}.
\end{aligned}
\end{equation}

The first term in this objective function is the usual quadratic regular term used in SVMs. The second term is a hinge-loss soft-counting how many pairs $q > t$ are incorrectly ranked by the scoring function. Note in particular that a pair is considered correctly ranked only if scores are separated by at least a unit margin, $i.e.~ S(q|\textbf{d}) > S(t|\textbf{d}) + 1$. 

Optimizing the above equation defines a function $\rho(I_{1},...,I_{T};\varphi)$ that maps a sequence of $T$ depth video frames to a single vector $d^{*}$. Since this vector contains enough information to rank all the frames in the video, it aggregates information from all of them and can be used as a video descriptor. This process is called rank pooling. 

\subsubsection{Construction of DDI}
%We adopted the ranking pooling method~\cite{bilen2016dynamic} described above  directly to the depth map sequence $(X,X,X)$ to convert it into dynamic images, where $X$ represents the grey depth map sequence. Different from ~\cite{bilen2016dynamic} we used bidirectional rank pooling converting one depth map sequence into two DDIs. This representation strongly describe the posture information, similar to key poses, as illustrated in Fig.~\ref{fig:DIs}. 
Given a sequence of depth maps, the ranking pooling method~\cite{bilen2016dynamic} described above is employed to generate a dynamic depth image (DDI).  The DDI is fed to the three channel of a ConvNet. Different from~\cite{bilen2016dynamic} the rank pooling is applied in a bidiretional way to convert one depth map sequence into two DDIs. As shown in Fig.~\ref{fig:DIs}, DDIs effectively capture the posture information, similar to key poses.

\subsubsection{Construction of DDNI}

In order to simultaneously exploit the posture and motion information in depth sequences, it is proposed to extract normals from depth maps and construct the so called DDNIs (dynamic depth normal images). For each depth map, the surface normal $(n_x,n_y,n_z)$ at each location is calculated. Thus, three channels $(N_x,N_y,N_z)$, referred to as a Depth Normal Image (DNI), are generated from the calculated normals, where $(N_x,N_y,N_z)$ represents normal images for the three components $(n_x,n_y,n_z)$ respectively. The sequence of DNIs goes through bidirectional rank pooling to generate two DDNIs, one  being from forward ranking pooling and the other from backward rank pooling.

To minimise the interference of the background, it is assumed that the background in the histogram of depth maps occupies the last peak representing far distances. Specifically, pixels whose depth values are greater than a threshold defined by the last peak of the depth histogram minus a fixed tolerance (0.1 was set in our experiments) are considered as background and removed from the calculation of DDNIs by re-setting their depth values to zero. Through this simple process, most of the background can be removed and has much contribution to the DDNIs.  Samples of DDNIs can be seen in Fig.~\ref{fig:DIs}.

\subsubsection{Construction of DDMNI}

The purpose of construction of a DDMNI is to further exploit the motion in depth maps. Gaussian Mixture Models (GMM) is applied to depth sequences to detect moving foreground. The same process as the construction of a DDNI ( but without using histogram-based foreground extraction) is employed to the moving foreground. This process generates two DDMNIs, which specifically capture the motion information as illustrated in Fig.~\ref{fig:DIs}. 


\begin{figure}[t]
\begin{center}{\includegraphics[height = 120mm, width = 85mm]{DIs}}
\end{center}
\caption{Samples of generated forward and backward DDIs, DDNIs and DDMNIs for gesture Mudra1/Ardhapataka.}
\label{fig:DIs}
\end{figure}


\subsection{Network Training}
After the construction of DDIs, DDNIs and DDMNIs, there are six dynamic images, as illustrated in Fig.~\ref{fig:DIs}, for each depth map sequence. Six ConvNets were trained on the six channels individually. Different layer configurations were used for the validation and testing sets provided by the Challenge. For validation, the layer configuration of six ConvNets follows the one in~\cite{krizhevsky2012imagenet}.  For testing, VGG-16~\cite{simonyan2014very} was adopted for fine-tuning. The implementation is derived from the publicly available Caffe toolbox~\cite{jia2014caffe} based on three {NVIDIA Tesla K40 GPU} cards for both validation and testing.

The training procedure for validation is similar to the one in~\cite{krizhevsky2012imagenet}. The network weights were learned using the mini-batch stochastic gradient descent with the momentum being set to 0.9 and weight decay being set to 0.0005. All hidden weight layers use the rectification (RELU) activation function. At each iteration, a mini-batch of 256 samples is constructed by sampling 256 shuffled training samples. All the images are resized to 256 $\times$ 256. The learning rate was set to $10^{-3}$ for fine-tuning with pre-trained models on ILSVRC-2012, and then it is decreased according to a fixed schedule, which is kept the same for all training sets. For each ConvNet the training undergoes 20K iterations and the learning rate decreases every 5K iterations. For all experiments, the dropout regularisation ratio was set to 0.5 in order to reduce complex co-adaptations of neurons in the nets.

For testing, the training procedure is similar to the one in~\cite{simonyan2014very}. The network weights were learned using the mini-batch stochastic gradient descent with the momentum being set to 0.9 and weight decay being set to 0.0005. All hidden weight layers use the rectification (RELU) activation function. At each iteration, a mini-batch of 32 samples was constructed by sampling 256 shuffled training samples. All the images are resized to 224 $\times$ 224. The learning rate was set to $10^{-3}$ for fine-tuning with pre-trained models on ILSVRC-2012, and then it is decreased according to a fixed schedule, which is kept the same for all training sets. For each ConvNet the training undergoes 50K iterations and the learning rate decreases every 20K iterations. For all experiments, the dropout regularisation ratio was set to 0.9 in order to reduce complex co-adaptations of neurons in the nets.

\subsection{Score Fusion for Classification}

Given a testing depth video sequence (sample), three pairs of dynamic images (DDIs, DDNIs, DDMNIs) are generated and fed into six different trained ConvNets. For each image pair, multiply-score fusion was used. The score vectors outputted by the two pair ConvNets are multiplied in an element-wise  way and then the resultant score vectors are normalized using $L_{1}$ norm. The three normalized score vectors are then multiplied in an element-wise fashion and the max score in the resultant vector is assigned as the probability of the test sequence being the recognized class. The index of this max score corresponds to the recognized class label.

\begin{table*}[!ht]
\centering
\caption{Information of the ChaLearn LAP IsoGD Dataset. \label{table1}}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Sets &\# of labels &\# of gestures & \# of RGB videos & \# of depth videos & \# of subjects & label provided \\
\hline
Training & 249 & 35878 & 35878 & 35878 & 17 & Yes \\
\hline
Validation & 249 & 5784 & 5784 & 5784 & 2 & No \\
\hline
Testing & 249 &  6271 & 6271 & 6271 & 2 & No \\
\hline
All & 249 & 47933 & 47933 & 47933 & 21 & - \\
\hline
\end{tabular}
\end{table*}

\section{Experiments}
In this section, the Large-scale Isolated Gesture Recognition Dataset at the ChaLearn LAP challenge 2016 (ChaLearn LAP IsoGD Dataset)~\cite{ICPRW2016} and the evaluation protocol are described. The experimental results of the proposed method on this dataset are presented.

\subsection{Dataset}
The ChaLearn LAP IsoGD Dataset is derived from the ChaLearn Gesture Dataset (CGD)~\cite{guyon2014chalearn}. It includes 47933 RGB-D depth sequences, each RGB-D video representing one gesture instance. There are 249 gestures performed by 21 different individuals.  The detailed information of this dataset are shown in Table~\ref{table1}.  In this paper, only depth maps were used to evaluate the performance of the proposed method. Some samples of depth sequences are shown in Fig.~\ref{fig:samples}.


\begin{figure*}[t]
\begin{center}{\includegraphics[height = 70mm, width = 180mm]{iso_pic}}
\end{center}
\caption{The samples of 21 out of 249 gestures. From top left to bottom right, they are: \\(a) ItalianGestures/Madonna; (b) GestunoTopography/92\_harbour\_port; (c) TaxiSouthAfrica/TaxiHandSigns2;\\ (d) GestunoSmallAnimals/129\_cat\_chat;(e) RefereeWrestlingSignals2/Reversal; (f) DivingSignals3/NotUnderstood;\\(g) SurgeonSignals/ArmyNavyRetractor; (h) GangHandSignals1/EastSide; (i) SwatHandSignals1/DogNeeded;\\(j) HelicopterSignals/MoveLeft; (k) GangHandSignals2/Killas; (l) TaxiSouthAfrica/TaxiHandSigns6;\\ (m) DivingSignals4/HowMuchAir; (n) ChineseNumbers/wu,TaxiSouthAfrica/TaxiHandSigns7;\\ (o) Mudra2/Vitarka,DivingSignals4/OK,GangHandSignals2/OK; (p) DivingSignals1/Around;\\(q) CanadaAviationGroundCirculation1/DirigezVousVers; (r) MusicNotes/do; (s) GangHandSignals1/Crip;\\ (t) SwatHandSignals1/Stop; (u) RefereeWrestlingSignals2/Stalling,SwatHandSignals1/Breacher.}
\label{fig:samples}
\end{figure*}


\subsection{Evaluation Protocal}

The dataset is divided into training, validation and test sets. All three sets consist of samples of different subjects so ensure that the gestures of one subject in validation and test sets will not appear in the training set. 

For the isolated gesture recognition challenge, recognition rate $r$ is used as the evaluation criteria. The recognition rate is calculated as:
\begin{equation}
r = \dfrac{1}{n}\delta(p_{l}(i),t_{l}(i))
\end{equation}
where $n$ is the number of samples; $p_{l}$ is the predicted label; $t_{l}$ is the ground truth; $\delta(j_{1},j_{2}) = 1$, if $j_{1} = j_{2}$, otherwise $\delta(j_{1},j_{2}) = 0$. 
 
 
\subsection{Experimental Results}

The results obtained by the proposed method on the validation and test sets are listed and compared with the baseline methods~\cite{pami16Jun} (MFSK and MFSK+DeepID) in Table~\ref{table2}. The codes and models can be downloaded at the author's homepage:\url{https://sites.google.com/site/pichaossites/}\href{https://sites.google.com/site/pichaossites/}.

\begin{table}[!ht]
\centering
\caption{Comparative accuracy of proposed method and baseline 
methods on the ChaLearn LAP IsoGD Dataset. \label{table2}}
\begin{tabular}{|c|c|c|}
\hline
Method & Set & Recognition rate $r$\\
\hline
MFSK & Validation & 18.65\%\\
\hline
MFSK+DeepID & Validation & 18.23\%\\
\hline
Proposed Method & Validation & \textbf{39.23\%}\\
\hline
MFSK & Testing & 24.19\% \\
\hline
MFSK+DeepID & Testing & 23.67\%\\
\hline
Proposed & Testing &\textbf{55.57\%}\\
\hline
\end{tabular}
\end{table}


The results showed that the proposed method 
significantly outperformed the baseline methods, even though only single 
modality, i.e. depth data, was used while the baseline method used both RGB and 
depth videos.

The challenge results are summarized in Table~\ref{table3}. We can see that our method is among the top performers and our recognition rate is very close to the best performance of this challenge (55.5733\% vs. 56.8968\%), even though we only used depth data for proposed method while the winner~\cite{yunanli} adopted both depth and RGB modalities.

\begin{table}[!ht]
\centering
\caption{Comparsion the performance of our submission with those of other teams. Our team secures the second place in the ICPR ChaLearn LAP challenge 2016. \label{table3}}
\begin{tabular}{|c|c|c|}
\hline
Rank & Team & Recognition rate $r$\\
\hline
1 & FLiXT~\cite{yunanli} & 56.8968\%\\
\hline
2 & \textbf{AMRL (ours)} & 55.5733\%\\
\hline
3 & XDETVP-TRIMPS~\cite{guangming} & 50.9329\%\\
\hline
4 & ICT\_NHCI & 46.8027\%\\
\hline
5 & XJTUfx & 43.9164\%\\
\hline
6 & TARDIS & 40.1531\%\\
\hline
7 & NTUST & 20.3317\%\\
\hline
\end{tabular}
\end{table}

\section{Conclusions}

This paper presented three simple, compact yet effective representations of depth sequences for gesture recognition using convolutional Neural networks. They are all based on bidirectional rank pooling method converting the depth sequences into images. Such representations enables the use of existing ConvNets models directly on video data with fine-tuning without introducing large parameters to learn. The three representations represent the posture and motion in different levels and they are complementary to each other and improve the recognition accuracy largely. Experimental results on ChaLearn LAP IsoGD Dataset verified the effectiveness of the proposed method.



\section*{Acknowledgment}
The authors would like to thank NVIDIA Corporation for the donation of a Tesla K40 GPU card used in this challenge.




% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{escalera2016challenges}
S.~Escalera, V.~Athitsos, and I.~Guyon, ``Challenges in multimodal gesture
  recognition,'' \emph{Journal of Machine Learning Research}, vol.~17, no.~72,
  pp. 1--54, 2016.

\bibitem{li2010action}
W.~Li, Z.~Zhang, and Z.~Liu, ``Action recognition based on a bag of {3D}
  points,'' in \emph{Proc. IEEE Computer Society Conference on Computer Vision
  and Pattern Recognition Workshops (CVPRW)}, 2010, pp. 9--14.

\bibitem{wang2012mining}
J.~Wang, Z.~Liu, Y.~Wu, and J.~Yuan, ``Mining actionlet ensemble for action
  recognition with depth cameras,'' in \emph{Proc. IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR)}, 2012, pp. 1290--1297.

\bibitem{Yang2012a}
X.~Yang, C.~Zhang, and Y.~Tian, ``Recognizing actions using depth motion
  maps-based histograms of oriented gradients,'' in \emph{Proc. ACM
  international conference on Multimedia (ACM MM)}, 2012, pp. 1057--1060.

\bibitem{Oreifej2013}
O.~Oreifej and Z.~Liu, ``{HON4D}: Histogram of oriented {4D} normals for
  activity recognition from depth sequences,'' in \emph{Proc. IEEE Conference
  on Computer Vision and Pattern Recognition (CVPR)}, 2013, pp. 716--723.

\bibitem{Gowayyed2013_HOD}
M.~A. Gowayyed, M.~Torki, M.~E. Hussein, and M.~El-Saban, ``Histogram of
  oriented displacements ({HOD}): Describing trajectories of human joints for
  action recognition,'' in \emph{Proc. International Joint Conference on
  Artificial Intelligence (IJCAI)}, 2013, pp. 1351--1357.

\bibitem{yangsuper}
X.~Yang and Y.~Tian, ``Super normal vector for activity recognition using depth
  sequences,'' in \emph{Proc. IEEE International Conference on Computer Vision
  and Pattern Recognition (CVPR)}, 2014, pp. 804--811.

\bibitem{rahmani2014hopc}
H.~Rahmani, A.~Mahmood, D.~Q. Huynh, and A.~Mian, ``{HOPC}: Histogram of
  oriented principal components of {3D} pointclouds for action recognition,''
  in \emph{Proc. European Conference on Computer Vision (ECCV)}, 2014, pp.
  742--757.

\bibitem{pichao2014}
P.~Wang, W.~Li, P.~Ogunbona, Z.~Gao, and H.~Zhang, ``Mining mid-level features
  for action recognition based on effective skeleton representation,'' in
  \emph{Proc. International Conference on Digital Image Computing: Techniques
  and Applications (DICTA)}, 2014, pp. 1--8.

\bibitem{lurange}
C.~Lu, J.~Jia, and C.-K. Tang, ``Range-sample depth feature for action
  recognition,'' in \emph{Proc. IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2014, pp. 772--779.

\bibitem{escalante2015principal}
H.~J. Escalante, I.~Guyon, V.~Athitsos, P.~Jangyodsuk, and J.~Wan, ``Principal
  motion components for one-shot gesture recognition,'' \emph{Pattern Analysis
  and Applications}, pp. 1--16, 2015.

\bibitem{Vemulapallia2016}
R.~Vemulapalli and R.~Chellappa, ``Rolling rotations for recognizing human
  actions from 3d skeletal data,'' in \emph{Proc. IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR)}, 2016, pp. 1--9.

\bibitem{zhang2016rgb}
J.~Zhang, W.~Li, P.~O. Ogunbona, P.~Wang, and C.~Tang, ``{RGB-D}-based action
  recognition datasets: A survey,'' \emph{Pattern Recognition}, vol.~60, pp.
  86--105, 2016.

\bibitem{pichao2015}
P.~Wang, W.~Li, Z.~Gao, C.~Tang, J.~Zhang, and P.~O. Ogunbona, ``Convnets-based
  action recognition from depth maps through virtual cameras and
  pseudocoloring,'' in \emph{Proc. ACM international conference on Multimedia
  (ACM MM)}, 2015, pp. 1119--1122.

\bibitem{pichaoTHMS}
P.~Wang, W.~Li, Z.~Gao, J.~Zhang, C.~Tang, and P.~Ogunbona, ``Action
  recognition from depth maps using deep convolutional neural networks,''
  \emph{Human-Machine Systems, IEEE Transactions on}, vol.~46, no.~4, pp.
  498--509, 2016.

\bibitem{pichao2016}
P.~Wang, Z.~Li, Y.~Hou, and W.~Li, ``Action recognition based on joint
  trajectory maps using convolutional neural networks,'' in \emph{Proc. ACM
  international conference on Multimedia (ACM MM)}, 2016, pp. 1--5.

\bibitem{pichaoicprwa}
P.~Wang, W.~Li, S.~Liu, Y.~Zhang, Z.~Gao, and P.~Ogunbona, ``Large-scale
  continuous gesture recognition using convolutional neural networks,'' in
  \emph{Proceedings of ICPRW}, 2016.

\bibitem{pichaocsvt2016}
Y.~Hou, Z.~Li, P.~Wang, and W.~Li, ``Skeleton optical spectra based action
  recognition using convolutional neural networks,'' in \emph{Circuits and
  Systems for Video Technology, IEEE Transactions on}, 2016, pp. 1--5.

\bibitem{du2015hierarchical}
Y.~Du, W.~Wang, and L.~Wang, ``Hierarchical recurrent neural network for
  skeleton based action recognition,'' in \emph{Proc. IEEE Conference on
  Computer Vision and Pattern Recognition (CVPR)}, 2015, pp. 1110--1118.

\bibitem{veeriah2015differential}
V.~Veeriah, N.~Zhuang, and G.-J. Qi, ``Differential recurrent neural networks
  for action recognition,'' in \emph{Proc. IEEE International Conference on
  Computer Vision (ICCV)}, 2015, pp. 4041--4049.

\bibitem{zhu2015co}
W.~Zhu, C.~Lan, J.~Xing, W.~Zeng, Y.~Li, L.~Shen, and X.~Xie, ``Co-occurrence
  feature learning for skeleton based action recognition using regularized deep
  {LSTM} networks,'' in \emph{The 30th AAAI Conference on Artificial
  Intelligence (AAAI)}, 2016.

\bibitem{shahroudy2016ntu}
A.~Shahroudy, J.~Liu, T.-T. Ng, and G.~Wang, ``{NTU RGB+ D}: A large scale
  dataset for {3D} human activity analysis,'' in \emph{Proc. IEEE Conference on
  Computer Vision and Pattern Recognition (CVPR)}, 2016.

\bibitem{bilen2016dynamic}
H.~Bilen, B.~Fernando, E.~Gavves, A.~Vedaldi, and S.~Gould, ``Dynamic image
  networks for action recognition,'' in \emph{Proc. IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR)}, 2016.

\bibitem{wanchalearn}
J.~Wan, S.~Z. Li, Y.~Zhao, S.~Zhou, I.~Guyon, and S.~Escalera, ``Chalearn
  looking at people rgb-d isolated and continuous datasets for gesture
  recognition,'' in \emph{Proc. IEEE Computer Society Conference on Computer
  Vision and Pattern Recognition Workshops (CVPRW)}, 2016, pp. 1--9.

\bibitem{li2008}
W.~Li, Z.~Zhang, and Z.~Liu, ``Expandable data-driven graphical modeling of
  human actions based on salient postures,'' \emph{Circuits and Systems for
  Video Technology, IEEE Transactions on}, vol.~18, no.~11, pp. 1499--1510,
  2008.

\bibitem{yue2015beyond}
J.~Yue-Hei~Ng, M.~Hausknecht, S.~Vijayanarasimhan, O.~Vinyals, R.~Monga, and
  G.~Toderici, ``Beyond short snippets: Deep networks for video
  classification,'' in \emph{Proc. IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2015, pp. 4694--4702.

\bibitem{simonyan2014two}
K.~Simonyan and A.~Zisserman, ``Two-stream convolutional networks for action
  recognition in videos,'' in \emph{Proc. Annual Conference on Neural
  Information Processing Systems (NIPS)}, 2014, pp. 568--576.

\bibitem{ji20133d}
S.~Ji, W.~Xu, M.~Yang, and K.~Yu, ``{3D} convolutional neural networks for
  human action recognition,'' \emph{Pattern Analysis and Machine Intelligence,
  IEEE Transactions on}, vol.~35, no.~1, pp. 221--231, 2013.

\bibitem{tran2015learning}
D.~Tran, L.~Bourdev, R.~Fergus, L.~Torresani, and M.~Paluri, ``Learning
  spatiotemporal features with {3D} convolutional networks,'' in \emph{Proc.
  IEEE International Conference on Computer Vision (ICCV)}, 2015, pp.
  4489--4497.

\bibitem{donahue2015long}
J.~Donahue, L.~Anne~Hendricks, S.~Guadarrama, M.~Rohrbach, S.~Venugopalan,
  K.~Saenko, and T.~Darrell, ``Long-term recurrent convolutional networks for
  visual recognition and description,'' in \emph{Proc. IEEE Conference on
  Computer Vision and Pattern Recognition (CVPR)}, 2015, pp. 2625--2634.

\bibitem{smola2004tutorial}
A.~J. Smola and B.~Sch{\"o}lkopf, ``A tutorial on support vector regression,''
  \emph{Statistics and computing}, vol.~14, no.~3, pp. 199--222, 2004.

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' in \emph{Proc. Annual Conference on
  Neural Information Processing Systems (NIPS)}, 2012, pp. 1106--1114.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{jia2014caffe}
Y.~Jia, E.~Shelhamer, J.~Donahue, S.~Karayev, J.~Long, R.~B. Girshick,
  S.~Guadarrama, and T.~Darrell, ``Caffe: Convolutional architecture for fast
  feature embedding.'' in \emph{Proc. ACM international conference on
  Multimedia (ACM MM)}, 2014, pp. 675--678.

\bibitem{ICPRW2016}
H.~J. Escalante, V.~Ponce-Lpez, J.~Wan, M.~A. Riegler, B.~Chen, A.~Claps,
  S.~Escalera, I.~Guyon, X.~Bar, P.~Halvorsen, H.~Mller, and M.~Larson,
  ``Chalearn joint contest on multimedia challenges beyond visual analysis: An
  overview,'' in \emph{Proceedings of ICPRW}, 2016.

\bibitem{guyon2014chalearn}
I.~Guyon, V.~Athitsos, P.~Jangyodsuk, and H.~J. Escalante, ``The chalearn
  gesture dataset ({CGD} 2011),'' \emph{Machine Vision and Applications},
  vol.~25, no.~8, pp. 1929--1951, 2014.

\bibitem{pami16Jun}
J.~Wan, G.~Guo, and S.~Z. Li, ``Explore efficient local features from rgb-d
  data for one-shot learning gesture recognition,'' \emph{IEEE Transactions on
  Pattern Analysis and Machine Intelligence}, vol.~38, no.~8, pp. 1626--1639,
  Aug 2016.

\bibitem{yunanli}
Y.~Li, Q.~Miao, K.~Tian, Y.~Fan, X.~Xu, R.~Li, and J.~Song., ``Large-scale
  gesture recognition with a fusion of rgb-d data based on the c3d model,'' in
  \emph{Proceedings of ICPRW}, 2016.

\bibitem{guangming}
G.~Zhu, L.~Zhang, L.~Mei, J.~Shao, J.~Song, and P.~Shen, ``Large-scale isolated
  gesture recognition using pyramidal 3d convolutional networks,'' in
  \emph{Proceedings of ICPRW}, 2016.

\end{thebibliography}


% that's all folks
\end{document}


