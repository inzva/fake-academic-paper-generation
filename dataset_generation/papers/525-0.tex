% Template for ICASSP-2018 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass[9pt]{article}
\usepackage{spconf,amsmath,graphicx}

\usepackage{spconf,amsmath,amsfonts,amssymb,amsbsy}
\usepackage{amsthm}
\usepackage{graphicx,color}
\usepackage{float}
\usepackage{verbatim}
\usepackage{pslatex}
\usepackage{cite,url,bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{epsf,subfigure,psfig}
\usepackage{latexsym,amsmath,epsfig}
\usepackage{multirow}
%\usepackage{hyperref}
%\usepackage{authblk}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%\renewcommand{\thesubsubsection}{\arabic{subsection}.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Extra definitions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\vect}[1]{\pmb{#1}}
\newcommand{\mat}[1]{\pmb{#1}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}

\newtheorem{prop}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\def\mb{\mathbf}

\def\ben{\begin{equation*}}
\def\een{\end{equation*}}
\def\be{\begin{equation}}
\def\ee{\end{equation}}
\def\beaa{\begin{eqnarray*}}
\def\eeaa{\end{eqnarray*}}
\def\bea{\begin{eqnarray}}
\def\eea{\end{eqnarray}}
\def\tb{\textbf}
\def\tcb{\textcolor{blue}}
\def\tcr{\textcolor{red}}
\def\bleq{\begin{flalign}}
\def\eleq{\end{flalign}}
\DeclareMathOperator{\Tr}{Tr}


%\setcounter{secnumdepth}{4}


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}



% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
%\title{Image Super Resolution with Deep Neural Networks via Natural Image Priors}
\title{Deep Image Super Resolution via Natural Image Priors}
%
% Single address.
% ---------------
\name{Hojjat S. Mousavi, Tiantong Guo, Vishal Monga\thanks{This work is supported by NSF Career Award to V. Monga.}}
\address{\vspace{-0.1in}Dept. of Electrical Engineering, The Pennsylvania State University}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
%
%\twoauthors
%  {Hojjat S. Mousavi, }
%	{Apple Inc. \\ California }
%  {Tiantong Guo, Vishal Monga }
%	{Dept. of Electrical Engineering\\
%	The Pennsylvania State University, PA}


\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
Single image super-resolution (SR) via deep learning has recently gained significant attention in the literature. Convolutional neural networks (CNNs) are typically learned to represent the mapping between low-resolution (LR) and high-resolution (HR) images/patches with the help of training examples. Most existing deep networks for SR produce high quality results when training data is abundant. However, their performance degrades sharply when training is limited. We propose to regularize deep structures with prior knowledge about the images so that they can capture more structural information from the same limited data. In particular, we incorporate in a tractable fashion within the CNN framework, natural image priors which have shown to have much recent success in imaging and vision inverse problems. Experimental results show that the proposed deep network with natural image priors is particularly effective in training starved regimes.
\end{abstract}
%
%\begin{keywords}
%deep learning, super-resolution, natural image priors, regularized networks \tcb{we can remove this save space :*}
%\end{keywords}
%

\section{Introduction}
\label{sec:intro}

A popular branch of image reconstruction methods is image Super-Resolution (SR), which focuses on the enhancement of image resolution. %Traditional SR approaches focus on multi-frame super resolution where the input is multiple Low Resolution (LR) images of the same scene and maps them to a High Resolution (HR) image \cite{Freeman:ExampleBasedSR_CompGraph2002 ,Farsiu:MultiFrameSR_TIP2004, Park:ReviewSR_SPM2003}. This %mapping can be seen as an inverse problem which recovers the high-resolution image by fusing a set of low-resolution images.
Single image SR methods consider generating the HR image only based on a \emph{single} low-resolution image as input. Classically, the solution to this problem is based on example-based methods exploiting nearest neighbor estimations \cite{Freeman:ExampleBasedSR_CompGraph2002, Glasner:SRSingle2009ICCV}. In addition, many machine learning techniques have been developed attempting to capture the co-occurrence of low-resolution (LR) and high-resolution (HR) image patches \cite{Sun:HallucinationSR_CVPR2003, Chang:NeighborEmbeddingSR_CVPR2004}.
Generally, SR task is a severely  ill-posed problem due to information loss and hence the solution is not unique. The use of prior information about the expected HR image has been suggested to yield realistic and robust solutions in traditional SR set-ups \cite{Tappen:SparsePriorSR_2003,Fattal:SRstatistic_ACM2007,Dai:edgeSR_CVPR2007, mousavi2016ColorSR_ICIP}.

Sparsity-based methods have in particular been widely applied to the single image SR problem. Essentially in these techniques, examples of corresponding HR and LR patches are collected as columns of two dictionaries (matrices). A sparse code is obtained for LR patches via its corresponding dictionary but applied to the HR dictionary to yield a HR image patch \cite{YangAndWright:SparseSR_TIP2010}. Other sparsity-based methods include  single image scale-up \cite{Zeyde:SR_Springer2012}, Anchored Neighborhood Regression (ANR) \cite{Timofte:AnchoredANR_ICCV2013, Timofte:AnchoredARN+_ACCV2014} and color SR \cite{mousavi2017ColorSR_TIP}.


Deep learning based SR has been of recent interest and has been shown to improve results over sparsity based methods which were previously considered state of the art for SR. Deep learning promotes the design of large-scale networks \cite{hinton2006fast,bengio2007greedy,poultney2006efficient} for a variety of problems including SR. Invariably, a network, e.g.\a deep convolutional neural network (CNN) or auto-encoder is trained to learn the relationship between LR and HR image patches.
Among the first deep learning based super-resolution methods, Dong \emph{et al.} \cite{dong2014learning} trained a deep convolution neural network (SRCNN) to accomplish the image SR task. %In this work, the training set comprises of example LR inputs and their corresponding HR output images which were fed as training data to the SRCNN network. %However, the process of generating SR results also require a large amount of training time.
Among other such methods we can name coupled auto-encoders \cite{tiantong16deep}, Wavelet SR \cite{guo2017DWSR_CVPR}, cascaded networks  \cite{cui2014deep},  Deep Joint Super Resolution (DJSR)  \cite{wang2015self}, and self-example networks \cite{huang2015single}.
%Combined with sparse coding methods, \cite{tiantong16deep} proposed a coupled network structure utilizing middle layer representations for generating SR results. In different approaches, Cui {\em et al.} \cite{cui2014deep}  proposed a cascade network to gradually upscale LR images after each layer, while \cite{wang2015self} trained a high complexity convolutional auto-encoder called Deep Joint Super Resolution (DJSR) to obtain the SR results. Self-examples of images were explored in \cite{huang2015single} where training sets exploit self-example similarity, which leads to enhanced results. %However, similar to SRCNN, DJSR suffers from the expensive computation in training and processing to generate the SR images.
Recently, residual net \cite{he2016deep} has shown great ability at reducing training time and faster convergence rate. Based on this idea, a Very Deep Super-Resolution (VDSR) \cite{Kim_2016_VDSR} method is proposed which emphasizes on reconstructing the residuals between LR and HR images.

\noindent{\bf Motivation:} The performance improvements in the sense of image quality for SR deep networks have been facilitated by abundant training, which means that thousands or millions of training LR and HR pairs are available. We investigate the performance of existing SR deep structures in low training regime and show that their performance drops significantly.  Note that the assumption of limited availability of training data is very reasonable in many image processing and vision applications, namely the enhancement of resolution of medical images such as MRI and CT. We propose a remedy for this performance degradation by developing a Deep network with Natural Image Priors (DNIP) for the SR task.

The \textbf{main contributions} of this paper are the following: 1)  we propose to incorporate Natural Image Priors (NIP) \cite{kim2010single,tappen2003exploiting} into the learning of a CNN for the SR task,  2.) a regularization term is developed which involves making suitable approximations to the prior and results in a penalty function that is smooth and differentiable and hence usable with backpropogation schemes 3) experimental validation reveals that DNIP outperforms competing methods particularly in the low-training regime even with a simpler network structure viz. smaller number of layers.


%The remainder of this chapter is divided into two parts. After providing a short background on wavelet coefficients and their application in SR, we provide some background on structured as follows: First, we describe the ********************. At the end, we show the prior idea

%-------------------------------------------------------------------------


\vspace{-5mm}
\section{Integrating Natural Image Priors in Deep SR Networks}
\label{sec:img_priors}


%To compensate the performance drop of deep networks in low training scenarios, we utilize image statistics and priors, among which the Natural Image Priors (NIP) can be well-adapted to SR task.
%\subsection{Deep Super Resolution in Low Training Scenario}
%\label{Sec:SR_lowTr}
% As mentioned above, deep networks' performance significantly drops in absence of abundant training data.
% %The abundance of training data in deep learning provides very compelling results in areas such as object recognition and natural language processing as well as low-level vision tasks such as super-resolution and denoising. However, the performance of such networks degrades when the presence of abundant training is not an option and usually they perform very bad in these low training scenarios.
% To compensate for lack of enough training data we propose to use image statistics and priors. Among the most suitable such priors for SR task are the Natural Image Priors (NIP) as described below which we modify to be applicable to SR task. %We aim to apply NIPs on deep networks specialized for the SR task.
%The statistical information embedded in natural images have recently received much attention from different communities for both understanding the human visual system and also designing effective image processing algorithms \cite{field1987relations}.
%Natural images are different from images generated by random noise in that they exhibit meaningful structures. Examples of such structures are local regularities, such as edges and self-similarities, etc. %Consequently, the natural images are only a tiny fraction of the space of all the images that can be generated by all the possibilities of pixel values.
Natural images have many unique statistical properties \cite{zontak2011internal, weiss2007makes}.
One of the most well known such properties is that they exhibit heavy-tailed
distribution when applying derivative filters onto them.
Intuitively, natural images are locally smooth; therefore, local differences
will be small and  the distribution will decrease
faster than the Gaussian.
On the other hand,
natural images have many structural details such as edges, where
the derivative response can be large and it contributes to the
heavier tails than the Gaussian distribution.
This prior knowledge has been successfully applied in a wide range of applications, including image denoising \cite{weiss2007makes}, deblurring \cite{field1987relations} and super-resolution with sparse regression models \cite{kim2010single}.
%Apart from this heavy-tailed statistics characteristic of natural images, they also have many other statistical properties, such as scale invariance and similar joint statistics. The former states that the natural images have similar heavy-tailed distributions at different scales \cite{zontak2011internal} and the latter means that the neighboring pixels in the natural images exhibit high statistical dependency \cite{roth2009fields}.

%Image priors and statistics are a very active research topic and researchers are still investigating it.
The heavy-tail characteristics of  images can be captured using the following probability distribution as prior: $P(\vect y) \propto \exp \Big( -\frac{\|\nabla \vect y \|_\alpha^\alpha }{\eta^2} \Big)$
%\bea
%    P(\vect y) \propto \exp \Big( -\frac{\|\nabla \vect y \|_\alpha }{\eta^2} \Big).
%\eea
where $\nabla \vect y$ represents the gradient of the image $\vect y$ and the norm $\|\vect y\|_\alpha^\alpha$ is defined as $\|\vect y\|_\alpha^\alpha = \sum_i |y_i|^\alpha$. The parameter $\alpha$ determines the family of priors being applied. For example, the most classical image prior is the Gaussian model with $\alpha=2$. It is widely used due to its simplicity; however, it usually fails to produce satisfying solutions as it smooth-out the image.
%The most classical image prior is Gaussian model applied to the derivatives of images, is widely used due to its simplicity:
%\bea
%    P(\vect y) \propto \exp \Big( -\frac{\|\nabla \vect y \|_2^2}{\eta^2} \Big)
%\eea
%where $\nabla \vect y$ represents the gradient of the image $\vect y$.
%Gaussian prior has the advantage of having a closed form solution for many optimization problems;
 To overcome this problem and preserve the edge structure, Laplacian prior is used which has been proved to preserve image discontinuities better.
%\bea
%    P(\vect y) \propto \exp \Big( -\frac{\|\nabla \vect y \|_1^2}{\eta^2} \Big). \label{Eq:NIP_l1}
%\eea
Laplacian priors are related to $\ell_1$-norm regularization with $\alpha=1$ which promote the sparsity in the solution. Such priors can preserve edges in the image; however,  resulting images look piecewise linear. This is due to the fact that natural images follow a distribution with heavier tails than Laplacian or Gaussian. Therefore,  hyper-Laplacian distribution is introduced for the edges \cite{zhang2012generative,krishnan2009fast} with  $\alpha < 1$.
%\bea
%    P(\vect y) \propto \exp \Big( -\frac{\|\nabla \vect y \|_\alpha }{\eta^2} \Big).
%\eea
%where the norm $\|\vect y\|_\alpha$ is defines as $\|\vect y\|_\alpha = \sum_i |y_i|^\alpha$.
%As suggested by the literature \cite{krishnan2009fast,kim2010single}, the parameter $\alpha$ which controls the sparseness of the desired gradient in the natural images is usually picked between 0.5 and 0.8.
%More complicated priors that can take into account the long-range inter-relations of pixels can also be used to further capture statistical data in natural images. These priors can be set manually, or more interestingly they can be learned from training data and applied on unseen images for super-resolution.
In this paper, we take the image priors as suggested by Kim \emph{et al.} \cite{kim2010single,tappen2003exploiting} and improve upon them for SR task.
\bea\resizebox{0.96\columnwidth}{!}{$
        P(\vect {y_h} | \vect {y_l}) = \frac{1}{C} \underbrace{ \prod_{\substack{\{i,j\} \\ \{s,t\}\in \\\mathcal{N}(i,j)} }  \exp \left[ - \left( \frac{|y_h(i,j) - y_h(s,t)|}{\sigma_N} \right)^\alpha \right] }_{\text{prefer strong edges (edge based prior)}}
                                    \underbrace{ \prod_{ \{i,j\}  }  \exp \left[ - \left( \frac{|\mathbb{T} \big( y_h(i,j) \big) - y_l(i,j)|}{\sigma_R} \right)^2 \right] }_{\text{Reconstruction is faithful to LR image}} \label{Eq:NIP_orig}
                                    $}
\eea
The above prior tries to capture natural image characteristics  and  the reconstruction model in one framework. $\vect{y_h}$ represents the estimated high resolution image and $\vect{y_l}$ denotes the corresponding low resolution image and $\mathcal{N}(i,j)$ represents a neighborhood of pixels at location $(i,j)$. For a given image, the second product
term is a reconstruction constraint and ensures that  when the same downsampling kernel $\mathbb{T}$ (blurring and sub-sampling) is applied on the super resolution result ($\vect{y_h}$), it is prevented from flowing far away from the input low resolution image $\vect{y_l}$.
%In this form of NIP framework, the second term is the reconstruction constraint which measures the distance
%between the input low-resolution image and an image reconstructed from the high-resolution configuration according to the down-sampling model (blurring and sub-sampling),
While the first product term (NIP term) tends to penalize pixel value differences in a neighborhood of each pixel $(i,j)$. Subsequently, this distribution prefers a strong edge rather than
a set of small edges (such as ringing artifacts) and can be used to resolve the problem
of smooth edges.

\noindent \textbf{Incorporating NIP in a CNN Framework:} To adapt the NIP prior and the reconstruction constraint to SR in a deep learning-based framework, we propose to revise the prior distribution in \eqref{Eq:NIP_orig}. This is done   to ensure that the reconstruction constraint is penalizing the difference between the estimated HR image ($\vect y_h$) and the ground truth HR image ($\vect y_g$). %This is a better fit for learning-based methods in super-resolution where the cost function is the difference between the inferred image and the ground truth.
We then rewrite the NIP as follows:
\bea \resizebox{\columnwidth}{!}{$
        P(\vect {y_h} | \vect {y_g}) = \frac{1}{C} \displaystyle\prod_{\substack{\{i,j\} \\ \{s,t\}\in \\\mathcal{N}(i,j)} }  \exp \left[ - \left( \frac{|y_h(i,j) - y_h(s,t)|}{\sigma_N} \right)^\alpha \right]
                                    \underbrace{ \prod_{ \{i,j\}  }  \exp \left[ - \left( \frac{| y_h(i,j)  - y_g(i,j)|}{\sigma_R} \right)^2 \right] }_{\text{Compares output with ground truth HR image}}
                                    $}
                                    \label{Eq:NIP_prior}
\eea


%***************-modification without T kernel to adapt to Deep framework************
Also, note that in the revised NIP prior no downsampling/blurring kernel ($\mathbb{T}$) is used. In our version of NIP prior, which is specific to deep SR, we want the inferred super-resolution result to be statistically close to the ground truth image. % The above  formulation is very similar to \eqref{Eq:NIP_orig} where the low resolution image $\vect{y_l}$ is now replaced with the ground truth high resolution image ($\vect{y_g}$). It essentially encourages the inferred high-resolution image to be close to the ground truth using the NIP probabilistic prior.
The NIP prior provides a MAP framework where we can take the negative log-likelihood of the posterior and find its minimum. Essentially, maximizing the posterior using NIP priors leads to the following minimization problem:
%\bea
%\vect{y_h} =     &\arg\min\limits_{\vect{y_h}}   & \sum\limits_{\substack{\{i,j\} \\ \{s,t\}\in \mathcal{N}(i,j)} }  \left( \frac{|y_h(i,j) - y_h(s,t)|}{\sigma_N} \right) ^\alpha +
%                                    \sum\limits_{ \{i,j\}  }  \left( \frac{| y_h(i,j)  - y_g(i,j)|}{\sigma_R} \right)^2 \label{Eq:NIP_cost1}\\
%            = &   \arg\min\limits_{\vect{y_h}}  &  \frac{\sigma_R^2}{\sigma_N^\alpha}  \sum\limits_{\substack{\{i,j\} \\ \{s,t\}\in \mathcal{N}(i,j)} }   |y_h(i,j) - y_h(s,t)|^\alpha +
%                                   \sum\limits_{ \{i,j\}  }  | y_h(i,j)  - y_g(i,j)|^2
%                                   \label{Eq:NIP_cost2}
%\eea
\bea \resizebox{\columnwidth}{!}{$
\vect{y_h}  =   \arg\min\limits_{\vect{y_h}}   \frac{\sigma_R^2}{\sigma_N^\alpha}  \sum\limits_{\substack{\{i,j\} \\ \{s,t\}\in \mathcal{N}(i,j)} }   |y_h(i,j) - y_h(s,t)|^\alpha +
                                   \sum\limits_{ \{i,j\}  }  | y_h(i,j)  - y_g(i,j)|^2
                                   $}
                                   \label{Eq:NIP_cost2}
\eea
Rewriting the MAP estimation in the form above helps us interpret the cost function often used for image super-resolution and also implement the new NIP cost function in an efficient manner using convolutions. The second sum in \eqref{Eq:NIP_cost2} is essentially summing up pixel level square differences between the estimated HR image and the HR ground truth image. This can be easily captured by $\| \mat{y_h}  - \mat{y_g} \|_F^2$. It is noteworthy to mention that this is the most commonly used cost function for image SR in the deep learning frameworks %which essentially back-propagate the gradient of error terms to the weights of the deep network.
On the other hand, the first term in \eqref{Eq:NIP_cost2} is a \emph{local} error constraint on pixel values and summed for all the pixels in the images. If we assume a simple neighbourhood $\mathcal{N}(i,j)$ to be the 8-neighbourhood vicinity around any pixel $(i,j)$, the NIP prior as defined above can be written as summation over $8$ filtered images  that are also passed through a special non-linear activation function. Since the aforementioned filters ($\mat F_k$) are simple difference filters and linear, they can be implemented with eight convolution filters (shown in Fig. \ref{Fig:Filters}) and followed by a non-linear activation function i.e. $|\cdot|^\alpha$. This is more efficient for implementation purposes in the deep leaning structures using CNNs. %Despite the regular NIP assumption that $\alpha$ is fixed, it is assumed here to be learnable so we can find the best $\alpha$ that fits the training images we have.
The overall cost function can be rewritten as:
\bea
    \vect y_h = &\arg\min\limits_{\vect{y_h}}   &  \frac{\sigma_R^2}{\sigma_N^\alpha}  \left(\sum\limits_{k=1}^{8}   \|\mat{y_h} \ast \mat{F_k}  \|_\alpha^\alpha \right) +
                                    \| \mat{y_h}  - \mat{y_g} \|_F^2 \label{Eq:NIP_cost3}
\eea
%It is noteworthy to mention  that this new cost function can be implemented using convolutions followed by a simple non-linearity layer. This makes it efficient for implementation purposes in the deep leaning structures using convolutional neural networks.
\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.7\columnwidth]{figs/8Filters.png}\vspace{-0.15in}
  \caption{ 8 convolution filters used to implement the NIP loss}
  \label{Fig:Filters}
\end{figure}
To optimize a deep network using the cost function in \eqref{Eq:NIP_cost3}, we need to make sure the cost function is differentiable so the error can propagate back through the network using a back-propagation approach. However, for $\alpha <1$, the cost function in \eqref{Eq:NIP_cost3} is not differentiable at zero since it has an infinite slope. %Therefore, in the optimization procedure it produces infinitely large gradients which makes the network unstable (see figure \ref{Fig:NIP} on the right). One way to alleviate this problem is to fix the parameter $\alpha$ to be exactly equal to one which is exactly a relaxation equivalent to \eqref{Eq:NIP_l1} (see figure \ref{Fig:NIP} on the left for $\alpha=1$).
To mitigate this problem, we propose to approximate the $\alpha$-norm function with something that has a large but finite derivative at zero. In particular,  we approximate $|x|_\alpha$ for $\alpha=0.1$ with $0.1 \log\Big((e^{10}-1)|x|+1\Big)$. %(See figure \ref{Fig:NIP} in the middle).
%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=0.99\columnwidth]{figs/priors.png}
%  \caption{ Image priors introduced in this section. Illustrations are for one dimension only. From left to right: $\|x\|_1$,  $10 \log\Big((e^{10}-1)|x|+1\Big)$, $\|x\|_{0.1}$}
%  \label{Fig:NIP}
%\end{figure}





\textbf{Network Structure:} The proposed network structure is shown in Fig. \ref{fig:NIP_network}, which consists of an SR network for generating the super-resolution result and also a few additional convolutional layers to impose the NIP prior.
The ``SR Network" in Fig. \ref{fig:NIP_network} can be chosen to be any network specific for SR task and here we pick variants of the Very Deep Super Resolution (VDSR) \cite{Kim_2016_VDSR} as one of the state-of-the-art methods for validating our idea. However, this idea can be applied to any other SR network such as SRCNN, etc. VDSR  is a residual network with $N$ convolutional layers that takes the input LR image as input and generates the output residuals that needed to be added to the input image in order to generate the HR output image. Each layer has $64$ filters of size $3\times 3\times c$, where $c$ is the number of required channels.


\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=\columnwidth]{figs/NIP_conv1.png}\vspace{-0.15in}
  \caption{The network structure for imposing NIP priors}
  \label{fig:NIP_network}
\end{figure}
%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%    \includegraphics[width=0.9\columnwidth]{figs/VDSR.png}
%    \caption{VDSR network for super resolution}
%    \label{fig:VDSR}
%\end{figure}

The output of the ``SR Network" goes into another layer of convolution with 8 non-learnable filters of size $3\times 3$ that are illustrated in Fig. \ref{Fig:Filters} and passed through the nonlinear equivalent of $\alpha$-norm to form a data cube with 8 channels. The output cube is summed across channels and then across spatial dimensions to provide the NIP part of the loss function in \eqref{Eq:NIP_cost3}. %Note that the parameter $\alpha$ here can be learnable.
Although the filters in the last layer are not learnable and are fixed, the error that is caused by NIP layers propagates back to the main SR network and hence the SR network weights are optimized with the knowledge of the NIP.
%\tcb{ The cost function to be minimized here is as   proposed in \eqref{Eq:NIP_cost3} which is approximated using the log-function so that it is differentiable. The back-propagation rules needed to optimize the network stay the same for the reconstruction error term (second term in \eqref{Eq:NIP_cost3}) but for the last layer which corresponds only to computing the loss for NIP priors can be summarized as follows where we need $\frac{\partial E}{\partial W_{ab}^{\ell_p}}$ for every layer $\ell_p=2,..,L_p-1$. Using chain rule we can derive:
%}
%\tcb{
%\bea
%    \frac{\partial E }{\partial W_{a,b,c}^{\ell_p}} = \sum_{k=1}^{3}  \sum_{i=0}^{n-1}  \sum_{j=0}^{n-1} \delta_{i,j,k}^{\ell_p+1} ~.~ A_{i+a,j+b,k+c}^{\ell_p}
%    \label{Eq:dEdW}\\
%    \delta_{i,j,k}^{\ell_p} = \sum_{s=1}^{3} \; \sum_{u=0}^{n-1} \; \sum_{v=0}^{n-1} \delta_{i-u,j-v,k-s}^{\ell_p+1} ~.~ W_{u,v,s}^{\ell_p}
%    \label{Eq:modified_delta}
%\eea
%And for the last layer $\ell_p=L_p$:
%\bea
%    \delta_{i,j}^{L_p} = \frac{\partial E }{\partial Z_{i,j}^{L_p}} \stackrel{\mat Z^{L_p}=\mat A^{L_p}}{=\joinrel=\joinrel=\joinrel=} \frac{\partial E }{\partial A_{i,j,k}^{L_p}}
%        = 1
%\eea
%where $\mat W^{\ell_p}$ are the weights associated with layer $\ell_p$ of convolutions, $\mat A^{\ell_p}$ are the activation outputs of layer $\ell_p$, $\mat Z^{\ell_p}$ are the inputs to the layer $\ell_p$ and $\delta_{\ell_p+1}$ is the error propagated back from layer $\ell_{p+1}$ to layer $\ell_p$. For the last layer, Activation maps $\mat A^{L_p}$ and inputs $\mat Z^{L_p}$ are the same, i.e. $\mat Z^{L_p}=\mat A^{L_p}$.
%}
This network is equipped with image priors so that it can capture image statistics from the training data and generate output images with respect to the natural image prior. Especially, in scenarios where training data is limited and generic deep SR networks fail to provide satisfying results for super-resolution, our DNIP is capable of recovering HR images using the prior knowledge that is incorporated into the network. %In the following sections we provide different types of prior that can come in handy in low training scenarios, and then discuss the effects of such priors in practice.



%\subsection{Other Image Priors for Image Super Resolution}
%\label{Sec:other_priors}
%-color constraints
%
%- gradient constraints
%
%for each
%
%-show the network structure shape
%
%- derive the new cost function
%
%-derive the back propagation


\section{Experimental Results}
In this section, we provide the experimental results and procedures corresponding to our DNIP method. We evaluate our method in high training and low training scenarios to show the benefits of regularizing deep networks with image priors.

\subsection{Dataset Preparation and Training Procedure }
For training dataset, we use the 291 images from \cite{schulter2015fast} which contains natural images. Data Augmentation, including flipping, rotation, and scaling, was performed. To train the network, the HR training images are scaled down and up by bicubic interpolation with a scaling factor of $3$ from which $140,000+$ patches of size $41\times 41$ pixels are extracted. For the test scenario, we use the `set 14' \cite{Zeyde:SR_Springer2012} dataset for a scaling factor of $3$. The training procedure of DNIP is chosen to be similar to VDSR \cite{Kim_2016_VDSR}. Additional convolutional layers with non-learnable (fixed) weights are also added to compute the loss function corresponding to NIP. Training uses batches of size 64 and momentum and weight decay parameters are set to $0.9$ and $0.0001$. Also, gradient clipping is used as proposed by \cite{Kim_2016_VDSR} to prevent gradients from exploding. We train all experiments over 300 epochs over all training data (no matter how much training data is used). The learning rate was initially set to $0.1$ and then decreased by a factor of 10 at epochs 60 and 140. Similar to other recent SR methods, our framework applies bicubic interpolation to color components and only the luminance channel is fed to the deep network.

\subsection{DNIP with Variable Training and Variable Depth }
We compare the performance of DNIP and VDSR with $N=20, 12, 5$ layers and with varying amount of training data. We show that when generous training is available (using $100\%$ of available training) both methods show comparable performance. Fig. \ref{Fig:HighTr_lenna} shows the the output of VDSR-20 and DNIP-20 (both with $N=20$ layers) when using $100\%$ of training patches. Numbers in parenthesis denote the PSNR and SSIM values, respectively. Also, the last two columns of Table \ref{Table:All_depth_all_percentages} shows the same but averaged over Set-14. Additionally, Table \ref{Table:All_depth_all_percentages} also reports results with different number of layers in the network varying from $5$ to $20$ with varying amount of training.  It may be inferred that despite different networks' depths, there is no significant difference between the two networks when using $100\%$ of training data. However, when we decrease the amount of training data, the benefits of DNIP are readily apparent.
\begin{table*}[t!]
\caption{ {Quantitative Results average over Set 14 for variable amount of training and network depth ($N$)} } % title of Table
\centering
%\tiny
%\vspace{-2mm}
\resizebox{1.5\columnwidth}{!}{
\begin{tabular}{|c||c c|c c|c c|c c| } % centered columns (4 columns)
\hline %inserts double horizontal lines
Training $\%$     &    \multicolumn{2}{c|}{1$\%$ }    &    \multicolumn{2}{c|}{4$\%$ }    &    \multicolumn{2}{c|}{20$\%$ }&    \multicolumn{2}{c|}{100$\%$} \\ \hline
Net type        &    PSNR        &    SSIM        &    PSNR        &    SSIM        &    PSNR        &    SSIM        &    PSNR        &    SSIM    \\ \hline\hline
VDSR-20            &    27.0168        &    0.7723        &    28.4169        &    0.8042        &    29.1632        &    0.8193        &    29.7396        &    0.8301  \\
DNIP-20            &    27.1448        &    0.7477        &    28.4622        &    0.8048        &    29.1665        &    0.8191        &    29.7342        &    0.8264  \\
\hline\hline
VDSR-12         &    27.5787        &    0.7881        &    28.4482        &    0.8032        &    29.164        &    0.8191        &    29.6691        &    0.8284  \\
DNIP-12         &    27.6454        &    0.7873        &    28.4828        &    0.8038        &    29.1757        &    0.8189        &    29.6741        &    0.8282  \\
\hline\hline
VDSR-5          &    27.6952        &    0.7917        &    28.0328        &    0.7947        &    28.8706        &    0.8118        &    29.4195        &    0.8233  \\
DNIP-5          &    27.7766        &    0.7919        &    28.1107        &    0.7945        &    28.9363        &    0.8107        &    29.4053        &    0.8222  \\
\hline
%inserts single line
\end{tabular}\vspace{-0.25in}
}
\label{Table:All_depth_all_percentages} % is used to refer this table in the text
\normalsize
\end{table*}
\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \vspace{-0.1in}\includegraphics[width=\columnwidth]{figs/lenna-prior_FullTr.png}\vspace{-0.15in}
  \caption{VDSR-20 and DNIP-20 with abundant training.}\vspace{-0.1in}
  \label{Fig:HighTr_lenna}
\end{figure}%\vspace{-0.07in}
\begin{figure}[t!]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=\columnwidth]{figs/ppt3_LowTr-priorVSno_prior.png}\\
  \includegraphics[width=\columnwidth]{figs/monarch_LowTr-priorVSno_prior.png}\vspace{-0.15in}
  \caption{DNIP with incorporated priors boost the SR results.}\vspace{-0.17in}
  \label{Fig:VDSR_DNIP_20_4percent}
\end{figure}%\vspace{-0.1in}
\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\columnwidth]{figs/lineGraph_2.pdf}\vspace{-0.15in}
  \caption{PSNR values with different amount ($\%$) of training data for DNIP and VDSR with various number of layers.}\vspace{-0.15in}
  \label{Fig:PSNR_plot_all_layers_all_percentages}
\end{figure}


%For instance, Fig. \ref{Fig:VDSR_HighTrVsLowTr} shows how the performance of VDSR-20 drops when trained with only $4\%$ of training patches. It is apparent that both visually and in terms of PSNR and SSIM \cite{Wang:SSIM_TIP2004} values, performance of VDSR significantly drops.
%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=\columnwidth]{figs/HighTRvsLowTR.png}
%  \caption{VDSR-20 when $100\%$ and $4\%$ of data is used.}
%  \label{Fig:VDSR_HighTrVsLowTr}
%\end{figure}

Fig. \ref{Fig:VDSR_DNIP_20_4percent} shows the comparison of VDSR-20 and DNIP-20 networks when trained only with $4\%$ of training patches. Clearly, DNIP-20 shows a significant boost in terms of PSNR and SSIM for both ``ppt3" and ``monarch" images. Also, much less artifacts around edges are seen in the result of DNIP which now uses the prior knowledge compared to VDSR network which does not do so.



We also carried out another experiment which examines the effect of the number of layers on the performance of DNIP and exploiting prior knowledge. We show that in the limited training scenarios, regardless of the number of layers in each network, regularizing our DNIP network with prior knowledge results in better performance than VDSR. Essentially, when less training data is available the more helpful are the incorporated priors in our DNIP network. For instance, Fig. \ref{Fig:PSNR_plot_all_layers_all_percentages}  shows when the available training data is decreasing, the performance of both VDSR-5 and DNIP-5 \tcr{(red square markers)} decays; however, DNIP exhibits a more graceful decay.
%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=\columnwidth]{figs/Table-5layers-PSNR_v1.png}
%      \caption{PSNR values with different amount of training data and for DNIP-20 and VDSR-20.}
%      \label{Fig:PSNR_plot_5layers}
%\end{figure}


%Table \ref{Table:All_depth_all_percentages} summarizes the effect of prior knowledge and regularizing the deep networks in terms of PSNR and SSIM. It shows the performance of DNIP and VDSR for a various number of layers ($N=5,12,20$) and also under different learning scenarios where networks are trained with $1\%, 5\%, 20\%$ and $100\%$ of training data. Also, Fig. \ref{Fig:PSNR_plot_all_layers_all_percentages} shows the same PSNR values, emphasizing the role of priors in low training scenarios. It again shows as the available training data is decreasing, the performance of both VDSR and DNIP with any number of layers decays; however, DNIP suffers from a more graceful decay in each scenario.

An interesting observation here is that as the amount of training data decreases, networks with fewer number of layers begin to show better performance compared to deeper structures. This shows for limited data the shallower networks are more capable of capturing statistical and geometrical structures in the training process. This observation is aligned with the fact that deeper networks need more training data for learning and shallower networks with less parameters can be learned with less training data. However, both cases can benefit from incorporating prior knowledge.





\section{Conclusion}

In this paper, we analyze deep network structures for the SR task in the absence of abundant training data and showed that their performance significantly drops under such conditions. To overcome this problem, we develop a novel deep network that is regularized with prior knowledge of images (natural image priors). We propose suitable approximations to the prior so that it results in a regularization term that fits with existing backpropogation schemes and hence enables tractable learning of a deep CNN with image priors (DNIP). Experiments confirm that our proposed DNIP is capable of capturing more structural image details from limited training data. More elaborate priors may be investigated in future work and from an experimental viewpoint the impact of incorporating priors on the complexity of the network structure (e.g. number of layers) may be investigated in greater detail.


% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
%\begin{figure}[htb]
%
%\begin{minipage}[b]{1.0\linewidth}
%  \centering
%  \centerline{\includegraphics[width=8.5cm]{image1}}
%%  \vspace{2.0cm}
%  \centerline{(a) Result 1}\medskip
%\end{minipage}
%%
%\begin{minipage}[b]{.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image3}}
%%  \vspace{1.5cm}
%  \centerline{(b) Results 3}\medskip
%\end{minipage}
%\hfill
%\begin{minipage}[b]{0.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image4}}
%%  \vspace{1.5cm}
%  \centerline{(c) Result 4}\medskip
%\end{minipage}
%%
%\caption{Example of placing a figure with experimental results.}
%\label{fig:res}
%%
%\end{figure}


% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak




\vfill\pagebreak

\small

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,Biblio-Database}

\end{document}
