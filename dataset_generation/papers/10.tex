\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfig}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\wacvfinalcopy % *** Uncomment this line for the final submission

\def\wacvPaperID{****} % *** Enter the wacv Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifwacvfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Efficient Action Detection in Untrimmed Videos via Multi-Task Learning}

% Authors at the same institution
%\author{First Author \hspace{2cm} Second Author \\
%Institution1\\
%{\tt\small firstauthor@i1.org}
%}
% Authors at different institutions
\author{Yi Zhu and Shawn Newsam\\
University of California, Merced\\
{\tt\small \{yzhu25,snewsam\}@ucmerced.edu}
}

\maketitle
\ifwacvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
	This paper studies the joint learning of action recognition and temporal localization in long, untrimmed videos. We employ a \textit{ multi-task learning framework} that performs the three highly related steps of action proposal, action recognition, and action localization refinement in parallel instead of the standard sequential pipeline that performs the steps in order. 
	%Performing the steps in parallel significantly (SDN: is this word too strong?) improves the action detection performance due to better network regularization. It also results in greatly reduced training time. 
	% We carefully address the data imbalance problem that results when simultaneously training modules with different training data requirements. 
	We develop a novel \textit{temporal actionness} regression module that estimates what proportion of a clip contains action. We use it for temporal localization but it could have other applications like video retrieval, surveillance, summarization, etc. 
	We also introduce random shear augmentation during training to simulate viewpoint change.
	%which is important for understanding actions. 
	We evaluate our framework on three popular video benchmarks. Results demonstrate that our joint model is efficient in terms of storage and computation in that we do not need to compute and cache dense trajectory features, and that it is several times faster than its sequential ConvNets counterpart. Yet, despite being more efficient, it outperforms state-of-the-art methods with respect to accuracy.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
The enormous explosion of user-generated videos on the Internet represents a wealth of information. It is essential to understand and analyze these videos for tasks like video recommendation and search, video highlighting, video surveillance, human-robot interaction, human skill evaluation, etc. Automatically localizing and recognizing human actions or events in long, untrimmed videos can save tremendous manual effort and cost.
Much progress has been made on action detection in manually trimmed, short videos over the past several years \cite{yizhu_depth2action_eccvw_2016,15000object2015,Objects2action_Jain_iccv15,actionLocal_context_Soomro_iccv15,actionLocal_track_Weinzaepfel_iccv15,action_tubes_cvpr15_Gkioxari} by employing hand-crafted local features, such as improved dense trajectories (IDT) \cite{idtfWang2013}, or video representations that are learned directly from the data itself using deep convolutional neural networks (ConvNets). However, these approaches are much less effective when applied to long, untrimmed, realistic videos due to the amount of background clutter (non-action clips) as well as the large variation in scene context, motion speed, action style, and camera viewpoint. 

\begin{figure*}[t]
	\centering
	\includegraphics[width=1.0\linewidth,height=5.0in,trim=0 0 0 0,clip]{network_v4.pdf}
	\vspace{-46ex}
	\caption{Overview of our multi-task learning framework for action detection. The 3D ConvNet is initialized from \cite{c3d2015}. The three branches of our model simultaneously perform complementary tasks: \textit{Proposal} to determine if a video clip is an action or background, \textit{Classification} of a clip into categories, and \textit{Regression} of what proportion of a clip contains action. The two green dashed lines indicate the multiple intermediate losses used in the action classification branch. The individual outputs are combined to produce refined prediction scores which are used in post-processing to obtain the final detection results.}
	\label{fig:modelConfigure}
	\vspace{-1ex}
\end{figure*}

Presently, the dominant approach to action detection \cite{fast_temporal_action_proposal_fabian_cvpr16,tubelets_cvpr14_jain,yu_FAP_cvpr15,scnn_shou_wang_chang_cvpr16} is a sequential pipeline usually consisting of three steps, either using IDT features plus shallow classifiers or through deep neural networks. First, a video is processed in a temporal sliding window manner to generate action proposals. The candidate proposals are then used to train a more sophisticated action classifier for recognition. Finally, the temporal boundaries of action instances are estimated through post-processing by applying duration priors or non-maximum suppression (NMS). 
While this pipeline has achieved state-of-the-art performance in recent years \cite{learThumos2014,motion_appearance_thumos_wang14,florence_thumos2014,adsc_thumos_15}, we argue that \textit{learning and applying the closely related steps independently has several drawbacks}. First, the performance is upper-bounded by the proposal stage. Second, sequential training prevents the modules from sharing information and restricts the training data each stage sees. Together, these limit the generalizability of the model especially when there is large intra-class variation. Finally, training the stages sequentially is computationally expensive especially for large-scale video analysis.

Inspired by multi-task learning, such as the work on facial landmark detection in \cite{multi_task_face_zhang_eccv14}, we develop a joint learning framework based on 3D ConvNets \cite{c3d2015}. Our goal is to answer the following three questions simultaneously: (1) \textit{Does a video clip contain an action of interest?} (2) \textit{What action class does it belong to?} and (3) \textit{How much temporal actionness does it have?} 
These three problems are closely related and so a joint learning scheme should have better regularization. This will likely improve the network's capacity to discriminate.
%and make it converge faster compared to its sequential ConvNets counterpart \cite{scnn_shou_wang_chang_cvpr16}.

Our multi-task learning framework for action detection is outlined in Figure \ref{fig:modelConfigure}. 
%Bearing efficiency in mind, we design the network with shared low-level representations and branch out after the last fully connected layer of the 3D ConvNet. The later the branching point is in a multi-task network, the fewer the number of parameters and the smaller the memory footprint of the model \cite{cross_residual_multitask_2016}. 
The top ``Proposal'' branch is a binary classifier that determines whether a video clip is an action of interest or just background. 
The middle ``Classification'' branch is a conventional action recognition module that classifies a video clip into different action categories. 
This branch combines multiple intermediate losses calculated from last convolutional layer (\textsf{conv5}) as well as the first (\textsf{fc6}) and last (\textsf{fc8}) fully connected layers in order to improve regularization.
The bottom ``Regression'' branch predicts what proportion of a video clip contains the action of interest temporally. This temporal actionness is related to but different from the spatial actionness introduced in \cite{actionness_cvpr14_chen}. That quantity is purely spatial and refers instead to the likelihood that a specific image region contains a generic action instance. 
%All losses are added together with cross-validated weights.
%with weights and back-propagated through the network during training.

We show that our joint learning scheme can perform action detection using a single 3D ConvNet. 
%without needing to compute dense trajectory features. 
In addition, we introduce a data augmentation technique based on shear transformation to increase view-point invariance. 
%This augmentation is shown to be particularly effective for action recognition in untrimmed videos. 
We also include multiple intermediate losses during training to help prevent overfitting. This strategy is shown to be particularly effective for action recognition in untrimmed videos. Our multi-task network is significantly more efficient, requiring only a fraction of the time of a cascaded ConvNets approach, yet achieves better performance on three popular benchmarks, ranging from THUMOS 2014 (complex) \cite{THUMOS14} to MSR Actions II (simple) \cite{MSR_action_II_PAMI11} and MPII cooking (fine-grained) \cite{MPII_cooking_cvpr12}.

\section{Related Work}
\label{sec:related}
There exists an extensive body of literature on video analysis and human action detection. Here we review only the most related work.

\noindent \textbf{Action Localization:} This field has been studied from two directions: spatio-temporal action localization and temporal action localization. 

Spatio-temporal action localization is an active research area and aims to localize an action simultaneously in space and time. There is a sizable body of work on this problem \cite{DAP3D2016,15000object2015,Objects2action_Jain_iccv15,actionLocal_context_Soomro_iccv15,actionLocal_track_Weinzaepfel_iccv15,action_tubes_cvpr15_Gkioxari,joint_lan_iccv11}; however, most of this work focuses on short, trimmed videos. In addition, spatio-temporal localization requires substantial effort to manually generate the object bounding boxes needed for training. This is not feasible for large-scale video analysis such as unconstrained Internet videos.

In contrast, our focus is on temporal action localization. Given an untrimmed video, we want to determine the temporal bounds of one or more action instances. Many real world applications only need the start and end times of the action/event in a video stream such as in surveillance. Further, temporal action localization can serve as a relatively fast pre-processing step that allows more expensive spatio-temporal localization methods to be applied to just those time periods in which action is detected. 
%Much less research has focused on temporal action localization \cite{yu_FAP_cvpr15,scnn_shou_wang_chang_cvpr16,action_detection_language_richard_cvpr16,AP_atoms_Gaidon_PAMI13,granuality_Ni_cvpr14,grammer_Prirsiavash_cvpr14,DPM_actionDetection_Tian_CVPR13}. 
There has been previous work on temporal action localization.
Yeung et al. \cite{frame_glimpse_yeung_cvpr16} employ a recurrent neural network (RNN)-based agent and use REINFORCE \cite{REINFORCE_1992} to learn the agent's decision policy. Their fully end-to-end approach achieves promising results on two large-scale benchmarks while only needing to observe a fraction of the video frames.
Shou et al. \cite{scnn_shou_wang_chang_cvpr16} adapt 3D ConvNets \cite{c3d2015} for action recognition and design a new multi-stage framework. 
They explicitly take temporal overlap into consideration which benefits the final post-processing step to obtain state-of-the-art results on THUMOS 2014.
%\cite{THUMOS14}.
Sun et al. \cite{FGA_web_sun_mm15} use noisy tagged web images to discover localized action frames in videos and model temporal information with long short-term memory (LSTM) networks \cite{LSTM_1997}.
Very recently, Heilbron et al. \cite{fast_temporal_action_proposal_fabian_cvpr16} introduced a sparse learning framework to represent and retrieve activity proposals efficiently. 
While our approach shares some structural similarity with these works, it is distinct in that it exploits regularities between closely related tasks for action detection in a multi-task learning framework.

\noindent \textbf{Multi-task Learning:} Multi-task deep networks have become a popular approach to solve multiple semantically related tasks in computer vision. In general, they can be formulated as a fan-out model, where the earlier layers share a single representation and the later layers diverge. 
%In \cite{multi_task_face_zhang_eccv14, 44},  
%And in \cite{29}, 
Multi-task learning has shown promising results in various domains, such as action parsing \cite{DAP3D2016}, semantic segmentation \cite{segment_multitask_cvpr16_dai}, natural language processing \cite{seq2seq_multitask_iclr16_luong}, object detection \cite{object_multitask_iccv15_ghifary}, etc.

Among these works, the DAP3D-Net in \cite{DAP3D2016} is most closely related to our work. They use a general fan-out model to automatically parse actions in videos and  output action types, action attributes, and action locations, simultaneously. However, our work is different from theirs in several key ways: (a) \cite{DAP3D2016} performs spatio-temporal action localization, while our goal is temporal action localization; and (b) they train on well-aligned synthetic action data. In contrast, we utilize widely adopted, untrimmed, realistic video benchmarks.
We take inspiration from \cite{scnn_shou_wang_chang_cvpr16} in designing our multi-task learning framework. But turning a cascaded pipeline into a single network is not easy. In particular, care must be taken to address the data imbalance problem when training a parallel network. 
%We describe our solution to this problem below. 
We also introduce a training data augmentation technique based on shear transformation, design a multiple intermediate loss fusion strategy, and develop a temporal actionness regression module. These contributions highlight the novelty of our joint learning framework compared to previous work on action detection in large-scale untrimmed videos.

\section{Overview}
\label{sec:overview}
\textit{Our goal is to develop a model that can determine the temporal bounds of one or more action instances in a long, untrimmed video.} We particularly want the model to be efficient during training and testing (application) in terms of both computation and storage.

\noindent \textbf{Problem Formulation:} 
For an untrimmed video dataset $\mathbb{V}$, each video $V$ may contain one or multiple action instances $\mathbb{A}$. 
Here, $V$ consists of a set of individual frames $v_{t}$, where $t$ indicates the time index of the frame. 
%$\mathbb{A}$ is represented as a set of tuples $\{(v_{m}^{s}, v_{m}^{e}, l_{m} )\}_{m=1}^{M}$ to describe temporal action annotations. $\{(v_{m}^{s}, v_{m}^{e}, l_{m} )\}$ is the start frame, end frame, and action label of the m-th instance, and $M$ is the total number of action instances in $V$.
During training, we generate fixed-length video clips from the annotated videos via temporal sliding windows. These clips are the inputs to our multi-task learning framework. 
The network is trained using five losses: a softmax loss of action/non-action, three softmax losses of action categorization, and a Euclidean squared loss of temporal actionness. 
During prediction, a video is decomposed into clips which are fed to the trained model to obtain per-clip action categorization scores from the last fully connected layer.  These scores are weighted by the output of the other two network branches: the proposal confidence score and the estimated temporal actionness. Finally, non-maximum suppression is applied using the refined action categorization scores, and the top-$k$ results are selected as the temporally localized actions of interest.

\noindent \textbf{Motivation:} 
There are several reasons to develop a multi-task learning framework. First, most existing work does not fully exploit the usefulness of background videos to help train a more robust classifier. Most background videos are discarded after the action proposal stage in order to produce a cleaner and more balanced training data set. Further, as shown in \cite{non_action_wang_cvpr16}, the overall action recognition/detection performance depends on the performance of the action proposal module which is never perfect and will incorrectly label some true actions as background. 
In addition,  the prediction scores of a video clip from the classification module can be high even if it only has a few action frames and the rest are background. This can be problematic because subsequent post-processing steps, such as non-maximum suppression, might remove clips with overall lower action categorization scores but which temporally have a large proportion of action \cite{scnn_shou_wang_chang_cvpr16}. 
%We want to push up the prediction scores of the clip with larger portion being actions and decrease the scores of the clip with less portion being action.
We therefore develop a temporal actionness module to quantitatively measure the proportion of action in a video clip. 
In sum, closely related tasks can be integrated in a multi-task learning framework without a reduction in accuracy but with much faster speed during training and evaluation \cite{DAP3D2016,segment_multitask_cvpr16_dai,seq2seq_multitask_iclr16_luong,object_multitask_iccv15_ghifary}. 
Our three main objectives are \textit{efficiency}, \textit{accuracy} and \textit{robustness}.
%very closely related and so integrating them provides improved accuracy along with significant gains in efficiency.

\section{Methodology}
\label{sec:methodology}
In this section, we first describe our training data augmentation techniques. We then describe our network architecture and the new regression module to predict temporal actionness. Finally, we provide details on post-processing using the refined action categorization scores.

\subsection{Training Data Augmentation}
Most video benchmarks for action detection are extremely small compared to the large-scale image datasets like ImageNet \cite{imagenet_cvpr09}. For example, the largest video dataset with temporal annotations to-date, ActivityNet \cite{activityNet}, only contains 15,410 training action instances. This makes overfitting a challenge in deep learning. Data augmentation techniques, such as random flipping \cite{imagenet_deep_learning_nips12_alex}, corner cropping \cite{wanggoodpractice2015}, multi-scale spatial cropping \cite{sppnet,twostream2014}, and multi-scale temporal sliding windows \cite{scnn_shou_wang_chang_cvpr16}, have therefore been devised to improve the training of deep ConvNets.

In this paper, we utilize three standard data augmentation techniques and introduce a new one for action detection.
%multi-scale temporal sliding window, random flipping, corner cropping, and random shear transformation. 
We apply multi-scale temporal sliding windows to generate the training video clips. For an untrimmed video, we extract temporal sliding windows of lengths $16$, $32$, $64$, $128$, $256$, and $512$ frames \cite{scnn_shou_wang_chang_cvpr16}. For window lengths $16$ and $32$, the sliding stride is $16$, while for other lengths, it is $32$. This strategy effectively suppresses a bias for shorter windows during NMS \cite{compact_fisher_action_oneata_iccv13}. For each window, we uniformly sample $16$ frames to form our fixed-size input clips. All clips are spatially resized to have a frame size of $128 \times 171$ pixels \cite{c3d2015}.  
During training, we horizontally flip each input video clip with $50\%$ probability. We also do corner-cropping, extracting the four corners and the centers of the frames.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth,trim=0 0 0 0,clip]{shear_v2.pdf}
	\vspace{-20ex}
	\caption{Illustration of our random shear training data augmentation. Top: Sample video frames of action ``High jump'' from the THUMOS challenge 2014 training set.  Bottom: We pick the middle image from the top row and shear it between $-25$ and $25$ degrees for display. While subtle, the difference between the leftmost and rightmost frames does simulate limited viewpoint change and results in better models. This figure is best viewed in color. }
	\label{fig:shear}
	\vspace{-2ex}
\end{figure}

We introduce random shear transformation to simulate viewpoint change for action detection problems. This is motivated by the fact that the same action can appear quite different depending on the viewpoint. For example, sample video frames of the action ``High jump'' are shown in the top row of Figure \ref{fig:shear}. The intra-class variability due to viewpoint change is quite large. The ideal augmentation technique would generate multi-view training clips from a single-view input. This would help reduce overfitting. However, generating 3D videos from 2D is a difficult and computationally expensive problem itself \cite{2dto3d_pose_eccv12_rama,DeepStereo2015,3d_shapenet_cvpr15_wu}, and cannot be done on-the-fly while training deep networks. 

We therefore instead use shear transformations to efficiently simulate limited viewpoint changes. We perform random shearing with a shear parameter $\theta$ between $-25$ and $25$ degrees. This range is determined using cross-validation (too small a value makes no difference while too large introduces unrealistic distortions). Border elements are repeated for padding and the sheared image is cropped to its original size. Sample sheared frames can be seen in the bottom row of Figure \ref{fig:shear}. While subtle, shear transformations are shown to simulate limited viewpoint change. Compare the leftmost and rightmost frames. In the experiments below, we show quantitatively that this augmentation strategy does help prevent overfitting and improves the final detection accuracy on all three benchmarks.

\subsection{Network Architecture}
\label{sec:architecture}
In \cite{c3d2015}, the authors show that 2D ConvNets ``forget'' the temporal information in the input signal after every convolution operation. They propose 3D ConvNets which analyze sets of contiguous video frames organized as clips. 3D ConvNets have shown to be effective at learning spatio-temporal features in video volume data analysis problems \cite{vox2vox_cvprw16_dutran} such as video understanding, optical flow estimation, video coloring, etc.

We therefore base our framework on the 3D ConvNets model C3D described in \cite{c3d2015}. Our network has $8$ convolutional layers, $5$ pooling layers, followed by two fully connected layers, and three parallel outputs. All 3D convolution filters are $3 \times 3 \times 3$ with stride $1$, and all 3D poling layers are $2 \times 2 \times 2$ with stride $1$, except for \textsf{pool1}. In order to preserve temporal information in the earlier layers, \textsf{pool1} has size $1 \times 2 \times 2$. The detailed network structure is shown in Figure \ref{fig:modelConfigure}. 
Since C3D is trained on the large Sports1M video dataset \cite{KarpathyCVPR14}, it serves as a good initialization point for training our model. The inputs to our network are mini-batches of video clips of size $16 \times 3 \times 112 \times 112$, and the outputs are three sets of confidence scores: action or not, action category, and temporal actionness.

In order to reduce overfitting and increase the influence of the action categorization branch, we introduce two additional loss functions which are conceptually similar to the auxiliary classifiers in \cite{inception_v3_2015}. Besides the softmax loss from layer \textsf{fc8}, we add two more softmax losses to regularize the learning, one from layer \textsf{conv5} and another from layer \textsf{fc6}. The benefits of this are two-fold. 
(i) Layer \textsf{conv5} contains weak semantic, but strong spatial information, which is important for motion understanding; layers \textsf{fc6} and \textsf{fc8} can provide high-level semantic and abstract information about the video clip. These three loss functions thus incorporate complementary information from the top layers of the network. 
(ii). Incorporating these additional intermediate losses promotes action categorization as the most important of the three main tasks.
Improved action recognition also benefits the other two branches. 
Training our network thus considers a total of five losses.

%We also experiment with having the multi-task branching point before \textsf{fc7}, the last fully-connected layer. We try having it after \textsf{pool3} as well as after \textsf{pool5}. Since there will be a larger number of parameters in these cases, we reduce the number of neurons in layer \textsf{fc6} to $2048$ and in layer \textsf{fc7} to $512$ in the action proposal and regression branches. Experimental results show, though, that branching after the last fully-connected layer provides a good tradeoff between accuracy and efficiency.

%\subsection{Balanced Training Data}
Note that, though there is strong motivation to perform temporal action localization via a parallel multi-task framework, the joint learning of the tasks is a challenge due to potential imbalance in the training data. The training data that is optimal for one branch of the network might not be optimal for the others. The action proposal branch is a binary classification problem and seeks to distinguish between two classes, action and non-action. In contrast, the action categorization branch is a multi-class classification problem and seeks to distinguish between $N+1$ classes ($N$ classes plus background). Usually $N > 1$ so if we generate balanced training data for the action proposal branch, it will be unbalanced for the action categorization branch, and vice versa. 
%There is a conflict between these two branches in terms of the optimal training data balance. Note that this problem does not arise in sequential, multi-stage frameworks \cite{fast_temporal_action_proposal_fabian_cvpr16,scnn_shou_wang_chang_cvpr16} since the stages are trained separately.
We explore two ways to address this problem. (i) Provide balanced data for the action proposal task and tune the class weights when calculating the action categorization loss. This essentially means giving more attention to action clips when training the categorization branch. Or, (ii) provide balanced data for the action categorization branch and tune the class weights when calculating the action proposal loss.
We show in Section \ref{sec:experiments} that (ii) results in higher accuracy. One possible reason is that action categorization is our main goal, while action proposal is just a supporting task. If we provide balanced data for the action proposal task, our network sees too many non-action examples and learns features more from background video clips than from actions of interest.

\subsection{Temporal Actionness}
Actionness was introduced in \cite{actionness_cvpr14_chen} to indicate the likelihood that a specific spatial location contains a generic action instance. This measure is useful for generating accurate action proposals, as well as for related tasks such as action recognition/detection. It applies to the spatial dimension only though and is thus more like a spatial saliency measure. It works well for spatio-temporal action localization, but is not appropriate for temporal action localization.

Hence, we introduce a new concept, \textit{temporal actionness}, that is better suited to volume data analysis, especially when using 3D ConvNets. Temporal actionness is a fractional value $p_{a}$, ranging from $0$ to $1$, that indicates what proportion of a video volume contains an action of interest temporally. During training, $p_{a}$ is computed as the temporal intersection of a video clip with the actual action period, divided by the clip's length, and is thus a scalar. When $p_{a}$ equals $0$, the volume contains no action. When $p_{a}$ equals $1$, all the frames contain the action. If $p_{a}$ is limited to two values, $0$ or $1$, this problem reduces the standard binary classification problem of action or non-action. 

Compared to the temporal Intersection over Union (tIoU) metric, temporal actionness is more effective because its computation does not depend on the length of ground truth, which means this quantity can be predicted. It based only on how much action of interest a clip contains. On the contrary, predicting the value of tIoU is basically impossible when the ground truth information is missing. 
%Different actions change dramatically, even videos from same action vary a lot. Hence, 
This may be the primary reason that direct learning of a regressor for time shift, video segment duration, or tIoU usually does not work well in practice.

We design a regression module as shown in Figure \ref{fig:modelConfigure} to learn temporal actionness. 
To demonstrate its effectiveness, we compare it with another approach from \cite{scnn_shou_wang_chang_cvpr16}. That method contains a localization stage with a new loss function and uses tIoU to scale the confidence scores generated by the classification stage. It achieves state-of-the-art performance when training the modules separately. To compare approaches, we simply replace our regression module with their loss function. The results in Section \ref{sec:experiments} show that our proposed model achieves better performance. The benefit of our approach is that an explicit prediction of temporal actionness could also have other applications.

%However, under our multi-task framework, the performance of introducing a new loss function will be upper-bounded by the shared representation before the branching point. Since our model branches out in the very end of the network, the new loss doesn't benefit as much as separate training. In contrast, a totally different regression problem will add better regularization to the model. Moreover, we can explicitly predict the temporal actionness value to refine the post-processing step, while \cite{scnn_shou_wang_chang_cvpr16} can only use duration prior to refine their results. We show our results in section 4. 

\subsection{Post-processing}
For evaluation, given a video $V$,  we use multi-scale temporal sliding windows to generate input video clips $\{v_{m}\}_{m=1}^{M}$ where $m$ indicates the $m$-th clip and $M$ is the total number of clips for $V$. For each $v_{m}$, we obtain three outputs: the non-action confidence score $p_{b}^{m}$, the action categorization confidence score $\mathbf{p_{l}^{m}}$, and the estimated temporal actionness $p_{a}^{m}$. Bold font indicates vectors.

We design a hierarchical method to integrate these three results. \textit{Clip-level}: For each $v_{m}$, we refine $\mathbf{p_{l}^{m}}$ by multiplying by the temporal actionness value $p_{a}^{m}$. This boosts the categorization confidence scores in clips with lots of action. \textit{Video-level}: For the whole video $V$, we refine $\mathbf{p_{l}^{m}}$ through:
\begin{equation}
	% \vspace{-1ex}
	w_{m} \times \mathbf{p_{l}^{m}}, \quad \quad w_{m} = \frac{e^{-\alpha p_{b}^{m}}}{\sum_{n} e^{-\alpha p_{b}^{n}}}
	\label{eq:softmaxWeights}
\end{equation}
where $w_{m}$ is the softmax weights used to scale the categorization confidence scores of the $m$-th video clip \cite{non_action_wang_cvpr16}. In our pipeline, video clips that are predicted as background are not discarded, they are just suppressed. The lower $p_{b}^{m}$, the higher $w_{m}$, which means the prediction scores of a video clip will be increased if it is more likely to be an action clip, and they will be decreased if it is more likely to be a non-action clip. $\alpha$ is a parameter which can be tuned to find the best weighting.

After the clip-level and video-level refinement, we re-score each window using the frequency of its duration in the training set as a per-class duration prior \cite{compact_fisher_action_oneata_iccv13}. However, this re-scoring step may be optional because no substantial improvement was observed. Note that we can further reduce inference time by discarding non-action clips (i.e. when $p_{b}^{m}$ is smaller than some threshold) as most existing works do.
%in section \ref{sec:experiments}. 
Finally, we apply non-maximum suppression with overlap threshold $\delta$, and select the top-$k$ segments to form the final detection result.
%As reportted in nonaction

\section{Experiments and Results}
\label{sec:experiments}
In this section, we first describe the implementation details and the benchmark datasets used for evaluation. We then present the experimental results and study the effects of the various design choices described in section \ref{sec:methodology}. 

\subsection{Implementation Details}
%We adopt a class-balanced sampling method to prepare mini-batches during training. Such a strategy has been used frequently in recent years for large-scale image datasets due to its flexibility and efficiency. 
%instead of making the data file ready before training like most existing work \cite{c3d2015,scnn_shou_wang_chang_cvpr16} does. This method is used frequently recent years in large-scale image dataset because of its flexibility and efficiency. 
For the ConvNets, we use the Keras toolbox.
During training, the network weights are learned using mini-batch ($30$ video clips) stochastic gradient descent with momentum (set to $0.9$). The learning rate is initialized to $0.0001$, except for layer \textsf{fc8} with $0.01$. Both learning rates decrease to $1/10$ of their values whenever the performance saturates, and training is stopped when the learning rate is smaller than $10^{-6}$. Dropout is applied with a ratio of $0.5$. The five losses are fused with equal weights.
%which has shown the best performance through empirical validation.

For a temporal localization evaluation metric, we follow the standard protocol in THUMOS challenge 2014 to calculate the mean average precision (mAP). A detection is considered to be correct if its overlap with the ground truth action instance is above some threshold, and the action category is predicted correctly.

\subsection{THUMOS Challenge 2014}
THUMOS challenge 2014 is considered to be one of the most challenging datasets for temporal action detection. This dataset consists of $13,320$ trimmed videos from $101$ actions for training, $1,010$ temporally untrimmed videos for validation, and $1,574$ untrimmed videos for testing. The detection task involves $20$ out of the $101$ classes, which are all sports related. During testing, we only use $213$ test videos with annotations to evaluate our framework.

\begin{table}[t]
	\centering
	%\resizebox{\columnwidth}{!}{%
	\scalebox{0.7}{
		\begin{tabular}{ c | c | c | c | c| c }
			\hline
			Model							& 	$\alpha=0.1$		&  $\alpha=0.2$ 	&  $\alpha=0.3$  &  $\alpha=0.4$		&	$\alpha=0.5$ 	\\
			\hline		
			Karaman et al. \cite{florence_thumos2014} 			&   $4.6$ 	& $3.4$	&  $2.1$	&  $1.4$		&  $0.9$			\\
			Wang et al. \cite{motion_appearance_thumos_wang14}		&   $18.2$ 	& $17.0$	&  $14.0$	&  $11.7$		&  $8.3$			\\
			Oneata et al. \cite{learThumos2014}					&   $36.6$ 	& $33.6$	&  $27.0$	&  $20.8$		&  $14.4$			\\
			\hline
			Sun et al. \cite{FGA_web_sun_mm15}  &   $12.4$ 	& $11.0$	&  $8.5$	&  $5.2$		&  $4.4$			\\
			Heilbron et al. \cite{fast_temporal_action_proposal_fabian_cvpr16} &   $36.1$	& $32.9$ & $25.7$	& $18.2$ &  $13.5$		\\
			Richard et al. \cite{action_detection_language_richard_cvpr16}		&   $39.7$ 	& $35.7$	&  $30.0$	&  $23.2$		& $15.2$			\\
			Yeung et al. \cite{frame_glimpse_yeung_cvpr16}	&   $\mathbf{48.9}$ 	& $\mathbf{44.0}$	&  $36.0$	&  $26.4$		&  $17.1$		\\
			%Shou et al. \cite{scnn_shou_wang_chang_cvpr16}	&   $47.7$ 	& $43.5$	&  $36.3$	&  $28.7$		&  $19.0$		\\
			\hline
			Ours \textsf{fc8}		&   $45.6$ 	& $41.2$	&  $34.5$	&  $26.1$		&  $17.0$		\\	
			Ours \textsf{conv5} + \textsf{fc8}		&   $46.6$ 	& $42.9$	&  $35.6$	&  $28.5$		&  $18.7$		\\	
			Ours \textsf{conv5} + \textsf{fc6} + \textsf{fc8}	&   $47.7$ 	& $43.6$	&  $\mathbf{36.2}$	&  $\mathbf{28.9}$		&  $\mathbf{19.0}$	\\	
			\hline
		\end{tabular}
		%}
	}
	\caption{Action detection results on THUMOS 2014 with varied IoU threshold $\alpha$. All performances are reported using mAP. Top section: Top three performers in the 2014 challenge. Middle section: Recent state-of-the-art approaches.  Bottom section: Our approach. We observe that our network with multiple intermediate losses performs the best. Note that \textsf{conv5}, \textsf{fc6}, \textsf{fc8} here refer to losses not features. \label{tab:thumos14Result}}
	\vspace{-2ex}
\end{table} 

\noindent \textbf{Results:}
Since our goal is temporal action localization for untrimmed videos, we include the validation videos in our training data. We have already described how we generate temporal multi-scale video clips for untrimmed videos in Section \ref{sec:methodology}. For trimmed videos, we uniformly sample $16$ frames as clips without multi-scale augmentation, and each clip has the same action label as the video. 
%Due to the class-balanced sampling strategy, we don't need to explicitly make the data balance before training.
During training, we perform $15$K iterations with learning rate $10^{-4}$, $15$K iterations with $10^{-5}$, and $10$K iterations with $10^{-6}$. During prediction, the overlap threshold in NMS is set to $0.4$.

Table \ref{tab:thumos14Result} shows that our method outperforms previous state-of-the-art approaches.
The three models in the top section of Table \ref{tab:thumos14Result} use Fisher vector encoded IDT features on video clips and perform post-processing to refine the localization scores. \cite{learThumos2014} is the winning approach of the 2014 challenge. It uses video-level action classification scores as a context feature which greatly improves the performance.
The four methods in the middle section of Table \ref{tab:thumos14Result} are the most recent state-of-the-art results. 
Note that \cite{frame_glimpse_yeung_cvpr16} achieves better accuracy in the small $\alpha$ regime. One possible reason is that their learned observation policy tends to output shorter segments compared to a standard sliding window approach.
Significantly, though, we estimate that the computation time required to train the approach of \cite{frame_glimpse_yeung_cvpr16} is much higher than ours. Their observation network, a fine-tuned VGG-16 model, by itself requires approximately the same amount of computation time to train as our single 3D ConvNets. They also have to train the RNN-based agent to learn the decision policy.
\cite{scnn_shou_wang_chang_cvpr16} reports a mAP of $19.0$ when $\alpha = 0.5$. This is using the updated evaluation toolkit \cite{THUMOS14}, however, and so is not directly comparably with  the results in Table \ref{tab:thumos14Result}. For completeness, we also evaluate our result using the updated toolkit and obtain an accuracy of $19.2$ given our multi-loss fusion network. 
Furthermore, our approach is several times faster to train/evaluate than \cite{scnn_shou_wang_chang_cvpr16} and we only need to store one model instead of three. More importantly, our method is simpler to train than the multi-stage pipeline of \cite{scnn_shou_wang_chang_cvpr16} which requires manually balanced datasets and separate input files for the three stages.
% adopt a RNN-based agent, and their method is able to produce action predictions end-to-end. The 
%\cite{FGA_web_sun_mm15} and \cite{fast_temporal_action_proposal_fabian_cvpr16} still use IDT features as basic building block, while
%\cite{frame_glimpse_yeung_cvpr16} uses a RNN-based agent and achieves the best performance. Its advantage is seen to increase as $\alpha$ is decreased. Our model achieves similar accuracy in the large $\alpha$ regime. Significantly, though, we estimate that the computation time required to train the approach of \cite{frame_glimpse_yeung_cvpr16} is much higher than ours. Their observation network, a fine-tuned VGG-16 model, by itself requires approximately the same amount of computation time to train as our single 3D ConvNets. They also have to train the RNN-based agent to learn the decision policy.

%they first incorporate static image features (SIFT, Color and CNN), motion features (IDT) and audio features (ASR and MFCC) to train an action classifier. and apply duration prior to refine their results. 
%however, and so is not directly comparably with the results in Table \ref{tab:thumos14Result}. For completeness, we also evaluate our result using the updated toolkit and obtain an accuracy of $17.4$. Since our model branches out at the very end of the network, our performance may be upper-bounded by the shared representation. Again, though, our approach is almost $3$ times faster to train than \cite{scnn_shou_wang_chang_cvpr16} and we only need to store one model instead of three. More importantly, our method is simpler to train than the multi-stage pipeline of \cite{scnn_shou_wang_chang_cvpr16} which requires manually balanced datasets and separate input files for the three stages.

Figure \ref{fig:thumosdemo} shows three examples of predicted action instances from the THUMOS 2014 challenge test set. Our approach is shown to accurately localize the action instances. 
%$V_{T}$ + $V_{U}$ 						  &	  $45.6$ 	& $41.2$	&  $34.5$	&  $26.1$	&  $17.0$		\\	
%$\frac{3}{4} V_{T}$ + $V_{U}$ 		&	$45.5$ 	  & $41.1$	  &  $34.3$	  &  $25.7$	  &  $16.5$			\\
%$\frac{1}{2} V_{T}$ + $V_{U}$  		&   $44.4$ 	  & $39.8$	  &  $33.0$	  &  $24.4$	  &  $15.1$		\\	

\begin{table}[t]
	\begin{minipage}{0.25\textwidth}%
		\centering
		\subfloat[][Sequential Network \cite{scnn_shou_wang_chang_cvpr16}]{
			\scalebox{0.7}{
				\begin{tabular}{| c | c | c | }
					\hline
					Training Set							&  $\alpha=0.2$ 	&	$\alpha=0.5$ 	\\
					\hline		
					$V_{T}$ + $V_{U}$ 						  	& $43.5$	&  $19.0$		\\	
					$\frac{3}{4} V_{T}$ + $V_{U}$ 			  & $41.7$	  	  &  $17.9$			\\
					$\frac{1}{2} V_{T}$ + $V_{U}$  			  & $36.9$	 &  $14.4$		\\	
					\hline
				\end{tabular}
			}
		}
	\end{minipage}%
	\begin{minipage}{0.25\textwidth}%
		\centering
		\subfloat[][Our Parallel Network]{
			\scalebox{0.7}{
				\begin{tabular}{| c | c | c | }
					\hline
					Training Set							&  $\alpha=0.2$ 	&	$\alpha=0.5$ 	\\
					\hline		
					$V_{T}$ + $V_{U}$ 						  	& $43.6$	&  $19.2$		\\	
					$\frac{3}{4} V_{T}$ + $V_{U}$ 			  & $42.9$	  	  &  $18.7$			\\
					$\frac{1}{2} V_{T}$ + $V_{U}$  			  & $39.4$	 &  $17.3$		\\	
					\hline
				\end{tabular}
			}
		}
	\end{minipage}%
	\caption{Comparison between our parallel multi-task learning pipeline and its sequential counterpart in \cite{scnn_shou_wang_chang_cvpr16}. Our method is robust to large reductions in training data. Note the mAP here is reported using the updated evaluation toolkit.\label{tab:regularization}}
\end{table}

%\begin{table}[t]
%	\begin{center}
%		\begin{tabular}{| c | c | c | }
%			\hline
%			Training Set							&  $\alpha=0.2$ 	&	$\alpha=0.5$ 	\\
%			\hline		
%			$V_{T}$ + $V_{U}$ 						  	& $41.2$	&  $17.0$		\\	
%			$\frac{3}{4} V_{T}$ + $V_{U}$ 			  & $41.1$	  	  &  $16.5$			\\
%			$\frac{1}{2} V_{T}$ + $V_{U}$  			  & $39.8$	 &  $15.1$		\\	
%			\hline
%		\end{tabular}
%	\end{center}
%	\caption{Action detection results of our framework on THUMOS 2014 under different configurations. (a):  Our method is robust to large reductions in training data. \label{tab:regularization}}
%\end{table}

\subsection{Discussion}
In this subsection, we will discuss the benefits of using our parallel learning framework over its sequential counterpart. We choose THUMOS 2014 as the illustrating dataset. 

\noindent \textbf{Enhanced Regularization}
Multi-task learning can benefit from the enhanced regularization provided by closely-related tasks. This can increase a model's robustness, especially under the fewer training samples regime. Here, we explore this advantage of our joint learning scheme by gradually reducing the number of trimmed videos in the training set. Our training set for THUMOS 2014 is composed of two parts, the original trimmed training videos which we denote as $V_{T}$, and the untrimmed validation videos which we denote as $V_{U}$. By decreasing the proportion of $V_{T}$ in our training set, we can validate our framework's robustness.

As shown in Table \ref{tab:regularization}(b), we achieve the best performance using the full training dataset, as expected. Notably, though, the performance decreases only slightly when just $3/4$ of $V_{T}$ is used. The relative reduction is still only $9.8\%$ when $1/2$ of $V_{T}$ is used. 

By contrast, less training data is much more likely to result in overfitting in a multi-stage approach like \cite{scnn_shou_wang_chang_cvpr16}. To demonstrate this, we trained the second ``action categorization'' stage of \cite{scnn_shou_wang_chang_cvpr16} using $1/2$ of $V_{T}$ and all $V_{U}$ and the classification accuracy of the 3D ConvNets dropped dramatically. This, in turn, will degrade the performance of the whole system since the localization network is initialized from the categorization stage. The final action detection mAP is $14.4$ as shown in Table \ref{tab:regularization}(a). The relative performance decrease is $24.2\%$ for the sequential framework. This demonstrates that our joint learning framework is more robust when fewer trimmed video examples are available. This is important for video analysis because current video datasets do not have large numbers of labeled training samples. 

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth,trim=0 0 0 0,clip]{thumos_demo.pdf}
	\vspace{-10ex}
	\caption{Examples of predicted action instances on THUMOS 2014. Faded frames indicate background. Ground truth is green while our predictions are blue. Top: CliffDiving. Middle: LongJump. Bottom: ThrowDiscus. This figure is best viewed in color. }
	\label{fig:thumosdemo}
	% \vspace{-2ex}
\end{figure}

\noindent \textbf{Computational and Storage Efficiency}
Our approach is computationally efficient compared to approaches that compute and fuse expensive features \cite{learThumos2014} or to approaches with multiple stages \cite{scnn_shou_wang_chang_cvpr16,fast_temporal_action_proposal_fabian_cvpr16,frame_glimpse_yeung_cvpr16}. Our single model only requires $40K$ iterations to train and each iteration takes approximately one second. Although non-action video clips are not discarded, our class-balanced sampling strategy effectively controls how many clips our ConvNet sees during training. For THUMOS 2014, our model sees less than $10\%$ of the video frames. 

For test time efficiency, our network is almost two times faster than the multi-stage pipeline \cite{scnn_shou_wang_chang_cvpr16}. Intuitively, although \cite{scnn_shou_wang_chang_cvpr16}'s proposal stage can discard many video segments before the classification stage, all the slided windows need to go through a 3D ConvNet first, and then the remaining candidates go through a classification 3D ConNet one more time. Our multi-tasking network only require all the slided windows to go though a 3D ConvNet once. 
We can further increase test time efficiency by controlling the proportion of non-action video clips to preserve by setting a threshold. 

With regards to storage, our approach does not need to store high-dimensional representations, such as Fisher Vector encoded features. And, only a single ConvNet model is required for deployment whose size is less than $0.35$GB. 

\begin{table}[t]
	\begin{minipage}{0.24\textwidth}%
		\centering
		\subfloat[][]{
			\scalebox{0.7}{
				\begin{tabular}{| c | c | c | }
					\hline
					Data Balance						&  $0.2$ 	&	$0.5$ 	\\
					\hline		
					Proposal 						  	& $40.1$	&  $17.9$		\\	
					Categorization			  		& $43.6$	  	  &  $19.0$			\\
					\hline
				\end{tabular}
			}
		}
	\end{minipage}%
	\begin{minipage}{0.24\textwidth}%
		\centering
		\subfloat[][]{
			\scalebox{0.7}{
				\begin{tabular}{| c | c | c |}
					\hline
					Overlap							&  $0.2$ 	&	$0.5$ 	\\
					\hline		
					Localization loss \cite{scnn_shou_wang_chang_cvpr16} 						  	& $43.2$	&  $18.9$		\\	
					Temporal actionness & $43.6$	 &  $19.0$		\\	
					\hline
				\end{tabular}
			}
		}
	\end{minipage}%
	\vspace{1ex}
	\begin{minipage}{0.24\textwidth}%
		\centering
		\subfloat[][]{
			\scalebox{0.7}{
				\begin{tabular}{ | c | c | c |}
					\hline
					Shear Param			&  $0.2$ 	&	$0.5$ 	\\
					\hline		
					$\theta = 0$ 			  & $40.6$	&  $18.3$		\\	
					$\theta = 10$ 			  & $40.9$	&  $18.3$		\\	
					$\theta = 25$ 			  & $43.6$	  	  &  $19.0$			\\
					$\theta = 40$  			  & $39.7$	 &  $17.8$		\\	
					\hline
				\end{tabular}
			}
		}
	\end{minipage}%
	\begin{minipage}{0.24\textwidth}%
		\centering
		\subfloat[][]{
			\scalebox{0.7}{
				\begin{tabular}{ | c | c | c |}
					\hline
					Contribution							&  $0.2$ 	&	$0.5$ 	\\
					\hline		
					w/o proposal						  	& $41.7$	&  $18.2$		\\	
					w/o regression 			  & $40.0$	 &  $17.5$		\\	
					full model						  	& $43.6$	&  $19.0$		\\	
					\hline
				\end{tabular}
			}
		}
	\end{minipage}%
	%\vspace{2ex}
	\caption{Action detection results of our framework on THUMOS 2014 under different configurations. (a) Data imbalance during training. (b) Effectiveness of proposed temporal actionness. (c) Random shear augmentation. (d) Impact of each branch. See the text in Ablation Study for more details. \label{tab:Ablation}}%
	\vspace{-2ex}
	%\end{center}
\end{table} 

\noindent \textbf{Ablation Study} 
We conduct ablative experiments to understand the effects of the different components and parameters in our framework.

%It involves several options 
%\begin{itemize}
%	\item Selection of shear parameter, the angle $\theta$ can be $10$, $25$ and $40$ degrees.
%	\item We explore the locations of branching point after the \textsf{pool3}, \textsf{pool5} or \textsf{fc7} layer.
%	\item Balance training data, we either make proposal task balance or categorization balance.
%	\item We compare our regression loss based on temporal actionness with localization loss  proposed in \cite{scnn_shou_wang_chang_cvpr16} based on clip IoU.
%	\item We specify contributions from proposal and regression tasks.
%\end{itemize}
First, we compare different strategies for balancing the training data. As shown in Table \ref{tab:Ablation}(a), balancing for the action categorization task yields better performance, especially when $\alpha=0.2$. As mentioned before, balancing for the action proposal task likely results in the network seeing too many negative examples and features that are tuned more to background video clips than to actions of interest. 

In Table \ref{tab:Ablation}(b), we replace our regression module based on temporal actionness with the well-designed localization loss function proposed in \cite{scnn_shou_wang_chang_cvpr16}. Our approach obtains slightly better performance, but note the temporal actionness explicitly predicted by our network can have other applications.

We also determine the optimal value of the shear parameter $\theta$ from among $10$, $25$ and $40$ degrees. In Table \ref{tab:Ablation}(c), $\theta=0$ means there is no shear augmentation. When $\theta=10$, the performance does not change. One possible reason is that the corner-cropping augmentation might already provide similar benefits. When $\theta=40$, the performance decreases by $0.5\%$, possibly due to too much distortion. For $\theta=25$, we observe an improvement of $0.7\%$ when $\alpha=0.5$. Considering that the best mAP reported is $19.0$ when tIoU is $0.5$, an improvement from random shear of $0.7\%$ is relatively significant. Most importantly, random shear improves the accuracy of the ``classification'' branch by $1.6\%$ on THUMOS 2014. After the post-processing and NMS, the improvement reduces to $0.7\%$ for the final action detection. Hence, random shear augmentation is very useful to help overcome overfitting during model training. 
%We feel that an accuracy increase of 1.6 is significant for an augmentation method since most augmentation methods bring less than 2 percent improvement. 

%We next explore branching after the \textsf{pool3}, \textsf{pool5}, or \textsf{fc7} layers. Interestingly, as shown in Table \ref{tab:Ablation}(c), branching after \textsf{pool5} performs the worst. This corresponds to pushing the fully-connected layers past the branching point but keeping all the convolutional layers before. However, pushing some convolutional layers after the branching point seems to compensate for this reduced performance. This corresponds to branching after \textsf{pool3}. This may attributed to the specialized representations that are learned by the later layers. We conclude that branching after the last fully-connected layer, \textsf{fc7}, is a good trade-off between accuracy and efficiency.
%The performance from branching after \textsf{pool3} is similar to branching after the last fully-connected layer even with the reduced dimensional \textsf{fc6} and \textsf{fc7} layers. 
%(SDN: I'll need to work with you on these next two sentences). This indicates that the specialize training of each task may learn better representations in the top layers. However, there is no benefit in branching out after \textsf{pool5} because all tasks already share the same convolutional features. 

Finally, we explore whether our action proposal and temporal actionness branches are redundant since the former can be seen as an extreme case of the latter. As shown in Table \ref{tab:Ablation}(d), the full model achieves the best performance, indicating the two branches are complementary. The \textit{w/o proposal} model performs $0.8\%$ worse than the full model, while the \textit{w/o regression} model performs $1.5\%$ worse. This demonstrates that estimating temporal actionness is more beneficial than performing action proposal.
%We make two observations: (1) Temporal actionnness can describe whether a video clip is an action or not to some extent. Thus if efficiency is desired in one system, we can remove proposal stage and make the system more efficient; (2) Estimating temporal actionness is important for later post-processing. It can significantly improve the action detection accuracy.

\begin{table}[t]
	\begin{minipage}{0.25\textwidth}%
		\centering
		\subfloat[][MSR Action II]{
			\scalebox{0.7}{
				\begin{tabular}{ | c | c | }
					\hline
					Model								&	$\alpha=0.5$ 	\\
					\hline
					Yu et al. \cite{yu_FAP_cvpr15} &  $28.2$	\\
					Gemert et al. \cite{APT_Gemert_bmvc15} &  $54.5$\\
					% Shou et al. \cite{scnn_shou_wang_chang_cvpr16}  & $58.7$ \\
					Heilbron et al. \cite{fast_temporal_action_proposal_fabian_cvpr16} 	&  $60.3$		\\
					\hline
					Ours + \textsf{fc8}		&  $59.6$		\\	
					Ours \textsf{conv5} + \textsf{fc6} + \textsf{fc8}  &  $\mathbf{61.1}$		\\	
					\hline
				\end{tabular}
			}
		}
	\end{minipage}%
	%\qquad
	\begin{minipage}{0.25\textwidth}%
		\centering
		\subfloat[][MPII Cooking]{
			\scalebox{0.7}{
				\begin{tabular}{| c | c | }
					\hline
					Model								&	$\alpha=0.5$ 	\\
					\hline
					Sliding Window  &  $7.9$	\\
					Gemert et al. \cite{APT_Gemert_bmvc15} &  $13.1$\\
					Richard et al. \cite{action_detection_language_richard_cvpr16} 	&  $14.0$		\\
					\hline
					Ours + \textsf{fc8}		&  $14.6$		\\	
					Ours \textsf{conv5} + \textsf{fc6} + \textsf{fc8}		&  $\mathbf{14.9}$		\\	
					\hline
				\end{tabular}
			}
		}
	\end{minipage}%
	%\vspace{2ex}
	\caption{Action detection results on MSR Action Dataset II and MPII Cooking dataset. Our approach consistently achieves better performance compared to state-of-the-art approaches at $0.5$ overlap threshold.\label{tab:MSRAndMEX}}%
	%\end{center}
\end{table} 

\subsection{MSR Action Dataset II}
The MSR Action Dataset II \cite{MSR_action_II_PAMI11} consists of $54$ video sequences ($203$ action instances) recorded in a crowded environment. There are three action types: hand waving, handclapping, and boxing. We follow the standard cross-dataset evaluation protocol of using the KTH dataset for training.  During training, we perform $8$K iterations with learning rate $10^{-4}$, $8$K iterations with $10^{-5}$, and $4$K iterations with $10^{-6}$. During prediction, the overlap threshold in NMS is $0.5$.
%The video resolution is 320x240 and frame rate is 15 frames per second. 

Table \ref{tab:MSRAndMEX}(a) shows that our joint learning framework with multiple intermediate losses performs better than the state-of-the-art approach of \cite{fast_temporal_action_proposal_fabian_cvpr16}. That method uses IDT features to represent the video clips, however, which prevents it from being applied to large-scale datasets since the Fisher vector encoded features are too large to be cached. Our approach can easily scale to large datasets. We attribute our good performance, at least in part, to the joint regularized learning and to the temporal actionness refined post-processing. Note that, \cite{yu_FAP_cvpr15} and \cite{APT_Gemert_bmvc15} are designed to generate spatio-temporal proposals. \cite{fast_temporal_action_proposal_fabian_cvpr16} projects these proposals to the temporal dimension and randomly picks a subset of temporal proposals to perform action detection. An example of predicted ``boxing'' instance using our approach can be seen in Figure \ref{fig:MSRandMPII} top.

\subsection{MPII Cooking}
MPII Cooking \cite{MPII_cooking_cvpr12} is a large, fine-grained cooking activities dataset. It contains $44$ videos with a total length of more than $8$ hours of $12$ participants performing $65$ different cooking activities. It consists of a total of 5,609 annotations spread over the $65$ activity categories, including a background class for the action detection task. It also has a second type of annotation, articulated human pose for a pose estimation problem, which we ignore. Following the standard protocol in \cite{MPII_cooking_cvpr12}, we have $7$ splits after performing leave-one-person-out cross-validation. Each split uses $11$ subjects for training, leaving one for validation. During training, we perform $10$K iterations with learning rate $10^{-4}$, $10$K iterations with $10^{-5}$, and $10$K iterations with $10^{-6}$. During prediction, the overlap threshold in NMS is set to $0.5$.

We compare our method to a sliding window baseline similar to \cite{action_detection_language_richard_cvpr16,MPII_cooking_cvpr12}.
Table \ref{tab:MSRAndMEX}(b) shows our method performs better than both the baseline and recent state-of-the-art approaches \cite{action_detection_language_richard_cvpr16,APT_Gemert_bmvc15}. \cite{action_detection_language_richard_cvpr16} is novel in that it includes a length and language model in addition to an action classifier. 
%At the same time, they can do joint inference of globally optimal action segmentation and classification efficiently using dynamic programming. 
However, their runtime during inference is quadratic in the number of frames. By limiting the maximal action length to a constant, they can solve the action detection problem in a reasonable time, but this is not easily scalable to long videos. 
When we compare our approach to \cite{APT_Gemert_bmvc15}, we report their performance by applying the implementation kindly provided by the authors. An example of predicted ``Take out from drawer'' instance using our approach can be seen in Figure \ref{fig:MSRandMPII} bottom.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth,trim=0 0 0 0,clip]{MSR_MPII_demo.pdf}
	\vspace{-18ex}
	\caption{Examples of predicted action instances. Faded frames indicate background. Ground truth is green while our predictions are blue. Top: ``Boxing'' in MSR Action Dataset II. Bottom: ``Take out from drawer'' in MPII Cooking dataset. This figure is best viewed in color. }
	\label{fig:MSRandMPII}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}
We present a novel multi-task learning framework to detect human actions in long, untrimmed videos. 
%Our joint model is more efficient in terms of storage and computation than current state-of-the-art methods, and yet more accurate. 
Performing the three related tasks in parallel exploits the training data more thoroughly and is more efficient in time and space. The random shear augmentation simulates viewpoint differences. The multiple intermediate losses force the network to learn better representations for action categorization. These theoretical insights are supported by our experimental results on three popular datasets.
%We will make our trained models publicly available for other researchers.
In future work, we plan to revise our multi-task approach to use a 3D residual learning framework \cite{residual_cvpr16}. 
We are also interested in optimizing the network end-to-end similarly to Faster R-CNN \cite{fasterRCNN}, since our temporal actionness scores can be considered as region proposal scores. 
%However, due to current GPU memory limit, a smaller but effective network architecture is needed.
%We are also interested in applying network pruning or binarization to speed up the training process and reduce the model size, which will make our method applicable to real-time video surveillance and deployable on mobile devices.  
\newline

\noindent \textbf{Acknowledgements.} This work was funded in part by a National Science Foundation CAREER grant, $\#$IIS-1150115, and a seed grant from the Center for Information Technology in the Interest of Society (CITRIS). We gratefully acknowledge NVIDIA Corporation through the donation of the Titan X GPU used in this work.

{\small
	\bibliographystyle{ieee}
	\bibliography{egbib_cvpr}
}

\end{document}
