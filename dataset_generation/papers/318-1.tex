% !TeX spellcheck = en_US
\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
 %\usepackage[nonatbib]{nips_2016}
 
\PassOptionsToPackage{numbers, compress}{natbib}
%\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{color,mathtools}
\usepackage{enumitem}
\usepackage{multirow}
%\usepackage{paralist}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithm2e,algorithmic,refcount}
%\usepackage[]{algorithm2e}
\graphicspath{ {images/} }

%\title{Improved Semi-supervised Learning with GANs using Manifold Invariances} %just a placeholder -- we can decide later
\title{Semi-supervised Learning with GANs:  Manifold Invariance with Improved Inference} 

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Abhishek Kumar\thanks{Contributed equally.} \\
  IBM Research AI\\
  Yorktown Heights, NY\\
  \texttt{abhishk@us.ibm.com} \\
  %% examples of more authors
  \And
  Prasanna Sattigeri\footnotemark[1]  \\
  IBM Research AI\\
  Yorktown Heights, NY\\
  \texttt{psattig@us.ibm.com} \\
  %% \texttt{email} \\
  \And
  P. Thomas Fletcher \\
  University of Utah\\
  Salt Lake City, UT\\
  \texttt{fletcher@sci.utah.edu} \\
}


\newcommand{\punt}[1]{}%%\newtheorem{thm}{Theorem}\newtheorem{thm}{Theorem}[section]\newtheorem{lem}{Lemma}[section]%\newtheorem{claim}[thm]{Claim}%\newtheorem{cor}[thm]{Corollary}%\newtheorem{prop}[thm]{Property}\newtheorem{prop}{Proposition}\newtheorem{defn}{Definition}[section]%\theoremstyle{definition}\newtheorem{condition}{Condition}%\newtheorem{thm}{Theorem}[section]\def\argmax{\mathop{\rm arg\,max}}\def\argmin{\mathop{\rm arg\,min}}\newcommand{\transpose}{{\scriptscriptstyle T}}\newcommand{\reals}{\mathbb{R}}\newcommand{\biggram}{\overrightarrow{\mathbf{K}}}\newcommand{\biggramsep}{{\vK \otimes \vL}}\newcommand{\veckernel}{\overrightarrow{k}}\newcommand{\chvec}{\ch_{\veckernel}}\newcommand{\chvecsep}{\ch_{k\vL}}\newcommand{\chdict}{\ch_{\cd}}\newcommand{\expect}{\mathbb{E}}\newcommand{\integers}{\mathbf{Z}}\newcommand{\naturals}{\mathbf{N}}\newcommand{\rationals}{\mathbf{Q}}\newcommand{\ca}{\mathcal{A}}\newcommand{\cb}{\mathcal{B}}\newcommand{\cc}{\mathcal{C}}\newcommand{\cd}{\mathcal{D}}\newcommand{\ce}{\mathcal{E}}\newcommand{\cf}{\mathcal{F}}\newcommand{\cg}{\mathcal{G}}\newcommand{\ch}{\mathcal{H}}\newcommand{\ci}{\mathcal{I}}\newcommand{\cj}{\mathcal{J}}\newcommand{\ck}{\mathcal{K}}\newcommand{\cl}{\mathcal{L}}\newcommand{\cm}{\mathcal{M}}\newcommand{\cn}{\mathcal{N}}\newcommand{\co}{\mathcal{O}}\newcommand{\cp}{\mathcal{P}}\newcommand{\cq}{\mathcal{Q}}\newcommand{\calr}{\mathcal{R}}\newcommand{\cs}{\mathcal{S}}\newcommand{\ct}{\mathcal{T}}\newcommand{\cu}{\mathcal{U}}\newcommand{\cv}{\mathcal{V}}\newcommand{\cw}{\mathcal{W}}\newcommand{\cx}{\mathcal{X}}\newcommand{\cy}{\mathcal{Y}}\newcommand{\cz}{\mathcal{Z}}\newcommand{\cS}{\mathcal{S}}%\newcommand{\ind}[1]{1(#1)}%\newcommand{\pr}{P}\newcommand{\pr}{\mathbb{P}}\newcommand{\predsp}{\cy}%{\hat{\cy}}\newcommand{\outsp}{\cy}\newcommand{\prxy}{P_{\cx \times \cy}}\newcommand{\prx}{P_{\cx}}\newcommand{\prygivenx}{P_{\cy\mid\cx}}%\newcommand{\ex}{E}\newcommand{\ex}{\mathbb{E}}\newcommand{\var}{\textrm{Var}}\newcommand{\cov}{\textrm{Cov}}%\newcommand{\sgn}{\textrm{sgn}}%\newcommand{\sign}{\textrm{sign}}\newcommand{\kl}{\textrm{KL}}\newcommand{\law}{\mathcal{L}}%\newcommand{\eps}{\varepsilon}\newcommand{\as}{\textrm{ a.s.}}\newcommand{\io}{\textrm{ i.o.}}\newcommand{\ev}{\textrm{ ev.}}\newcommand{\convd}{\stackrel{d}{\to}}\newcommand{\eqd}{\stackrel{d}{=}}\newcommand{\del}{\nabla}\newcommand{\loss}{V}\newcommand{\risk}{R}\newcommand{\emprisk}{\hat{R}_{\ell}}%\newcommand{\lossfnl}{L}%\newcommand{\emplossfnl}{\hat{L}}\newcommand{\lossfnl}{\risk}\newcommand{\emplossfnl}{\emprisk}\newcommand{\empminimizer}[1]{\hat{#1}_{\ell}}\newcommand{\empminimizerfa}{\hat{f}_{\ell}^{1}}\newcommand{\empminimizerfb}{\hat{f}_{\ell}^{2}}\newcommand{\minimizer}[1]{#1_{*}}\newcommand{\etal}{\textrm{et. al.}}%\newcommand{\tr}{\operatorname{tr}}\newcommand{\rademacher}[1]{\calr_{#1}}\newcommand{\emprademacher}[1]{\hat{\calr}_{#1}}\newcommand{\trace}{\operatorname{trace}}%\newcommand{\diag}{\text{diag}}\newcommand{\rank}{\text{rank}}\newcommand{\linspan}{\text{span}}\newcommand{\spn}{\text{span}}\newcommand{\proj}{\text{Proj}}%\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}%\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}\def\argmax{\mathop{\rm arg\,max}}\def\argmin{\mathop{\rm arg\,min}}%\newcommand{\inf}{\operatornamewithlimits{inf}}\newcommand{\bfx}{\mathbf{x}}\newcommand{\bfy}{\mathbf{y}}\newcommand{\bfl}{\bm{\lambda}}\newcommand{\bfm}{\mathbf{\mu}}\newcommand{\calL}{\mathcal{L}}%\newcommand{\v0}{\mathbf{0}}%\newcommand{\v1}{\mathbf{1}}\newcommand{\vX}{\mathbf{X}}\newcommand{\vY}{\mathbf{Y}}\newcommand{\vA}{\mathbf{A}}\newcommand{\vB}{\mathbf{B}}\newcommand{\vE}{\mathbf{E}}\newcommand{\vK}{\mathbf{K}}\newcommand{\vD}{\mathbf{D}}\newcommand{\vU}{\mathbf{U}}\newcommand{\vL}{\mathbf{L}}\newcommand{\vI}{\mathbf{I}}\newcommand{\vC}{\mathbf{C}}\newcommand{\vV}{\mathbf{V}}\newcommand{\vM}{\mathbf{M}}\newcommand{\vN}{\mathbf{N}}\newcommand{\vQ}{\mathbf{Q}}\newcommand{\vR}{\mathbf{R}}\newcommand{\vS}{\mathbf{S}}\newcommand{\vT}{\mathbf{T}}\newcommand{\vZ}{\mathbf{Z}}\newcommand{\vW}{\mathbf{W}}\newcommand{\vH}{\mathbf{H}}\newcommand{\vsig}{\bm{\Sigma}}\newcommand{\vLam}{\bm{\Lambda}}\newcommand{\vLambda}{\bm{\Lambda}}\newcommand{\vlam}{\bm{\lambda}}\newcommand{\vw}{\mathbf{w}}\newcommand{\vx}{\mathbf{x}}\newcommand{\vxi}{\bm{\xi}}\newcommand{\valpha}{\bm{\alpha}}\newcommand{\vbeta}{\bm{\beta}}\newcommand{\veta}{\bm{\eta}}\newcommand{\vsigma}{\bm{\sigma}}\newcommand{\vepsilon}{\bm{\epsilon}}\newcommand{\vdelta}{\bm{\delta}}\newcommand{\vnu}{\bm{\nu}}\newcommand{\vd}{\mathbf{d}}\newcommand{\vs}{\mathbf{s}}\newcommand{\vt}{\mathbf{t}}\newcommand{\vh}{\mathbf{h}}\newcommand{\ve}{\mathbf{e}}\newcommand{\vf}{\mathbf{f}}\newcommand{\vg}{\mathbf{g}}\newcommand{\vz}{\mathbf{z}}\newcommand{\vk}{\mathbf{k}}\newcommand{\va}{\mathbf{a}}\newcommand{\vb}{\mathbf{b}}\newcommand{\vv}{\mathbf{v}}\newcommand{\vr}{\mathbf{r}}\newcommand{\vy}{\mathbf{y}}\newcommand{\vu}{\mathbf{u}}\newcommand{\vp}{\mathbf{p}}\newcommand{\vq}{\mathbf{q}}\newcommand{\vc}{\mathbf{c}}\newcommand{\vn}{\mathbf{n}}%\newcommand{\vy}{\boldsymbol{y}}%\newcommand{\vA}{\boldsymbol{A}}%\newcommand{\vW}{\mathbf{W}}\newcommand{\vP}{\mathbf{P}}\newcommand{\vG}{\mathbf{G}}%\newcommand{\vH}{\mathbf{H}}%\def\v{}{\mathbf{{}}}%%\newcommand{\bold}[1]{\ensuremath{\mathbf{#1}}}\newcommand{\hil}{\ch}\newcommand{\rkhs}{\hil}\newcommand{\manifold}{\cm}\newcommand{\cloud}{\cc}\newcommand{\graph}{\cg}\newcommand{\vertices}{\cv}\newcommand{\coreg}{{\scriptscriptstyle \cc}}\newcommand{\intrinsic}{{\scriptscriptstyle \ci}}%\newcommand{\intrinsic}{{\ci}}\newcommand{\ambient}{{\scriptscriptstyle \ca}}%\newcommand{\isintrinsic}[1]{#1^\text{i}}\newcommand{\isintrinsic}[1]{#1^{\scriptscriptstyle \ci}}%\newcommand{\isambient}[1]{#1^\text{a}}\newcommand{\isambient}[1]{#1^{\scriptscriptstyle \ca}}\newcommand{\hilamb}{\ch^\ambient}\newcommand{\hilintr}{\ch^\intrinsic}\newcommand{\M}{\text{FIX ME NOW}}%temporary\newcommand{\ktilde}{\tilde{k}}\newcommand{\corkhs}{\tilde{\hil}}\newcommand{\cok}{\ktilde}\newcommand{\coK}{\tilde{K}}\newcommand{\copar}{\lambda}\newcommand{\reg}{\gamma}\newcommand{\rega}{\gamma_{1}}\newcommand{\regb}{\gamma_{2}}\newcommand{\regamb}{\gamma_{\ambient}}\newcommand{\regintr}{\gamma_{\intrinsic}}\newcommand{\regfn}{\Omega}\newcommand{\regfnintr}{\regfn_{\intrinsic}}\newcommand{\regfnamb}{\regfn_{\ambient}}\newcommand{\regfncoreg}{\regfn_{\coreg}}\newcommand{\bq}{\begin{equation}}\newcommand{\eq}{\end{equation}}\newcommand{\ba}{\begin{eqnarray}}\newcommand{\ea}{\end{eqnarray}}\newcommand{\spana}{\cl^{1}}\newcommand{\spanb}{\cl^{2}}\newcommand{\ha}{\hil^{1}}\newcommand{\hb}{\hil^{2}}\newcommand{\fa}{f^{1}}\newcommand{\fb}{f^{2}}\newcommand{\Fa}{\cf^{1}}\newcommand{\Fb}{\cf^{2}}\newcommand{\ka}{k^{1}}\newcommand{\kb}{k^{2}}\newcommand{\vcok}{\boldsymbol{\cok}}\newcommand{\vka}{\vk^{1}}\newcommand{\vkb}{\vk^{2}}\newcommand{\Ka}{K^{1}}\newcommand{\Kb}{K^{2}}\newcommand{\kux}{\vk_{Ux}}\newcommand{\kuz}{\vk_{Uz}}\newcommand{\kuu}{K_{UU}}\newcommand{\kul}{K_{UL}}\newcommand{\klu}{K_{LU}}\newcommand{\kll}{K_{LL}}\newcommand{\kuxa}{\vk_{Ux}^{1}}\newcommand{\kuza}{\vk_{Uz}^{1}}\newcommand{\Kuua}{K_{UU}^{1}}\newcommand{\Kula}{K_{UL}^{1}}\newcommand{\Klua}{K_{LU}^{1}}\newcommand{\Klla}{K_{LL}^{1}}\newcommand{\kuxb}{\vk_{Ux}^{2}}\newcommand{\kuzb}{\vk_{Uz}^{2}}\newcommand{\Kuub}{K_{UU}^{2}}\newcommand{\Kulb}{K_{UL}^{2}}\newcommand{\Klub}{K_{LU}^{2}}\newcommand{\Kllb}{K_{LL}^{2}}\newcommand{\Ksum}{S}\newcommand{\ksum}{s}\newcommand{\vksum}{\vs}\newcommand{\intrinsicRegMat}{M_{\intrinsic}}\newcommand{\coregPointCloudMat}{M_{\coreg}}\newcommand{\vid}[1]{#1^\text{vid}}\newcommand{\aud}[1]{#1^\text{aud}}\newcommand{\bad}[1]{#1_\text{bad}}\newcommand{\empcompat}{\hat{\chi}}\newcommand{\nn}{\ensuremath{k}}\newcommand{\uva}{\ushort{\va}}\newcommand{\uvf}{\ushort{\vf}}\newcommand{\uvg}{\ushort{\vg}}\newcommand{\uvk}{\ushort{\vk}}\newcommand{\uvw}{\ushort{\vw}}\newcommand{\uvh}{\ushort{\vh}}\newcommand{\uvbeta}{\ushort{\vbeta}}\newcommand{\uA}{\ushort{A}}\newcommand{\uG}{\ushort{G}}% For Vikas :)\def\la{{\langle}}\def\ra{{\rangle}}\def\R{{\reals}}%added by AK\newcommand{\mbf}[1]{\mathbf{#1}}\newcommand{\mbb}[1]{\mathbb{#1}}\newcommand{\mcal}[1]{\mathcal{#1}}\newcommand{\remove}[1]{}\newcommand{\nuc}[1]{\left\lVert #1\right\rVert_*}\newcommand{\spec}[1]{\left\lVert #1\right\rVert_2}\newcommand{\frob}[1]{\left\lVert #1\right\rVert_\fro}\newcommand{\norm}[1]{\left\lVert #1\right\rVert}\newcommand{\abs}[1]{\left\lvert #1\right\rvert}\newcommand{\obs}[1]{P_\Omega({#1})}\newcommand{\red}[1]{{\color{red} #1}}%\newenvironment{psmallmatrix}{\left(\begin{smallmatrix}} {\end{smallmatrix}\right)}%%%%%%%%%%%%%%%%%\newcommand{\ball}{\mathcal{B}}\newcommand{\sphere}{\mathcal{S}}%\newcommand{\R}{\mathbb{R}}\newcommand{\X}{\mathcal{X}}\newcommand{\domain}{\mathcal{C}}\newcommand{\polytope}{\mathcal{P}}\newcommand{\Am}{\bm{A}}\newcommand{\Bm}{\bm{B}}\newcommand{\xv}{\bm{x}}\newcommand{\yv}{\bm{y}}\newcommand{\zv}{\bm{z}}\newcommand{\sv}{\bm{s}}\newcommand{\av}{\bm{a}}\newcommand{\bv}{\bm{b}}%\newcommand{\cv}{\bm{c}}%\newcommand{\ev}{\bm{e}}\newcommand{\pv}{\bm{p}}\newcommand{\uv}{\bm{u}}%\newcommand{\vv}{\bm{v}}\newcommand{\wv}{\bm{w}}\newcommand{\omegav}{\bm{\omega}}\newcommand{\mv}{\bm{m}}\newcommand{\GW}{G_\setC}%{G_{\!f,\setC}}\newcommand{\G}{G}\newcommand{\0}{\bm{0}}\newcommand{\id}{\bm{\iota}}\newcommand{\alphav}{\bm{\alpha}}\newcommand{\nuv}{\bm{\nu}}\newcommand{\thetav}{\bm{\theta}}\newcommand{\lambdav}{\bm{\lambda}}\newcommand{\epsilonv}{\bm{\epsilon}}%\newcommand{\GW}{gap_{FW}}\newcommand{\row}{\text{row}}\newcommand{\col}{\text{col}}\newcommand{\lft}{\text{left}}\newcommand{\rgt}{\text{right}}\newcommand{\one}{\mathbf{1}}% all one vector\DeclareMathOperator{\diam}{diam}\DeclareMathOperator{\diag}{diag}\DeclareMathOperator{\dom}{dom}% domain\newcommand{\todo}[1]{\marginpar[\hspace*{4.5em}\textbf{TODO}\hspace*{-4.5em}]{\textbf{TODO}}\textbf{TODO:} #1}\newcommand{\aatop}[2]{\genfrac{}{}{0pt}{}{#1}{#2}}%\newcommand{\etal}{\textit{et al}.}\newcommand{\ie}{\textit{i}.\textit{e}.}\newcommand{\eg}{\textit{e}.\textit{g}.}\newcommand{\blue}[1]{\textcolor{blue}{#1}}
%\newcommand{\proposed}{LGIKA}
\newcommand{\Abhishek}[1]{\texttt{{\color{blue} Abhishek says: #1}}}

\begin{document}
% \nipsfinalcopy is no longer used
\maketitle

\begin{abstract}
Semi-supervised learning methods using Generative adversarial networks (GANs) have shown promising empirical success recently. Most of these methods use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label. Motivated by the ability of the GANs generator to capture the data manifold well, we propose to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier. In the process, we propose enhancements over existing methods for learning the inverse mapping (i.e., the encoder)  which greatly improves in terms of semantic similarity of the reconstructed sample with the input sample. We observe considerable empirical gains in semi-supervised learning over baselines, particularly in the cases when the number of labeled examples is low. We also provide insights into how fake examples influence the semi-supervised learning procedure. 
\end{abstract}

\section{Introduction}

Deep generative models (both implicit \cite{goodfellow2014generative,mohamed2016learning} as well as prescribed \cite{kingma2013auto}) have become widely popular for generative modeling of data. Generative adversarial networks (GANs) \cite{goodfellow2014generative} in particular have shown remarkable success in generating very realistic images in several cases \cite{radford2015unsupervised,berthelot2017began}.
The generator in a GAN  can be seen as learning a nonlinear parametric mapping $g: Z \to X$ to the data manifold. 
In most applications of interest (e.g., modeling images), we have $\text{dim}(Z) \ll \text{dim}(X)$. 
A distribution $p_z$ over the space $Z$ (e.g., uniform), combined with this mapping, induces a distribution $p_g$ over the space $X$ and a sample from this distribution can be obtained by ancestral sampling, i.e., $z\sim p_z,\, x = g(z)$.  GANs use adversarial training where the discriminator approximates (lower bounds) a divergence measure (e.g., an $f$-divergence) between $p_g$ and the real data distribution $p_x$ by solving an optimization problem, and the generator tries to minimize this \cite{nowozin2016f,goodfellow2014generative}. It can also be seen from another perspective where the discriminator tries to tell apart real examples $x\sim p_x$ from \emph{fake} examples $x_g\sim p_g$ by minimizing an appropriate loss function\cite[Ch.\ 14.2.4]{friedman2001elements}\cite{menon2016linking}, and the generator tries to generate samples that maximize that loss \cite{tu2007learning,goodfellow2014generative}. 

One of the primary motivations for studying deep generative models is for semi-supervised learning. Indeed, several recent works have shown promising empirical results on semi-supervised learning with both implicit as well as prescribed generative models \cite{kingma2014semi,rasmus2015semi,salimans2016improved,dumoulin2016adversarially,maaloe2016auxiliary,odena2016semi,santos2017learning}. Most state-of-the-art semi-supervised learning methods using GANs \cite{salimans2016improved,dumoulin2016adversarially,odena2016semi} use the discriminator of the GAN as the classifier which now outputs $k+1$ probabilities ($k$ probabilities for the $k$\emph{real} classes and one probability for the \emph{fake} class). 

When the generator of a trained GAN produces very realistic images, it can be argued to capture the data manifold well whose properties can be used for semi-supervised learning. In particular, the tangent spaces of the manifold can inform us about the desirable invariances one may wish to inject in a classifier \cite{simard1998transformation,rifai2011manifold}. In this work we make following contributions: 
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*]
\item We propose to use the tangents from the generator's mapping to automatically infer the desired invariances and further improve on semi-supervised learning. This can be contrasted with methods that assume the knowledge of these invariances (e.g., rotation, translation, horizontal flipping, etc.) \cite{simard1998transformation,laine2016temporal,mroueh2015,raj2016local}.  
\item Estimating tangents for a real sample $x$ requires us to learn an encoder $h$ that maps from data to latent space (inference), i.e., $h:X\to Z$. We propose enhancements over existing methods for learning the encoder \cite{donahue2016adversarial,dumoulin2016adversarially} which improve the semantic match between $x$ and $g(h(x))$ and counter the problem of \emph{class-switching}. 
\item Further, we provide insights into the workings of GAN based semi-supervised learning methods \cite{salimans2016improved} on how fake examples affect the learning. \end{itemize}\remove{
\Abhishek{\\
1. cite some results on transformations of random variables -- necessary conditions, etc? \\
2. Under what conditions on $g$, the mapping will generate a ``manifold''? Lemma 1 in \cite{arjovsky2017towards} says $g(Z)$ would be a countable union of manifolds of dimensions at most dim$(Z)$, for the $g()$ which is normally used in these generative models. \\
3. GANs learn the mapping $g$ by minimizing the distance between the two distributions: the one induced by $g(Z)$ and the data distribution $p(X)$, using a suitable distance measure, e.g. f-divergences \cite{nowozin2016f}, Earth-mover distance \cite{arjovsky2017wasserstein}, etc. How does the matching of distributions relate to ``learning the manifold''? 
}
}

\section{Semi-supervised learning using GANs}%a para about earlier work
Most of the existing methods for semi-supervised learning using GANs modify the regular GAN discriminator to have $k$ outputs corresponding to $k$ real classes~\cite{springenberg2015unsupervised}, and in some cases a $(k+1)$'th output that corresponds to \emph{fake} samples from the generator \cite{salimans2016improved,odena2016semi,dumoulin2016adversarially}. The generator is mainly used as a source of additional data (\emph{fake} samples) which the discriminator tries to classify under the $(k+1)$th label. We propose to use the generator to obtain the tangents to the image manifold and use these to inject invariances into the classifier \cite{simard1998transformation}.

%start with estimation of tangents, need for encoder%relation b/w tangents obtained from encoder and those obtained from decoder%avoiding the SVD in getting the tangents - PCA in latent space%approximation of tangent space - some theory\subsection{Estimating the tangent space of data manifold}
Earlier work has used contractive autoencoders (CAE) to estimate the local tangent space at each point \cite{rifai2011manifold}. CAEs optimize the regular autoencoder loss (reconstruction error) augmented with an additional $\ell_2$-norm penalty on the Jacobian of the encoder mapping. \citet{rifai2011manifold} intuitively reason that the encoder of the CAE trained in this fashion is sensitive only to the tangent directions and use the dominant singular vectors of the Jacobian of the encoder as the tangents. This, however, involves extra computational overhead of doing an SVD for every training sample which we will avoid in our GAN based approach. GANs have also been established to generate better quality samples than prescribed models (e.g., reconstruction loss based approaches) like VAEs \cite{kingma2013auto} and hence can be argued to learn a more accurate parameterization of the image manifold.

The trained generator of the GAN serves as a parametric mapping from a low dimensional space $Z$ to a manifold $\mcal{M}$ embedded in the higher dimensional space $X$, $g: Z\to X$, where $Z$ is an open subset in $\reals^d$ and $X$ is an open subset in $\reals^D$ under the standard topologies on $\reals^d$ and $\reals^D$, respectively ($d\ll D$). This map is not surjective and the range of $g$ is restricted to $\mcal{M}$.\footnote{We write $g$ as a map from $Z$ to $X$ to avoid the unnecessary (in our context) burden of manifold terminologies and still being technically correct. This also enables us to get the Jacobian of $g$ as a regular matrix in $\reals^{D\times d}$, instead of working with the \emph{differential} if $g$ was taken as a map from $Z$ to $\mcal{M}$.} We assume $g$ is a smooth, injective mapping,
so that $\mcal{M}$ is an embedded manifold. The Jacobian of a function $f:\reals^d\to \reals^D$ at $z\in\reals^d$, $J_z f$, is the matrix of partial derivatives (of shape $D\times d$).
The Jacobian of $g$ at $z\in Z$, $J_zg$, provides a mapping from the tangent space at $z\in Z$\emph{into} the tangent space at $x=g(z)\in X$, i.e., $J_z g: T_z Z\to T_x X$. It should be noted that $T_zZ$ is isomorphic to $\reals^d$ and $T_xX$ is isomorphic to $\reals^D$. However, this mapping is not surjective and the range of $J_z g$ is restricted to the tangent space of the manifold $\mcal{M}$ at $x=g(z)$, denoted as $T_x\mcal{M}$ (for all $z\in Z$). As GANs are capable of generating realistic samples (particularly for natural images), one can argue that $\mcal{M}$ approximates the true data manifold well and hence the tangents to $\mcal{M}$ obtained using $J_z g$ are \emph{close} to the tangents to the true data manifold. The problem of learning a smooth manifold from finite samples has been studied in the literature\cite{canas2012learning,bernstein2012tangent, niyogi2011topological, chen2011some, vidal2005generalized, lerman2010probabilistic, jia2015laplacian, bernstein2014data}, and it is an interesting problem in its own right to study the manifold approximation error of GANs, which minimize a chosen divergence measure between the data distribution and the \emph{fake} distribution \cite{nowozin2016f,mohamed2016learning} using finite samples, however this is outside the scope of the current work.

For a given data sample $x\in X$, we need to find its corresponding latent representation $z$ before we can use $J_z g$ to get the tangents to the  manifold $\mcal{M}$ at $x$. For our current discussion we assume the availability of a so-called \emph{encoder}$h:X\to Z$, such that $h(g(z))=z\,\forall\,z\in Z$.
By definition, the Jacobian of the generator at $z$, $J_z g$, can be used to get the tangent directions to the manifold at a point $x=g(z)\in \mcal{M}$. The following lemma specifies the conditions for existence of the encoder $h$ and shows that such an encoder can also be used to get tangent directions. Later we will come back to the issues involved in training such an encoder. 

\begin{lem}
If the Jacobian of $g$ at $z\in Z$, $J_z g$, is full rank then $g$ is locally invertible in the open neighborhood $g(S)$ ($S$ being an open neighborhood of $z$), and there exists a smooth $h:g(S)\to S$ such that $h(g(y))=y,\forall\,y\in S$. In this case, the Jacobian of $h$ at $x=g(z)$, $J_x h$, spans the tangent space of $\mcal{M}$ at $x$.
\label{lem:tangenc}
\end{lem}\begin{proof}
We refer the reader to standard textbooks on multivariate calculus and differentiable manifolds for the first statement of the lemma (e.g., \cite{spivak_CompVol1}).\\
The second statement can be easily deduced by looking at the Jacobian of the composition of functions $h\circ g$. We have $J_z (h\circ g) = J_{g(z)} h\, J_z g = J_x h \,J_z g = I_{d\times d}$, since $h(g(z))=z$. This implies that the row span of $J_x h$ coincides with the column span of $J_z g$. As the columns of $J_z g$ span the tangent space $T_{g(z)}\mcal{M}$, so do the the rows of $J_x h$. 
\end{proof}\subsubsection{Training the inverse mapping (the encoder)}
To estimate the tangents for a given real data point $x\in X$, we need its corresponding latent representation $z=h(x) \in Z,$ such that $g(h(x))=x$ in an ideal scenario. However, in practice $g$ will only learn an approximation to the true data manifold, and the mapping $g\circ h$ will act like a projection of $x$ (which will almost always be off the manifold $\mcal{M}$) to the manifold $\mcal{M},$ yielding some approximation error. This projection may not be orthogonal, i.e., to the nearest point on $\mcal{M}$. Nevertheless, it is desirable that $x$ and $g(h(x))$ are semantically close, and at the very least, the class label is preserved by the mapping $g\circ h$. We studied the following three approaches for training the inverse map $h$, with regard to this desideratum : 
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*]
\item {\bf Decoupled training.~} This is similar to an approach outlined by \citet{donahue2016adversarial} where the generator is trained first and fixed thereafter, and the encoder is trained by optimizing a suitable reconstruction loss in the $Z$ space, $L(z,h(g(z)))$ (e.g., cross entropy, $\ell_2$). This approach does not yield good results and we observe that most of the time $g(h(x))$ is not semantically similar to the given real sample $x$ with change in the class label. One of the reasons as noted by \citet{donahue2016adversarial} is that the encoder never sees real samples during training. To address this, we also experimented with the combined objective $\min_h L_z(z,h(g(z)))+L_h(x,g(h(x)))$, however this too did not yield any significant improvements in our early explorations. 
\item {\bf BiGAN.~} \citet{donahue2016adversarial} propose to jointly train the encoder and generator using adversarial training, where the pair $(z,g(z))$ is considered a \emph{fake} example ($z\sim p_z$) and the pair $(h(x), x)$ is considered a \emph{real} example by the discriminator. A similar approach is proposed by \citet{dumoulin2016adversarially}, where $h(x)$ gives the parameters of the posterior $p(z|x)$ and a stochastic sample from the posterior paired with $x$ is taken as a real example. We use BiGAN \cite{donahue2016adversarial} in this work, with one modification: we use \emph{feature matching} loss \cite{salimans2016improved} (computed using features from an intermediate layer $\ell$ of the discriminator $f$), i.e., $\lVert \mbb{E}_x f_{\ell}(h(x),x) - \mbb{E}_z f_{\ell}(z,g(z))\rVert_2^2$, to optimize the generator and encoder, which we found to greatly help with the convergence \footnote{Note that other recently proposed methods for training GANs based on Integral Probability Metrics \cite{arjovsky2017wasserstein,gulrajani2017improved,mroueh2017mcgan,mroueh2017fisher} could also improve the convergence and stability during training.}. We observe better results in terms of semantic match between $x$ and $g(h(x))$ than in the decoupled training approach, however, we still observe a considerable fraction of instances where the class of $g(h(x))$ is changed (let us refer to this as \emph{class-switching}). 
\item {\bf Augmented-BiGAN.~} To address the still-persistent problem of \emph{class-switching} of the reconstructed samples $g(h(x))$, we propose to construct a third pair $(h(x), g(h(x))$ which is also considered by the discriminator as a \emph{fake} example in addition to $(z,g(z))$. Our Augmented-BiGAN objective is given as
\begin{align}
%\min_{g,h} \max_d\, 
\hspace{-2mm} \mbb{E}_{x\sim p_x} \log f(h(x),x) + \frac{1}{2}\mbb{E}_{z\sim p_z} \log (1-f(z,g(z))) + \frac{1}{2} \mbb{E}_{x\sim p_x} \log(1-f(h(x),g(h(x))),
\label{eq:augBigan}
\end{align}
where $f(\cdot,\cdot)$ is the probability of the pair being a real example, as assigned by the discriminator $f$. We optimize the discriminator using the above objective \eqref{eq:augBigan}. The generator and encoder are again optimized using \emph{feature matching} \cite{salimans2016improved} loss on an intermediate layer $\ell$ of the discriminator, i.e., $L_{gh} = \lVert \mbb{E}_x f_{\ell}(h(x),x) - \mbb{E}_z f_{\ell}(z,g(z))\rVert_2^2$, to help with the convergence. Minimizing $L_{gh}$ will make $x$ and $g(h(x))$ similar (through the lens of $f_{\ell}$) as in the case of BiGAN, however the discriminator tries to make the features at layer $f_{\ell}$ more difficult to achieve this by directly optimizing the third term in the objective~\eqref{eq:augBigan}. This results in improved semantic similarity between $x$ and $g(h(x))$.

%The third term in Objective~\eqref{eq:augBigan} influences the discriminator parameters which in turn result in better matching of $x$ and $g(h(x))$ through optimizing %Due to the additional third term in the Objective~\ref{eq:augBigan}, the discriminator will try to learn features that can differentiate between pairs $(h(x),x)$ and $(h(x),g(h(x)))$ 
\end{itemize}

We empirically evaluate these approaches with regard to similarity between $x$ and $g(h(x))$ both quantitatively and qualitatively, observing that Augmented-BiGAN works significantly better than BiGAN. We note that ALI~\cite{dumoulin2016adversarially} also has the problems of semantic mismatch and \emph{class switching} for reconstructed samples as reported by the authors, and a stochastic version of the proposed third term in the objective~\eqref{eq:augBigan} can potentially help there as well, investigation of which is left for future work.

\subsubsection{Estimating the dominant tangent space}
Once we have a trained encoder $h$ such that $g(h(x))$ is a good approximation to $x$ and $h(g(z))$ is a good approximation to $z$, we can use either $J_{h(x)} g$ or $J_xh$ to get an estimate of the tangent space. Specifically, the columns of $J_{h(x)} g$ and the rows of $J_x h$ are the directions that approximately span the tangent space to the data manifold at $x$. Almost all deep learning packages %\cite{tensorflow,theano,torch} 
implement reverse mode differentiation (to do backpropagation) which is computationally cheaper than forward mode differentiation for computing the Jacobian when the output dimension of the function is low (and vice versa when the output dimension is high). Hence we use $J_{x}h$ in all our experiments to get the tangents.

As there are approximation errors at several places ($\mcal{M}\sim $ data-manifold, $g(h(x))\sim x$, $h(g(z))\sim z$), it is preferable to only consider dominant tangent directions in the row span of $J_xh$. These can be obtained using the SVD on the matrix $J_x h$ and taking the right singular vectors corresponding to top singular values, as done in \cite{rifai2011manifold} where $h$ is trained using a contractive auto-encoder.  However, this process is expensive as the SVD needs to be done independently for every data sample. We adopt an alternative approach to get dominant tangent direction: we take the pre-trained model with encoder-generator-discriminator ($h$-$g$-$f$) triple and insert two extra functions $p:R^d\to R^{d_p}$ and $\bar{p}:R^{d_p}\to R^d$ (with $d_p < d$) which are learned by optimizing $\min_{p,\bar{p}} \mbb{E}_x [\lVert g(h(x))-g(\bar{p}(p(h(x))))\rVert_1 + \lVert f^{X}_{-1}(g(h(x))) - f^X_{-1}(g(\bar{p}(p(h(x)))))\rVert]$ while $g,h$ and $f$ are kept fixed from the pre-trained model. Note that our discriminator $f$ has two pipelines $f^Z$ and $f^X$ for the latent $z\in Z$ and the data $x\in X$, respectively, which share parameters in the last few layers (following \cite{donahue2016adversarial}), and we use the last layer of $f^X$ in this loss. This enables us to learn a nonlinear (low-dimensional) approximation in the $Z$ space such that $g(\bar{p}(p(h(x))))$ is close to $g(h(x))$. We use the Jacobian of $p\circ h$, $J_x\, p\circ h$, as an estimate of the $d_p$ dominant tangent directions ($d_p=10$ in all our experiments)\footnote{Training the GAN with $z\in Z \subset \mbb{R}^{d_p}$ results in a bad approximation of the data manifold. Hence we first learn the GAN with $Z\subset\mbb{R}^d$ and then approximate the smooth manifold $\mcal{M}$ parameterized by the generator using $p$ and $\bar{p}$ to get the dominant $d_p$ tangent directions to $\mcal{M}$.}. 

\subsection{Injecting invariances into the classifier using tangents}\label{subsec:invar}
We use the \emph{tangent propagation} approach (TangentProp) \cite{simard1998transformation} to make the classifier invariant to the estimated tangent directions from the previous section. Apart form the regular classification loss on labeled examples, it uses a regularizer of the form $\sum_{i=1}^n \sum_{v\in T_{x_i} }\lVert (J_{x_i} c) \, v\rVert_2^2$, where $J_{x_i} c\in\mbb{R}^{k\times D}$ is the Jacobian of the classifier function $c$ at $x=x_i$ (with the number of classes $k$). and $T_x$ is the set of tangent directions we want the classifier to be invariant to. This term penalizes the \emph{linearized} variations of the classifier output along the tangent directions. \citet{simard1998transformation} get the tangent directions using slight rotations and translations of the images, whereas we use the GAN to estimate the tangents to the data manifold. 

We can go one step further and make the classifier invariant to small perturbations in \emph{all} directions emanating from a point $x$. This leads to the regularizer
\begin{align}
\sup_{v:\lVert v\rVert_p \leq \epsilon}\, \lVert (J_xc)\, v \rVert_j^j \leq \sum_{i=1}^k \sup_{v:\lVert v\rVert_p \leq \epsilon} \lvert (J_xc)_{i:}\, v \rvert^j = \epsilon^j \sum_{i=1}^k  \lVert (J_xc)_{i:}\rVert_q^j,  
\label{eq:jacobreg}
\end{align}\noindent where $\lVert\cdot\rVert_q$ is the dual norm of $\lVert\cdot\rVert_p$ (i.e., $\frac{1}{p}+\frac{1}{q}=1$), and $\lVert\cdot\rVert_j^j$ denotes $j$th power of $\ell_j$-norm. This reduces to squared Frobenius norm of the Jacobian matrix $J_xc$ for $p=j=2$. The penalty in Eq.~\eqref{eq:jacobreg} is closely related to the recent work on \emph{virtual adversarial training} (VAT) \cite{miyato2015distributional} which uses a regularizer (ref. Eq (1), (2) in \cite{miyato2015distributional})
\begin{align}
\sup_{v:\lVert v\rVert_2 \leq \epsilon} \text{KL} [ c(x) || c(x+v)], 
\label{eq:vatreg}
\end{align}\noindent where $c(x)$ are the classifier outputs (class probabilities). VAT\cite{miyato2015distributional} approximately estimates $v^*$ that yields the $\sup$ using the gradient of $\text{KL} [ c(x) || c(x+v)]$, calling $(x+v^*)$ as \emph{virtual adversarial example} (due to its resemblance to \emph{adversarial training}\cite{goodfellow2014explaining}), and uses $\text{KL} [ c(x) || c(x+v^*)]$ as the regularizer in the classifier objective. If we replace KL-divergence in Eq.~\ref{eq:vatreg} with total-variation distance and optimize its first-order approximation, it becomes equivalent to the regularizer in Eq.~\eqref{eq:jacobreg} for $j=1$ and $p=2$.  

In practice, it is computationally expensive to optimize these Jacobian based regularizers. Hence in all our experiments we use stochastic finite difference approximation for all Jacobian based regularizers. For TangentProp, we use $\lVert c(x_i+v) - c(x_i)\rVert_2^2$ with $v$ randomly sampled (i.i.d.) from the set of tangents $T_{x_i}$ every time example $x_i$ is visited by the SGD. For Jacobian-norm regularizer of Eq.~\eqref{eq:jacobreg}, we use $\lVert c(x+\delta) - c(x)\rVert_2^2$ with $\delta\sim N(0,\sigma^2 I)$ (i.i.d) every time an example $x$ is visited by the SGD, which approximates an upper bound on Eq.~\eqref{eq:jacobreg} in expectation (up to scaling) for $j=2$ and $p=2$. 

\subsection{GAN discriminator as the classifier for semi-supervised learning: effect of fake examples}

Recent works have used GANs for semi-supervised learning where the discriminator also serves as a classifier \cite{salimans2016improved,dumoulin2016adversarially,odena2016semi}. For a semi-supervised learning problem with $k$ classes, the discriminator has $k+1$ outputs with the $(k+1)$'th output corresponding to the \emph{fake} examples originating from the generator of the GAN. The loss for the discriminator $f$ is given as \cite{salimans2016improved}\begin{align}
\begin{split}
& L^f = L^f_{\text{sup}} + L^f_{\text{unsup}}, \text{where } L^f_{\text{sup}} = -\mbb{E}_{(x,y)\sim p_d(x,y)} \log p_{f}(y|x, y\leq k) \\
& \text{and } L^f_{\text{unsup}} =  -\mbb{E}_{x\sim p_g(x)} \log (p_f(y=k+1|x)) - \mbb{E}_{x\sim p_d(x)} \log (1- p_f(y=k+1|x))).
\end{split}
\label{eq:semisupganloss}
\end{align}
The term $p_f(y=k+1|x)$ is the probability of $x$ being a fake example and $(1-p_f(y=k+1|x))$ is the probability of $x$ being a real example (as assigned by the model). The loss component $L^f_{\text{unsup}}$ is same as the regular GAN discriminator loss with the only modification that probabilities for real vs. fake are compiled from $(k+1)$ outputs. \citet{salimans2016improved} proposed training the generator using \emph{feature matching} where the generator minimizes the mean discrepancy between the features for real and fake examples obtained from an intermediate layer $\ell$ of the discriminator $f$, i.e., $L^g = \lVert \mbb{E}_x f_{\ell}(x) - \mbb{E}_z f_{\ell}(g(z))\rVert_2^2$. Using feature matching loss for the generator was empirically shown to result in much better accuracy for semi-supervised learning compared to other training methods including \emph{minibatch discrimination} and regular GAN generator loss \cite{salimans2016improved}. 

Here we attempt to develop an intuitive understanding of how fake examples influence the learning of the classifier and why feature matching loss may work much better for semi-supervised learning compared to regular GAN. We will use the term classifier and discriminator interchangeably based on the context however they are really the same network as mentioned earlier. Following \cite{salimans2016improved} we assume the $(k+1)$'th logit is fixed to $0$ as subtracting a term $v(x)$ from all logits does not change the \emph{softmax} probabilities. Rewriting the unlabeled loss of Eq.~\eqref{eq:semisupganloss} in terms of \emph{logits}$l_i(x),\,i=1,2,\ldots,k$, we have
\begin{align}
L^f_\text{unsup} = \mbb{E}_{x_g\sim p_g} \log \left(1+\sum_{i=1}^k e^{l_i(x_g)}\right) - \mbb{E}_{x\sim p_d} \left[\log \sum_{i=1}^k e^{l_i(x)} - \log \left(1+\sum_{i=1}^{k} e^{l_i(x)}\right) \right] 
\end{align}
Taking the derivative w.r.t. discriminator's parameters $\theta$ followed by some basic algebra, we get $\nabla_\theta L^f_\text{unsup} =$\begin{align}
& \underset{x_g\sim p_g}{\mbb{E}} \sum_{i=1}^k p_f(y=i|x_g) \nabla l_i(x_g) - \underset{x\sim p_d}{\mbb{E}}  \left[ \sum_{i=1}^k p_f(y=i|x,y\leq k) \nabla l_i(x) -  \sum_{i=1}^k p_f(y=i|x) \nabla l_i(x) \right] \nonumber \\ 
&= \underset{x_g\sim p_g}{\mbb{E}} \sum_{i=1}^k \underbrace{p_f(y=i|x_g)}_{a_i(x_g)} \nabla l_i(x_g) - \underset{x\sim p_d}{\mbb{E}} \sum_{i=1}^k \underbrace{p_f(y=i|x,y\leq k) p_f(y=k+1|x)}_{b_i(x)}\nabla l_i(x) 
\label{eq:unsupgrad}
\end{align}
Minimizing $L^f_\text{unsup}$ will move the parameters $\theta$ so as to decrease $l_i(x_g)$ and increase $l_i(x)$ ($i=1,\ldots,k$). The rate of increase in $l_i(x)$ is also modulated by $p_f(y=k+1|x)$. This results in warping of the functions $l_i(x)$ around each real example $x$ with more warping around examples about which the current model $f$ is more confident that they belong to class $i$: $l_i(\cdot)$ becomes locally concave around those real examples $x$ if $x_g$ are loosely scattered around $x$. Let us consider the following three cases: 

{\bf Weak fake examples.~} When the fake examples coming from the generator are very \emph{weak} (i.e., very easy for the current discriminator to distinguish from real examples), we will have $p_f(y=k+1|x_g)\approx 1$, $p_f(y=i|x_g)\approx 0$ for $1\leq i\leq k$ and $p_f(y=k+1|x)\approx 0$. Hence there is no gradient flow from Eq.~\eqref{eq:unsupgrad}, rendering unlabeled data almost useless for semi-supervised learning. 

{\bf Strong fake examples.~} When the fake examples are very \emph{strong} (i.e., difficult for the current discriminator to distinguish from real ones), we have $p_f(k+1|x_g)\approx 0.5+\epsilon_1$, $p_f(y=i_\text{max}|x_g)\approx 0.5-\epsilon_2$ for some $i_\text{max}\in \{1,\ldots,k\}$ and $p_f(y=k+1|x)\approx 0.5-\epsilon_3$ (with $\epsilon_2 > \epsilon_1\geq 0$ and $\epsilon_3\geq 0$). Note that $b_i(x)$ in this case would be smaller than $a_i(x)$ since it is a product of two probabilities. If two examples $x$ and $x_g$ are close to each other with $i_\text{max} = \argmax_i l_i(x) = \argmax_i l_i(x_g)$ (e.g., $x$ is a \emph{cat} image and $x_g$ is a highly realistic generated image of a \emph{cat}), the optimization will push $l_{i_\text{max}}(x)$ up by some amount and will pull $l_{i_\text{max}}(x_g)$ down by a larger amount. We further want to consider two cases here: {\bf (i) Classifier with enough capacity:} If the classifier has enough capacity, this will make the curvature of $l_{i_\text{max}}(\cdot)$ around $x$ really high (with $l_{i_\text{max}}(\cdot)$ locally concave around $x$) since $x$ and $x_g$ are very close. This results in over-fitting around the unlabeled examples and for a test example $x_t$ closer to $x_g$ (which is quite likely to happen since $x_g$ itself was very realistic sample), the model will more likely misclassify $x_t$. 
{\bf (ii) Controlled-capacity classifier: } Suppose the capacity of the classifier is controlled with adequate regularization. In that case the curvature of the function $l_{i_\text{max}}(\cdot)$ around $x$ cannot increase beyond a point. However, this results in $l_{i_\text{max}}(x)$ being pulled down by the optimization process since $a_i(x_g) > b_i(x)$. This is more pronounced for examples $x$ on which the classifier is not so confident (i.e., $p_f(y=i_\text{max}|x,y\leq k)$ is low, although still assigning highest probability to class $i_\text{max}$) since the gap between $a_i(x_g)$ and $b_i(x)$ becomes higher. For these examples, the entropy of the distribution $\{p(y=i|x,y\leq k)\}_{i=1}^k$ may actually \emph{increase} as the training proceeds which can hurt the test performance. 

{\bf Moderate fake examples.~} When the fake examples from the generator are neither too weak nor too strong for the current discriminator (i.e., $x_g$ is a somewhat distorted version of $x$), the unsupervised gradient will push $l_{i_\text{max}}(x)$ up while pulling $l_{i_\text{max}}(x_g)$ down, giving rise to a moderate curvature of $l_i(\cdot)$ around real examples $x$ since $x_g$ and $x$ are sufficiently far apart (consider multiple distorted \emph{cat} images scattered around a real \emph{cat} image at moderate distances). This results in a smooth decision function around real unlabeled examples. Again, the curvatures of $l_i(\cdot)$ around $x$ for classes $i$ which the current classifier does not trust for the example $x$ are not affected much. 
Further, $p_f(y=k+1|x)$ will be less than the case when fake examples are very strong. Similarly $p_f(y=i_\text{max}|x_g)$ (where $i_\text{max}= \argmax_{1\leq i\leq k} l_i(x_g)$) will be less than the case of strong fake examples. Hence the norm of the gradient in Eq.~\eqref{eq:unsupgrad} is lower and the contribution of unlabeled data in the overall gradient of $L^f$ (Eq.~\eqref{eq:semisupganloss} is lower than the case of strong fake examples. This intuitively seems beneficial as the classifier gets ample opportunity to learn on supervised loss and get confident on the right class for unlabeled examples, and then boost this confidence slowly using the gradient of Eq.~\eqref{eq:unsupgrad} as the training proceeds.

We experimented with regular GAN loss (i.e., $L^g = \mbb{E}_{x\sim p_g} \log (p_f(y=k+1|x)))$, and feature matching loss for the generator \cite{salimans2016improved}, plotting  several of the quantities of interest discussed above for MNIST (with $100$ labeled examples) and SVHN (with $1000$ labeled examples) datasets in Fig.\ref{fig:analysis}. Generator trained with feature matching loss corresponds to the case of \emph{moderate fake examples} discussed above (as it generates blurry and distorted samples as mentioned in \cite{salimans2016improved}). Generator trained with regular GAN loss corresponds to the case of \emph{strong fake examples} discussed above. We plot $\mbb{E}_{x_g} a_{i_\text{max}}(x_g)$ for $i_\text{max}=\argmax_{1\leq i\leq k} l_i(x_g)$ and $\mbb{E}_{x_g} [\frac{1}{k-1}\sum_{1\leq i\neq i_\text{max}\leq k} a_{i}(x_g)]$ separately to look into the behavior of $i_\text{max}$ logit. Similarly we plot $\mbb{E}_{x} b_{t}(x)$ separately where $t$ is the true label for unlabeled example $x$ (we assume knowledge of the true label only for plotting these quantities and not while training the semi-supervised GAN). Other quantities in the plots are self-explanatory. As expected, the unlabeled loss $L^f_\text{unsup}$ for regular GAN becomes quite high early on implying that fake examples are strong. The gap between $a_{i_\text{max}}(x_g)$ and $b_t(x)$ is also higher for regular GAN pointing towards the case of \emph{strong fake examples with controlled-capacity classifier} as discussed above. Indeed, we see that the average of the entropies for the distributions $p_f(y|x)$ (i.e., $\mbb{E}_x H(p_f(y|x,y\leq k))$) is much lower for feature-matching GAN compared to regular GAN (seven times lower for SVHN, ten times lower for MNIST). Test errors for MNIST for regular GAN and FM-GAN were $2.49\%$  ($500$ epochs) and $0.86\%$ ($300$ epochs), respectively. Test errors for SVHN were $13.36\%$ (regular-GAN at $738$ epochs) and $5.89\%$ (FM-GAN at $883$ epochs), respectively\footnote{We also experimented with minibatch-discrimination (MD) GAN\cite{salimans2016improved} but the minibatch features are not suited for classification as the prediction for an example $x$ is adversely affected by features of all other examples (note that this is different from batch-normalization). Indeed we notice that the training error for MD-GAN is $10$x that of regular GAN and FM-GAN. MD-GAN gave similar test error as regular-GAN.}.  It should also be emphasized that the semi-supervised learning heavily depends on the generator dynamically adapting fake examples to the current discriminator -- we observed that freezing the training of the generator at any point results in the discriminator being able to classify them easily (i.e., $p_f(y=k+1|x_g)\approx 1$) thus stopping the contribution of unlabeled examples in the learning. 

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.23\textwidth}
        \centering
        \includegraphics[scale=0.32]{MNIST__Feature-Matching_GAN_entropy}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.23\textwidth}
        \centering
        \includegraphics[scale=0.32]{MNIST__Feature-Matching_GAN_probs}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.23\textwidth}
        \centering
        \includegraphics[scale=0.32]{SVHN__Feature-Matching_GAN_entropy}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.23\textwidth}
        \centering
        \includegraphics[scale=0.32]{SVHN__Feature-Matching_GAN_probs}
    \end{subfigure}
    
    \begin{subfigure}[t]{0.23\textwidth}
        \centering
        \includegraphics[scale=0.32]{MNIST__regular_GAN_entropy}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.23\textwidth}
        \centering
        \includegraphics[scale=0.32]{MNIST__regular_GAN_probs}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.23\textwidth}
        \centering
        \includegraphics[scale=0.32]{SVHN__regular_GAN_entropy}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.23\textwidth}
        \centering
        \includegraphics[scale=0.32]{SVHN__regular_GAN_probs}
    \end{subfigure}
    
    \caption{Plots of Entropy, $L^f_\text{unsup}$ (Eq.~\eqref{eq:semisupganloss}), $a_i(x_g)$, $b_i(x)$ and other probabilities (Eq.~\eqref{eq:unsupgrad}) for regular GAN generator loss and feature-matching GAN generator loss.}
    \label{fig:analysis}
\end{figure*}{\bf Our final loss for semi-supervised learning.~} We use feature matching GAN with semi-supervised loss of Eq.~\eqref{eq:semisupganloss} as our classifier objective and incorporate  invariances from Sec.~\ref{subsec:invar} in it. Our final objective for the GAN discriminator is
\begin{align}
L^f = L^f_\text{sup} + L^f_\text{unsup} + \lambda_1 \mbb{E}_{x\sim p_d(x)} \sum_{v\in T_{x}}\lVert (J_{x} f) \, v\rVert_2^2 + \lambda_2 \mbb{E}_{x\sim p_d(x)} \lVert J_x f\rVert_F^2.
\label{eq:finalloss}
\end{align}
The third term in the objective makes the classifier decision function change slowly along tangent directions around a real example $x$. 
As mentioned in Sec.~\ref{subsec:invar} we use stochastic finite difference approximation for both Jacobian terms due to computational reasons.

%Problem with encoder/generator in bigan - reconstructed samples may change class%two modifications to bigan/ALI training (subjective and quantitative results)%Tangent regularizer from TangentProp, its pproximation (Accuracy numbers)%Adversarial examples - sup over the tangents - Jacobian regularizer (Accuracy numbers)%connection to VAT paper%Semi-sup learning with GAN: the framework of 'fake' label; cite earlier work that uses this (improvedGAN, others)%Insights into workings of improved GAN (Plots for feature matching, minibatch disc)

% !TeX spellcheck = en_US\vspace{-2mm}\section{Experiments}\vspace{-2mm}\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.5in]{cifar10_ori_vs_augmented_recon_test_err}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=1.5in]{svhn_ori_vs_augmented_recon_test_err}
    \end{subfigure}
    
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=0.8in]{cifar_recons_ori_vs_mapped}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=0.8in]{svhn_recons_ori_vs_mapped}
    \end{subfigure}
    \caption{Comparing BiGAN with Augmented BiGAN based on the classification error on the reconstructed test images. \emph{Left column:} CIFAR10, \emph{Right column:} SVHN. In the images, the top row corresponds to the original images followed by BiGAN reconstructions in the middle row and the Augmented BiGAN reconstructions in the bottom row. More images can be found in the appendix.}
    \label{recons}
\end{figure*}{\bf Implementation Details.~} The architecture of the endoder, generator and discriminator closely follow the network structures in ALI \cite{dumoulin2016adversarially}. We remove the stochastic layer from the ALI encoder (i.e., $h(x)$ is deterministic). For estimating the dominant tangents,  we employ fully connected two-layer network with $tanh$ non-linearly in the hidden layer to represent $p\circ \bar{p}$. The output of $p$ is taken from the hidden layer. Batch normalization was replaced by weight normalization in all the modules to make the output $h(x)$ (similarly $g(z)$) dependent only on the given input $x$ (similarly $z$) and not on the whole minibatch. This is necessary to make the Jacobians $J_x h$ and $J_z g$ independent of other examples in the minibatch. We replaced all ReLU nonlinearities in the encoder and the generator with the Exponential Linear Units (ELU) \cite{clevert2015fast} to ensure smoothness of the functions $g$ and $h$. We follow \cite{salimans2016improved} completely for optimization (using ADAM optimizer \cite{kingma2014adam} with the same learning rates as in \cite{salimans2016improved}). %Following the feature matching GAN implementation\cite{salimans2016improved}, we maintain a running average of the discriminator updates and use them to perform weight updates. Also, 
Generators (and encoders, if applicable) in all the models are trained using feature matching loss.

{\bf Semantic Similarity.~} The image samples ${x}$ and their reconstructions ${g(h(x))}$ for BiGAN and Augemented-BiGAN can be seen in Fig. \ref{recons}. To quantitatively measure the semantic similarity of the reconstructions to the original images, we learn a supervised classifier using the full training set and obtain the classification accuracy on the reconstructions of the test images. The architectures of the classifier for CIFAR10 and SVHN are similar to their corresponding GAN discriminator architectures we have. The lower error rates with our Augmented-BiGAN suggest that it leads to reconstructions with reduced \emph{class-switching}. 

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[height=0.8in]{cifar_tangents_2_rows}
    \end{subfigure}
    
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[height=0.8in]{svhn_tangents_2_rows}
    \end{subfigure}
    \caption{Visualizing tangents. \emph{Top:} CIFAR10, \emph{Bottom: SVHN}. \emph{Odd rows:} Tangents using our method for estimating the dominant tangent space. \emph{Even rows:} Tangents using SVD on $J_{h(x)} g$ and $J_x h$. \emph{First column:} Original image. \emph{Second column:} Reconstructed image using $g \circ h$. \emph{Third column:} Reconstructed image using $g\circ \bar{p} \circ p \circ h$. \emph{Columns 4-13:} Tangents using encoder. \emph{Columns 14-23:} Tangents using generator.}
    \label{tans}
    \vspace{-4mm}
\end{figure}\begin{table}[t]
\centering
\begin{tabular}{c c c c  c  }
\toprule
\multirow{ 2}{*}{Model} & \multicolumn{2}{c}{~~~SVHN~~~}  & \multicolumn{2}{c}{CIFAR-10} \\
& {$N_l = 500$} & {$N_l=1000$} & {$N_l=1000$} & {$N_l=4000$} \\
\midrule
VAE (M1+M2) \cite{kingma2014semi} & --  & $36.02\pm 0.10$ & -- & --  \\
SWWAE with dropout \cite{zhao2015stacked} & --  & $23.56$ & -- & --  \\
VAT \cite{miyato2015distributional} & --  & 24.63 & -- & --  \\
Skip DGM \cite{maaloe2016auxiliary} & --  & $16.61\pm 0.24$ & -- & --  \\
Ladder network \cite{rasmus2015semi} & -- & -- & -- & $20.40$ \\
ALI \cite{dumoulin2016adversarially} &  -- & $7.41 \pm 0.65$ & $19.98\pm 0.89$ & $17.99\pm 1.62$  \\
FM-GAN \cite{salimans2016improved} & $18.44\pm 4.8$  & $8.11\pm 1.3$ & $21.83\pm 2.01$ & $18.63 \pm 2.32$  \\
Temporal ensembling \cite{laine2016temporal} & $5.12\pm 0.13$ & $4.42\pm 0.16$ & -- & ${\bf 12.16}\pm 0.24$ \\
FM-GAN + Jacob.-reg (Eq.~\eqref{eq:jacobreg}) & $10.28\pm 1.8$  & $4.74\pm 1.2$ & $20.87\pm 1.7$ &  $16.84\pm 1.5$ \\
FM-GAN + Tangents  & $5.88\pm 1.5$  & $5.26\pm 1.1$  & $20.23\pm 1.3$ & $16.96 \pm 1.4$  \\
FM-GAN + Jacob.-reg + Tangents  & ${\bf 4.87} \pm 1.6$   & ${\bf 4.39} \pm 1.2$ & ${\bf 19.52}\pm 1.5$ & ${16.20}\pm 1.6$  \\

\bottomrule
\end{tabular}
\vspace{2mm}
\caption{Test error with semi-supervised learning on SVHN and CIFAR-10 ($N_l$ is the number of labeled examples). All results for the proposed methods (last 3 rows) are obtained with training the model for $600$ epochs for SVHN and $900$ epochs for CIFAR10, and are averaged over $5$ runs. % with error-bars $=3 \sigma$. %ALI results for CIFAR10 are taken from \cite{dumoulin2016adversarially} (trained for $6475$ epochs).
}
\label{tab:semisup}
\vspace{-2mm}
\end{table}\begin{table}[t]
	\centering
	\begin{tabular}{c c c c  c c c c c c c c }
		\toprule
		& $d(\mcal{S}_1,\mcal{S}_2)$ & $\theta_1$ & $\theta_2$ & $\theta_3$ & $\theta_4$ & $\theta_5$ & $\theta_6$ & $\theta_7$ & $\theta_8$ & $\theta_9$ & $\theta_{10}$ \\
		\midrule
	Rand-Rand & 4.5 & 14 &  83 &  85 &  86 &  87 &  87 &  88 &  88 &  88 &  89 \\
	SVD-Approx. (CIFAR)& 2.6 & 2& 15 & 21& 26& 34& 40& 50& 61& 73& 85 \\
	SVD-Approx. (SVHN)& 2.3 & 1& 7&  12& 16& 22& 30& 41& 51& 67& 82 \\
	
		\bottomrule
	\end{tabular}
	\vspace{2mm}
	\caption{Dominant tangent subspace approximation quality: Columns show the geodesic distance and $10$ principal angles between the two subspaces. \emph{Top row} shows results for two randomly sampled $10$-dimensional subspaces in $3072$-dimensional space,  \emph{middle} and \emph{bottom} rows show results for dominant subspace obtained using SVD of $J_{x}h$ and dominant subspace obtained using our method,  for CIFAR-10 and SVHN, respectively. All numbers are averages $10$ randomly sampled test examples.  
	}
	\label{tab:tanapprox}
	\vspace{-4mm}
\end{table}{\bf Tangent approximations.~} Tangents for CIFAR10 and SVHN are shown in Fig. \ref{tans}. We show visual comparison of tangents from $J_x (p\circ h)$, from $J_{p(h(x))} g\circ \bar{p}$, and from $J_x h$ and $J_{h(x)} g$  followed by the SVD to get the dominant tangents. It can be seen that the proposed method for getting dominant tangent directions gives similar tangents as SVD. The tangents from the generator (columns 14-23) look  different (more colorful) from the tangents from the encoder (columns 4-13) though they do trace the boundaries of the objects in the image (just like the tangents from the encoder). %This can be due to the fact that $g$ and $h$ are not exact inverse of each other and the manifold $\mcal{M}$ parameterized by $g$ does not match the data manifold well. 
We also empirically quantify our method for dominant tangent subspace estimation against the SVD estimation by computing the geodesic distances and principal angles between these two estimations. These results are shown in Table \ref{tab:tanapprox}.  

{\bf Semi-supervised learning results.}
Table \ref{tab:semisup} shows the results for SVHN and CIFAR10 with various number of labeled examples. For all experiments with the tangent regularizer for both CIFAR10 and SVHN, we use $10$ tangents. The hyperparameters $\lambda_1$ and $\lambda_2$ in Eq.~\eqref{eq:finalloss} are set to $1$. We obtain significant improvements over baselines, particularly for SVHN and more so for the case of $500$ labeled examples. We do not get as good results on CIFAR10 which may be due to the fact that our encoder for CIFAR10 is still not able to approximate the inverse of the generator well (which is evident from the sub-optimal reconstructions we get for CIFAR10) and hence the tangents we get are not good enough. We think that obtaining better estimates of tangents for CIFAR10 has the potential for further improving the results. ALI \cite{dumoulin2016adversarially} accuracy for CIFAR ($N_l=1000$) is also close to  our results however ALI results were obtained by running the optimization for $6475$ epochs with a slower learning rate as mentioned in \cite{dumoulin2016adversarially}. Temporal ensembling \cite{laine2016temporal} using explicit data augmentation assuming knowledge of the class-preserving transformations on the input, while our method estimates these transformations from the data manifold in the form of tangent vectors. It outperforms our method by a significant margin on CIFAR-10 which could be due the fact that it uses horizontal flipping based augmentation for CIFAR-10 which cannot be learned through the tangents as it is a non-smooth transformation. The use of temporal ensembling in conjunction with our method has the potential of further improving the semi-supervised learning results. 



\vspace{-2mm}\section{Discussion}\vspace{-2mm}
Our empirical results show that using the tangents of the data manifold (as estimated by the generator of the GAN) to inject invariances in the classifier improves the performance on semi-supevised learning tasks. In particular we observe impressive accuracy gains on SVHN (more so for the case of $500$ labeled examples) for which the tangents obtained are good quality. We also observe improvements on CIFAR10 though not as impressive as SVHN. We think that improving on the quality of tangents for CIFAR10 has the potential for further improving the results there, which is a direction for future explorations. We also shed light on the effect of fake examples in the common framework used for semi-supervised learning with GANs where the discriminator predicts \emph{real} class labels along with the \emph{fake} label. Explicitly controlling the difficulty level of fake examples (i.e., $p_f(y=k+1|x_g)$ and hence indirectly $p_f(y=k+1|x)$ in Eq.~\eqref{eq:unsupgrad}) to do more effective semi-supervised learning is another direction for future work. One possible way to do this is to have a distortion model for the real examples (i.e., replace the \emph{generator} with a \emph{distorter} that takes as input the real examples) whose strength is controlled for more effective semi-supervised learning. 


\clearpage
\bibliography{ml}
\bibliographystyle{plainnat}
\newpage
\appendix
\begin{center}{\Large \bf Appendix }\end{center}\punt{
\section{Algorithm outline}

\begin{algorithm}[H]
		\KwData{Labeled examples $\{x_i,y_i\}_{i=1}^{N_l}$, unlabeled examples $\{x_i\}_{i=1}^{N_u}$}
	\begin{algorithmic}[1]

	
	\STATE \textbf{Train the Augmented-BiGAN} by optimizing the objective in Eq. \eqref{eq:augBigan} to learn the generator $g$, encoder $h$ and discriminator $f$. %      This gives an encoder $h(.)$ that maps data sample $x \in X$ to latent representation   $z \in Z$ as $z = h(x)$, a generator $g(.)$ that maps the latent representation back to    the data manifold as $x = g(z)$ and a discriminator $f(.)$ that maps the generated     image to $k+1$ classes.
	 \STATE \textbf{Estimate the dominant tangent directions} by learning a nonlinear (low-dimensional)      approximation in the $Z$-space such that $g(\bar{p}(p(h(x))))$ is close to $g(h(x))$, where     $p:R^d\to R^{d_p}$ and $\bar{p}:R^{d_p}\to R^d$ (with $d_p < d$). These are learned by optimizing      $\min_{p,\bar{p}} \mbb{E}_x [\lVert g(h(x))-g(\bar{p}(p(h(x))))\rVert_1 + \lVert f^{X}_{-1}(g(h(x))) - f^X_{-1}(g(\bar{p}(p(h(x)))))\rVert]$      while $g,h$ and $f$ are kept fixed from the pre-trained model. 
	%    Discriminator $f$ has two pipelines $f^Z$ and $f^X$ for the latent $z$ and the data $x$, \\      respectively, which share parameters in the last few layers, and the last layer of $f^X$ is used in this loss.  \\
	    Use the Jacobian of $p\circ h$, $J_x\, p\circ h$, as an estimate of the $d_p$ dominant tangent directions. 
	\STATE   - \textbf{Train Classifier using TangentProp}: Apart form the regular classification loss,      add the following terms based on stoachistoc approximation of Jacobians: 
	     $\lVert c(x_i+v) - c(x_i)\rVert_2^2$ with $v$ randomly sampled (i.i.d.) from the set of tangents $T_{x_i}$,
	    and $\lVert c(x+\delta) - c(x)\rVert_2^2$ with $\delta\sim N(0,\sigma^2 I)$ (i.i.d), to achieve invariance in tangent      directions and small random perturbations, respectively.
		\end{algorithmic}
	\caption{Outline of the proposed Semi-supervised learning framework.}
\end{algorithm}

\begin{table}[h]
	\centering
	\caption{Outline of the proposed Semi-supervised learning framework.}
	\begin{tabular}{|l|}
		\hline
		\textbf{Input} \\
		- Data matrix, $X \in \mbb{R}^{N \times D}$  \\
		- Class labels, $Y \in \mbb{R}^{N \times k}$ \\ \\
		\textbf{Algorithm}\\
		\quad - \textbf{Train the Augmented-BiGAN}: that optimizes the objective in equation \ref{eq:augBigan}. \\ \quad \quad  This gives an encoder $h(.)$ that maps data sample $x \in X$ to latent representation \\ \quad \quad $z \in Z$ as $z = h(x)$, a generator $g(.)$ that maps the latent representation back to \\ \quad \quad the data manifold as $x = g(z)$ and a discriminator $f(.)$ that maps the generated \\ \quad \quad image to $k+1$ classes.\\
		\quad - \textbf{Obtain the dominant tangent directions}: by learning a nonlinear (low-dimensional) \\ \quad \quad approximation in the $Z$ space such that $g(\bar{p}(p(h(x))))$ is close to $g(h(x))$, where \\ \quad \quad $p:R^d\to R^{d_p}$ and $\bar{p}:R^{d_p}\to R^d$ (with $d_p < d$). These are learned by optimizing \\ \quad \quad  $\min_{p,\bar{p}} \mbb{E}_x [\lVert g(h(x))-g(\bar{p}(p(h(x))))\rVert_1 + \lVert f^{X}_{-1}(g(h(x))) - f^X_{-1}(g(\bar{p}(p(h(x)))))\rVert]$ \\ \quad \quad while $g,h$ and $f$ are kept fixed from the pre-trained model. \\ 
		%\quad \quad Discriminator $f$ has two pipelines $f^Z$ and $f^X$ for the latent $z$ and the data $x$, \\ \quad \quad  respectively, which share parameters in the last few layers, and the last layer of $f^X$ is used in this loss.  \\
		\quad \quad Use the Jacobian of $p\circ h$, $J_x\, p\circ h$, as an estimate of the $d_p$ dominant tangent directions. \\
		\quad - \textbf{Train Classifier using TangentProp}: Apart form the regular classification loss, \\ \quad \quad add the following terms based on stoachistoc approximation of Jacobians: \\
		\quad \quad  $\lVert c(x_i+v) - c(x_i)\rVert_2^2$ with $v$ randomly sampled (i.i.d.) from the set of tangents $T_{x_i}$,\\ 
		\quad \quad and $\lVert c(x+\delta) - c(x)\rVert_2^2$ with $\delta\sim N(0,\sigma^2 I)$ (i.i.d), to achieve invariance in tangent \\ \quad \quad directions and small random perturbations, respectively.\\
		\hline
	\end{tabular}
	\label{Table:algorithm}
\end{table}
}%\newpage\section{Tangent Plots}\begin{figure}[!h]
	\centering
    \includegraphics[width=\textwidth]{cifar_tangents}
    \caption{CIFAR10 tangents. \emph{Odd rows:} Tangents using our method for estimating the dominant tangent space. \emph{Even rows:} Tangents using SVD on $J_{h(x)} g$ and $J_x h$. \emph{First column:} Original image. \emph{Second column:} Reconstructed image using $g \circ h$. \emph{Third column:} Reconstructed image using $g\circ \bar{p} \circ p \circ h$. \emph{Columns 4-13:} Tangents using encoder. \emph{Columns 14-23:} Tangents using generator.}
\end{figure}\begin{figure}[!h]
	\centering
    \includegraphics[width=\textwidth]{svhn_tangents}
    \caption{SVHN tangents. \emph{Odd rows:} Tangents using our method for estimating the dominant tangent space. \emph{Even rows:} Tangents using SVD on $J_{h(x)} g$ and $J_x h$. \emph{First column:} Original image. \emph{Second column:} Reconstructed image using $g \circ h$. \emph{Third column:} Reconstructed image using $g\circ \bar{p} \circ p \circ h$. \emph{Columns 4-13:} Tangents using encoder. \emph{Columns 14-23:} Tangents using generator.}

\end{figure}\newpage\section{Reconstruction Plots}\begin{figure}[h]
	\centering
    \includegraphics[width=\textwidth]{cifar_recons_ori_vs_mapped_large}
    \caption{CIFAR10 reconstructions: Comparing BiGAN reconstructions with Augmented-BiGAN reconstructions. For $i=0,1,\ldots,4$, $(3i+1)$th row shows the original images, followed by BiGAN reconstructions in the $(3i+2)$'th row, and the Augmented-BiGAN reconstructions in the $(3i+3)$'th row. Reconstructions shown for total $100$ randomly sampled images from the test set ($10\times 5=50$ images each in the left and right column) from the test set.}
\end{figure}\begin{figure}[h]
	\centering
    \includegraphics[width=\textwidth]{svhn_recons_ori_vs_mapped_large}
    \caption{SVHN reconstructions: Comparing BiGAN reconstructions with Augmented-BiGAN reconstructions. For $i=0,1,\ldots,4$, $(3i+1)$th row shows the original images, followed by BiGAN reconstructions in the $(3i+2)$'th row, and the Augmented-BiGAN reconstructions in the $(3i+3)$'th row. Reconstructions shown for total $100$ randomly sampled images from the test set ($10\times 5=50$ images each in the left and right column).}
\end{figure}
\end{document}
