
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

\documentclass[10pt,conference,a4paper]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}



% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
\usepackage{graphicx}
% *** GRAPHICS RELATED PACKAGES ***
\DeclareGraphicsExtensions{.eps,.png,.jpg,.pdf}
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}




% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
\usepackage{float}
\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Spectral Image Visualization Using Generative Adversarial Networks}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
\IEEEauthorblockN{Siyu Chen, Danping Liao, Yuntao Qian}
\IEEEauthorblockA{College of Computer Science\\Zhejiang University, HangZhou, China\\
sychen@zju.edu.cn, liaodanping@gmail.com, ytqian@zju.edu.cn}
%\and
%\IEEEauthorblockN{Liao Danping}
%\IEEEauthorblockA{College of Computer Science\\Zhejiang University, HangZhou, China\\
%liaodanping@gmail.com}
%\and
%\IEEEauthorblockN{Qian Yuntao}
%\IEEEauthorblockA{College of Computer Science\\Zhejiang University, HangZhou, China\\
%ytqian@zju.edu.cn}

}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3},
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Spectral images captured by satellites and radio-telescopes are analyzed to obtain information about geological compositions distributions, distant asters as well as undersea terrain. Spectral images usually contain tens to hundreds of continuous narrow spectral bands and are widely used in various fields. But the vast majority of those image signals are beyond the visible range, which calls for special visualization technique.
The visualizations of spectral images shall convey as much information as possible from the original signal and facilitate image interpretation.
However, most of the existing visualizatio methods display spectral images in false colors, which contradict with human's experience and expectation.
In this paper, we present a novel visualization generative adversarial network (GAN) to display spectral images in natural colors.
%Based on CycleGAN model and WGAN, we develop an end-to-end architecture using pixel-wise convolutions and residual learning technique.
%Unlike other methods that generate unnatural visualization which contradict with human's experience and expectation, or those produce natural visualizations by require pixel level matching for pixel referential color information, our approach is able to generate comprehensible visualizations in natural colors without the requirement of paired reference images or pixel-wise matching.
To achieve our goal, we propose a loss function which consists of an adversarial loss and a structure loss.
The adversarial loss pushes our solution to the natural image distribution using a discriminator network
that is trained to differentiate between false-color images and natural-color images.
We also use a cycle loss as the structure constraint to guarantee structure consistency.
Experimental results show that our method is able to generate structure-preserved and natural-looking visualizations.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}

The rapid development of imaging technology brings about the ability to capture images with high spectral resolution.
Hyperspectral imaging sensors, for example, routinely capture more than $100$ channels of spectral data, while medical imaging systems capture multi-dimensional, and multi-modal image set.
Ultimately these images are often interpreted by human observers for analysis or diagnosis.
However, human's eyes are merely capable of sensing a very narrow range of the electromagnetic wavelength.
It is thus crucial to reduce the dimensionality of spectral images so that the images can be displayed on an output device.
Although the requirements of visualization are task dependent, there are some common goals such as information preservation, consistent rendering and natural palette~\cite{jacobson2005design}.

The most straightforward visualization method is to select three of the original bands to display.
Band selection methods such as
linear prediction (LP) was applied to select the most informative bands~\cite{su2014LP}.
One disadvantage of band selection methods is that, except for the selected bands, all the information contained in other channels is ignored.

To preserve the information across the whole range of wavelength, dimension reduction methods such as principle component analysis~\cite{tyo2003principal} and independent component analysis~\cite{zhu2007evaluation} were proposed.
To better preserve the nonlinear and local structures, manifold learning methods were applied to spectral image visualization tasks~\cite{bachmann2005exploiting}.%,crawford2011nonlinear}.
In \cite{kotwal2010visualization}, in order to preserve the edge information, bilateral filtering is applied to calculate the band weights at each pixel for band image fusion.
These local structure-based approaches demonstrate excellent performance in preserving the intrinsic information of spectral images.

While most existing visualization methods try to preserve as much information as possible from the original data,
most of them display spectral images in false colors which are hard to interpret when the colors of objects are very different from what is expected by humans.
Moreover, most data-adaptive methods suffer from ``inconsistent rendering'' problem, i.e., very different colors might be assigned to the same objects/materials in different images, which also hinders the interpretation of spectral images.
Therefore, ``natural palette'' and ``consistent rendering'' gradually become two important criteria for visualization quality evaluation.

To produce consistent natural-looking images, Jacobson \emph{et al}.~\cite{jacobson2005design} %,jacobson2007linear}
proposed a stretched color matching function (CMF) for visualization, which stretches the
CIE 1964 tristimulus color matching functions from the visible range to the invisible range.
However, the strecthed CMF is fixed for each type of hyperspectral imaging sensor, which limits its capability in preserving the specific information in differrent spectral images.


Connah \emph{et al}. ~\cite{Connah2014} used a constrained contrast mapping paradigm in the gradient domain to generate a visualization that shares similar colors with a corresponding natural-looking RGB image. This method requires pixel-wise matching between the spectrtal image and the corresponding RGB image, which limits its applications in general scenarios where pixel-wise matching is hard to obtain.

Liao \emph{et al}. utilized manifold alignment to transfer colors from natural RGB images to hyperspectral images~\cite{liao2016hscolor,liao2014hscolor,liao2013hscolor}.
The approach is capable of visualizing the hyperspectral image in natural colors as well as preserving local similarity between hyperspectral pixels.
However, manifold alignment also requires a set of matching pixels between the hyperspectral image and the referencing RGB image. As a result, the corresponding RGB image should be captured at exactly the same site as the hyperspectral image and should not have large geometric distortion in order to obtain precise image registration.

Recently, Generative Adversarial Network (GAN) based techniques have demonstrated to be very effective in colorization and style transfer~\cite{cyclegan,pix2pix,dcgan}. Unlike traditional style transfer methods that specifically and explicitly design objectives for the model, GAN utilizes a discriminator network to guide the generator. Such an adversarial learning technique makes it possible to train a strong generative model in an unsupervised way without pixel labeling or paired samples. These advantages make GAN a promising visualization technique.
\begin{figure}[!t]
\centering
\includegraphics[width=3.6in]{imgs/dual_model.pdf}
\caption{Model structure.}
\label{fig:model}
\end{figure}

In this work we propose an end-to-end visualization generative adversarial network (VGAN), in which a deep residual network (ResNet) is used.
Our goal is to generate natural-looking and structure-preserved visualizations for spectral images.
Different from previous methods that require image-pairing and pixel-wise matching between natural images and the spectral images to ``transfer''  colors, our model is totally unsupervised and can automatically learn the correspondence between different data distributions.
To achieve our goal, we propose a loss function which consists of an adversarial loss and a cycle-consistency loss.
The adversarial loss pushes our solution to the natural image manifold using a discriminator network
that is trained to distinguish false-color images from natural-color images.
The cycle-consistency loss guarantees the structure of the spectral image to be preserved during color mapping.

%In VGAN the aim is to generate a natural-looking visualization $I$ for an input remote sensing spectral image $S$. In training, $I$ are satellite images obtained from Google Earth.
The model structure of the VGAN is shown in Fig. \ref{fig:model}.
Based on Cycle-GAN~\cite{cyclegan}, VGAN comprises two generators and two discriminators.
Generator1 contains: 1. a \textit{compressor} network that fuses input spectral images to a 3-band image; 2. a \textit{mapper} that translates the output of the compressor to a natural-looing image.
Discriminator1 encourages Generator1 to produce visualizations indistinguishable from natural color images.
We also use a second generator, a mapper from natural-looking images to the latent output space of the compressor, to guarantee structure consistency by minimizing the cycle loss.
%Generator2 is trained with the goal of fooling Discriminator2 that is trained to distinguish
%the output of Generator2 from the output of the compressor.
%Experiments show that the constraint imposed by the Generator2 is essential for generating structure-consistent visualizations.
Both compressor and mapper networks use very small convolution kernels which allows the feature maps to be more structural consistent. Also, using small size kernels makes the model faster and easier to train, as pointed out by recent model compression researches~\cite{squeezenet2016,mobilenet2017}.


This paper is organized in the following order: Section \ref{sec:gan} explains GAN and its adversarial learning techniques; Section \ref{sec:proposed_method} introduces the proposed GAN for spectral image visualization; Section \ref{sec:experiments} shows the experimental results and comparisons. The conclusions are presented in Section \ref{sec:conclusion}.


\section{Generative Adversarial Nets}
\label{sec:gan}

GAN was firstly introduced by Goodfellow \cite{goodfellow2014generative}, in which, noise $z$ sampled from uniform distribution is passed to an up-sampling network $G$ to generate image $G(z)$ from some latent distribution. The goal of network $G$ is to generate images that are highly similar to real images and thus difficult to classify by a differentiable discriminator $D$. This is achieved by maximizing the targets $L_D$ and $L_G$ alternately.
\begin{eqnarray}
% \nonumber % Remove numbering (before each equation)
  \label{eq:ld}
  L_D &=& \underset{\substack{ x\sim p_{data} \\ z\sim p_{noise}} }{\mathbb{E}}  \log(D(x)) + \log \left(1-D(G(z)) \right),\\
  \label{eq:lg}
  L_G &=&  \underset{z\sim p_{noise} }{\mathbb{E}} \log(D(G(z))) .
\end{eqnarray}

$D$ takes in real image $x\sim P_{data}$ and fake image $G(z)$ and gives discriminative confidence $y\in [0,1]$. Higher confidence indicates more likely of being a real image.

This adversarial process has an optimal solution where nash equilibrium is reached. The following equations hold when  the optimal solution is reached.
\begin{eqnarray}
  \underset{\substack{ x\sim p_{data} \\ z\sim p_{noise}} }{\mathbb{E}} \log(D^*(G^*(z))) - \log(D^*(x)) &=& 0 ,\\
  \underset{z\sim p_{noise} }{\mathbb{E}} D^*(G^*(z)) &=& 0.5 ,\\
  \underset{x\sim p_{data} }{\mathbb{E}} D^*(x) &=& 0.5 .
\end{eqnarray}

%The equilibrium means the generator generates delicate fakes which are indistinguishable from real images, thus yielding $50\%$ probability for both.

\section{Proposed method}
\label{sec:proposed_method}
The goal of VGAN is to estimate a structure-preserved and natural-looking visualization for an input spectral image.
In our model, a spectral image from domain $A$ is firstly transformed to domain $B$ by a compressor, which compresses the number of channels to 3 and outputs a false-color image.
Then the false-color image is mapped to domain $C$ where images have natural color distributions.
In order to generate structure-consistent visualizations, we impose a cycle loss  by using a second generator that maps images from domain $C$ to domain $B$ when training the model.

VGAN is constructed with heterogeneous network architecture for generators $G_{A\rightarrow C}$ and $G_{C\rightarrow B}$.
The generator $G_{A\rightarrow C}$ contains two parts: compressor $Cpr$ and mapper $M_1$. The generator $G_{C\rightarrow B}$ contains only one mapper $M_2$. The $M_1$, $M_2$ and $Cpr$ correspond with \textit{Mapper 1}, \textit{Mapper 2} and \textit{Compressor} in Fig. \ref{fig:model}.

\subsection{Formulation}
The objective of VGAN is to learn a mapping function from spectral image domain $A$ to natural RGB domain $C$ via a latent false-color domain $B$ as a bridge, given training samples ${\{x_i\}}_{i=1}^N$ where $x_i \in A$, and ${\{y_j\}}_{i=1}^M$ where $y_i \in C$. There are three mapping functions to be learned. One of the mapping functions maps the image from domain $A$ to $B$, one from $B$ to $C$ and another one maps the image from $C$ back to $B$. We denote the data distribution as $x\sim \mathbb{P}_A$, $y\sim \mathbb{P}_C$ and $z\sim \mathbb{P}_B$ where $z$ is translated from domain $A$. Formally, the three mappings are $Cpr:A\rightarrow B$, $M_1:B\rightarrow C$ and $M_2:C\rightarrow B$, which compose two generative mappings $G_{A\rightarrow C}(x)=M_1(Cpr(x))$ and $G_{C\rightarrow B}(y)=M_2(y)$. In addition, we use two adversarial discriminators $D_C$ and $D_B$, where $D_C$ aims to distinguish between images $\{y\}$ and visualized images $G_{A\rightarrow C}(x)$; in the same way, $D_B$ aims to discriminate between $\{z\}$ and $G_{C\rightarrow B}(y)$. Our objective contains two types of terms: \textit{adversarial losses} to constrain the output of mapping functions to be as similar as possible to the target domain; and \textit{cycle consistency losses} to preserve the structure.

\subsubsection{Adversarial Loss}
Adversarial losses are applied to both of the generators. For the generator $G_{A\rightarrow C}$ its discriminator $D_C$, the objective is formulated as follows:
\begin{multline}\label{eq:adv_loss}
  \mathcal{L}_{GAN}(G_{A\rightarrow C},D_C,A,C) = \underset{y\sim \mathbb{P}_C}{\mathbb{E}} \log(D_C(y)) \\
  + \underset{x\sim \mathbb{P}_A}{\mathbb{E}} \log(1-D_C(G_{A\rightarrow C}(x))),
\end{multline}
where $G_{A\rightarrow C}$ takes in images from domain $A$ and generate images similar to images from domain $C$, while $D_C$ aims to distinguish fake images generated by $G_{A\rightarrow C}$ from images in domain $C$. A similar adversarial loss is introduced for the generator $G_{C\rightarrow B}$ and its discriminator $D_B$ as well: i.e. $min_{G_{C\rightarrow B}}max_{D_B}\mathcal{L}_{GAN}(G_{C\rightarrow B},D_B,C,B)$ where the samples of domain $B$ is generated by $Cpr(x), x\sim \mathbb{P}_A$.

\subsubsection{Cycle Consistency Loss}
Because of the implications of adversarial loss that, given a mapping function $M$, any mappings obtained by permutating the images in the target domain gives similar loss value. Thus, with only adversarial loss, it cannot be guaranteed that the learned function can map input $x_i$ to a desired $y_i$.
Also, cycle consistency loss can help to prevent the function from mapping all the $x_i$ to a single highly realistic $y_i$, as have been reported in \cite{traingan2016}. If the mapping functions are cycle-consistent, for each image $x$ from domain $A$, the mapping cycle should bring $x$ back to a specified stage, i.e. $x\rightarrow Cpr(x) \rightarrow G_{A\rightarrow C}(x) \rightarrow G_{C\rightarrow B}(G_{A\rightarrow C}(x)) \approx Cpr(x)$. This means, to recover $Cpr(x)$ from $G_{C\rightarrow B}(x)$, the information contained in $Cpr(x)$ such as structure/texture should be preserved by $G_{A\rightarrow C}$. The cycle consistency loss is formulated as:
\begin{equation}\label{eq:cycleloss}
\begin{aligned}
  \mathcal{L}_{cyc}(M_1,M_2)&=\underset{x\sim \mathbb{P}_A}{\mathbb{E}} [\| M_2(M_1(Cpr(x)))-Cpr(x) \|] \\
  &+\underset{y\sim \mathbb{P}_C}{\mathbb{E}} [\|M_1(M_2(y))-y\|].
\end{aligned}
\end{equation}

\subsubsection{Full Objective}
Our overall objective is:
\begin{equation}\label{eq:objective}
\begin{aligned}
    \mathcal{L}(G_{A\rightarrow C},G_{C\rightarrow B},D_C,D_B)&=\mathcal{L}_{GAN}(G_{A\rightarrow C},D_C,A,C)\\
    &+\mathcal{L}_{GAN}(G_{C\rightarrow B},D_B,C,B)\\
    &+\lambda \mathcal{L}_{cyc}(M_1,M_2).
\end{aligned}
\end{equation}

Earth-Mover distance from WGAN \cite{wgan} is used to formulate objectives for optimization. In WGAN, discriminators are constrained to be 1-Lipschitz functions and their losses are constructed using the Kantorovich-Rubinstein duality.%\cite{Villani2009Optimal}.

There are works shows that the original WGAN tends to having extreme-valued or extremely distributed weights because of the weight clipping scheme \cite{improvewgan2017}. To alleviate this phenomenon, we optimize the expectation using softmax cross-entropy. The removal of the last activation in discriminator prevents the weights from growing too large. We argue that together with softmax cross-entropy, activation removal, batch normalization and data augmentation averting large weights caused by over-fitting, the loss can be an efficient approximation to K-Lipschitz function.

The adversarial loss of discriminator that distinguishes real samples in domain $C$ from fake samples generated by mapping functions $G_{A\rightarrow C}$ is:
\begin{multline}\label{eq:loss_da}
  \mathcal{L}_{D}(C,G_{A\rightarrow C}) = \underset{y\sim \mathbb{P}_C}{\mathbb{E}}  CE\left(S(D_C(x)),\vec{1} \right) +\\
  \underset{x\sim \mathbb{P}_A}{\mathbb{E}} CE\left(S(D_C( G_{A\rightarrow C}(x))),\vec{0} \right) ,
\end{multline}
where $CE$ stands for cross-entropy function and $S$ is softmax function.

The adversarial loss of generator that takes input from domain $A$ and generate samples in domain $C$ is:
\begin{equation}\label{eq:adv_loss_ga}
  \mathcal{L}_G(A,C)= \underset{x\sim \mathbb{P}_A}{\mathbb{E}} CE\left(S(D_C(G_{A\rightarrow C}(x))),\vec{1} \right).
\end{equation}

We learn the discriminative models via performing the minimization:
\begin{equation}\label{eq:objective_d}
J^{D^*} = \underset{\theta_D}{\min}~ \mathcal{L}_{D}(C,G_{A\rightarrow C})+\mathcal{L}_{D}(B,G_{C\rightarrow B}).
\end{equation}

The generators are learned by the following optimization:
\begin{equation}\label{eq:objective_g}
J^{G^*}= \underset{\substack{\theta_G}}{\min}~
\mathcal{L}_G(A,C)+
\mathcal{L}_G(C,B)+
\lambda \mathcal{L}_{cyc}(M_1,M_2).
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Architecture}
\label{sec:arch}
Prevailing architectures of generators like in most of cGANs \cite{cgan} are based on encoder-decoder structure for color stylization and transfer \cite{pix2pix,cyclegan}.
%This is partially because artistic stylization tasks require relatively strong re-creation based on the original input.
However, the auto-encoder architecture is prone to generating fabricated patterns/textures with inconsistent shapes and inconsistent coloring, as shown in Fig. \ref{fig:inconsistent_gans}, making it not suitable for visualization tasks which require precision and preciseness.
Some attempts have been made to use U-net to preserve the image structure \cite{pix2pix,dualgan} but the improvement is not significant.

\begin{figure}[!t]
    \centering
    \subfloat[]{
    \includegraphics[height=0.5in]{imgs/inconsistent_gan/pix2pix_shape_inconsistent.png}
    \label{fig:pix2pix_fronts}
    }
    \subfloat[]{
    \includegraphics[height=0.5in]{imgs/inconsistent_gan/progress_gan_horse.png}
    \label{fig:pgan_horse}
    }
    \subfloat[]{
    \includegraphics[height=0.5in]{imgs/inconsistent_gan/cyclegan_trees.png}
    \label{fig:cyclegan_trees}
    }
    \subfloat[]{
    \includegraphics[height=0.5in]{imgs/inconsistent_gan/cyclegan_cloud_highlight.png}
    \label{fig:cyclegan_cloud}
    }
%    \subfloat[]{
%    \includegraphics[height=0.5in]{imgs/inconsistent_gan/synthesis_gan_wall.png}
%    \label{fig:sgan_wall}
%    }
%    \subfloat[lost contour]{
%    \includegraphics[height=0.7in]{imgs/inconsistent_gan/synthesis_gan_ground.png}
%    \label{fig:sgan_roads}
%    }
    \caption{Common problems in GANs.
    (a) Inconsistent structure in pix2pix. (b) Fabricated patterns in Progressive-GAN.
    (c) Incorrect coloring in Cycle-GAN. (d) Inconsistent coloring in Cycle-GAN.}
    \label{fig:inconsistent_gans}

\end{figure}

The architecture of VGAN differs from previous GANs in two primary aspects. First, the images is not down-sampled or up-sampled spatially. This ensures that the original details are not destroyed by down-sampling and no extra texture is created by up-sampling. Second, the kernel size of all the convolutions are set to 1 to prevent the network from generating blurred texture.

\begin{figure}[!t]
\centering
\includegraphics[width=3.3in]{imgs/generator_resnet.pdf}
\caption{Network architecture of the generator(ours).}
\label{fig:gen_ours}
\end{figure}

The architecture of generator $G_{A\rightarrow C}$ is shown in Fig. \ref{fig:gen_ours}. The network architecture of the mapper in generator $G_{C\rightarrow B}$ is identical to the mapper in $G_{A\rightarrow C}$.
The compressor network contains two stride-1 convolutions. The output of compressor is activated by $tanh$ so that the output is in the same range with the output of generator $G_{C\rightarrow B}$. The mapper network in generator $G_{A\rightarrow C}$ contains 5 residual blocks and one convolution layer with stride set to 1. We leverage the residual blocks in our architecture based on \cite{resnet2015}. Instance normalization is used in generator networks to diminish the influence of instance-specific contrast information in each input image \cite{2016instancenorm}.

The architecture of the discriminator is shown in Fig. \ref{fig:disc}. It consists of 4 convolutional layers with batch-normalization and Leaky-ReLU activation. Each convolutional layer reduce both the height and width of an image by a factor of 2. At the end of the network, the values given by the last convolution operation are reshaped to a vector as the output. For better gradient behaviour, the final output is not activated by any non-linear operation~\cite{wgan}.
\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{imgs/discriminator.pdf}
\caption{Architecture of the discriminator.}
\label{fig:disc}
\end{figure}

%The two-generator structure is essential to our visualization architecture because the compressed 3-band image should contain color topology information and precise spatial structure so that the mapper could focus on (and is also designed to) transferring only the color instead of fabricating pixel topology or spatial texture.
%The topology is kept by imposing an explicit constraint on the network which requires the cycle consistent be minimized.


%Another implicit constraint is imposed by the pixel-wise architecture itself.
In our model, all the kernel size is set to 1. It is highly unlikely to generate extra fabricated artifacts and textures by such an pixel-wise computation.
On the contrary, pixel-wise operations are inclined to cause topology simplification and feature merging.
With the explicit cycle-consistency constraint reinforcing the information preservation and the implicit constraint reducing unwanted excessive creations, the networks is optimized to be structure consistent and nature in generation style.

We also tried a GAN model without the cycle architecture. The spatial structure in the images generated by the model were not well preserved. The colors in those visualization results were incorrect, either. In order to improve the performance, we explored several losses but did not observe significant improvement.

\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}
In our experiment, we used a remote sensing hyperspectral image and a set of natural RGB images obtained  from Google Earth for training.
The hyperspectral image is taken over Washington D.C. mall by the Hyperspectral
Digital Imagery Collection Experiment sensor. The
data consist of $191$ bands after noisy bands are removed, in
which the size of each band image is $1208 \times 307$. Fig. \ref{fig:band50}  shows its 50th band image.
We collected 12 RGB images including scenes of New York(see Fig. \ref{fig:googlemap_newyork}), Orlando, and Washington. These images are of roughly the same size (about $1600 \times 800$) and have different spatial resolution.
The spectral images and RGB images are sliced into 6000 overlapped patches with the size of $128\times 128$ for training.
We augmented the data set by applying flipping and rotation on the patches.
For testing, the spectral image of D.C. mall is cut into non-overlapping patches.
The visualization result is obtained by stitching the non-overlapping patches together.

\begin{figure}[t]
\centering
\includegraphics[angle=90,width=0.7\linewidth]{imgs/DC_band50.pdf}
\caption{The 50th band of the HSI}\label{fig:band50}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{imgs/googlemap_newyork1.png}
\caption{One of the RGB images taken from Google Maps.(New York city)}
\label{fig:googlemap_newyork}
\end{figure}

\subsection{Implementation Details}

We used ADAM for optimization and set the learning rate of the generator and the discriminator to 0.0001 and 0.00001, respectively.
The weight of cycle-loss $\lambda$ is set to 50.
In order for the generator to produce more diverse outputs, we follow the mini-batch technique proposed in ~\cite{traingan2016}, i.e., the discriminators are equipped with cached queues that store the most recent 50 fake images and 50 real images.
The epoch size is set to  6000 and the model is trained for 2 epochs using $A$ and $B$ training data with a batch size of 1.

\subsection{Visual and Quantitative Comparisons}

\begin{figure}[!t]
\centering
\subfloat[LP band selection]{
\includegraphics[angle=90,width=0.7\linewidth]{img/duqianDCband_15_27_54.pdf}
\label{fig:DClpbandselection}
}
\\
\subfloat[Manifold alignment]{
\includegraphics[angle=90,width=0.7\linewidth]{img/mani_new.png}
\label{fig:DCmanifoldalignment}
}
\\
\subfloat[Stretched CMF]{
\includegraphics[angle=90,width=0.7\linewidth]{img/dc_cmf.pdf}
\label{fig:DCCMF}
}
\\
\subfloat[Bilateral filtering]{
\includegraphics[angle=90,width=0.7\linewidth]{img/DC_bilateral.pdf}
\label{fig:DCBF}
}
\\
\subfloat[Bicriteria optimization]{
\includegraphics[angle=90,width=0.7\linewidth]{img/DCBCOCDM.pdf}
\label{fig:DCBCOCDM}
}
\\
%\subfloat[Ours]{
%\includegraphics[angle=90,width=0.7\linewidth]{img/ours_C2A_pixnet.png}
%\label{fig:DCMLS}
%}
%\\
\subfloat[VGAN]{
\includegraphics[angle=180,width=0.7\linewidth]{img/ours_C2A.png}
\label{fig:GAN}
}
\\
\subfloat[Ground-truth RGB image]{\includegraphics[angle=0,width=0.7\linewidth]{img/dcmall_full_registered.png}
\label{fig:registered2}
}
\caption{Visual comparison of different visualization approaches on the Washington DC Mall data set.
The spectral image of D.C. Mall was taken in Aug, 1995, and the RGB image from Google Earth was taken in 2017 winter.}
\label{fig:comparison}
\end{figure}


We compare our method with several methods including LP Band Selection \cite{su2014LP}, Stretched CMF \cite{cmf2005}, Bilateral Filtering \cite{kotwal2010visualization}, Bicriteria Optimization \cite{bicriteria2012} and Manifold Alignment \cite{liao2016hscolor}.
Fig. \ref{fig:comparison} shows the visual comparison of different visualization approaches on the Washington D.C. mall data.
It can be seen that the result of VGAN not only has a very natural tone but also preserves fine details.

Since there is no universally accepted standard for the quantitative assessment of spectral image visualization,  we adopt 4 metrics including entropy \cite{kotwal2010visualization}, root-mean-square error(RMSE) \cite{zhu2007evaluation}, separability of features \cite{cui2009interactive} and correlation coefficients between the RGB components (CORR) \cite{zhu2007evaluation}.

\emph{Entropy}: An image
with higher entropy contains richer information than ones with
low entropy. The entropy of a single-channel image is given by:
\begin{equation}\label{eq:entropy}
  h=-\sum_{x}p(x)\ln p(x),
\end{equation}
where $p(x)$ is the probability density of the intensity level $x$ in
the image. For an RGB image, its entropy is the average entropy
of all channels.

\emph{RMSE}: RMSE between the true color image and the visualized
image is a straight-forward way to evaluate whether the visualization
has natural colors~\cite{zhu2007evaluation}. The ground truth true color image is set to the RGB satellite image captured from the same location as the spectral image (Fig.~\ref{fig:registered2}).

Let $s(x)$ and $s'(x)$  denote the RGB vector of pixel $x$
in the visualized image and the true color image, respectively. The RMSE
between $s$ and $s'$ over the whole image is defined by:
\begin{equation}\label{eq:rmse}
  r = \sqrt{\frac{1}{N}\sum_x \left( s(x)-s'(x) \right)}.
\end{equation}

\emph{CORR}:
Since natural images have a high degree of correlation
between the RGB components~\cite{zhu2007evaluation}, the correlation between the RGB components of the
visualizations indicates the similarity of the visualizations to natural images.
The two-dimensional normalized correlation function for images is defined as:
\begin{equation}\label{eq:corr}
  CORR_{X,Y}=\frac{\sum_{m,n} (X_{mn}-\bar{X})(Y_{mn}-\bar{Y})}{\sum_{m,n}(X_{mn}-\bar{X})^2 \sum_{m,n} (Y_{mn}-\bar{Y})^2},
\end{equation}
where $X$, $Y$ are two image channel vectors; and $\bar{X}$, $\bar{Y}$ are the means
of $X$ and $Y$, respectively. $C_{XY}$ is a real number between -1
and 1.
the overall CORR is obtained as the mean value of $C_{R,G}$, $C_{R,B}$ and $C_{B,G}$.

\emph{Separability of Features}: Separability of features measures how
well distinct pixels are mapped to distinguishable colors. The
basic idea is that the average distance between two pixels in the
color space should be as large as possible \cite{cui2009interactive}.
Separability of features is defined as:
\begin{equation}\label{eq:separability}
  \delta= \frac{1}{(N-1)^2}\sum_{x\neq y}d(x,y),
\end{equation}
in which, $d(x,y)$ is the Euclidean distance between the pair of
pixels $x$ and $y$ in the RGB space and $N$ is the number of pixels.
$\delta$ denotes the average pairwise Euclidean distance in terms of
all pixel pairs. Larger $\delta$ yields better separability of
features.

The quantitative comparison results are shown in Table. \ref{table:performance}.
Comparisons on entropy and RMSE shows that manifold alignment method's performance stands out by solving the tradeoff between information preservation and natural rendering.
Without the requirement of paired labeling, our method still performs well under entropy and RMSE metrics. The results indicate that VGAN can visualize images with rich information and similar color to the unseen ground-truth.
Compared to manifold alignment, which requires a set of matching pixels for color transfer, our method achieves better CORR, which indicates the strong capability of VGAN in producing visualizations with natural color distribution. Also, the separability of our approach ranks the first place.
This means, our approach can render the image in natural color and, at the same time, preserve the pixel separability in a way that doesn't need extra explicit constraints or supervision.

Notably, form the results shown in Fig. \ref{fig:GAN} we can see that even when the real-color training samples and the spectral image were taken in different seasons, our method is able to learn the most likely appearance of trees in summer instead of the sallow color directly transferred by manifold alignment.
In our experiments we found that, the network without cycle-loss performs much worse in terms of separability. This meets our expectation since cycle-loss requires the separability be preserved so that the details can be best recovered.

\begin{table}[t]
\centering
 \caption{Performance comparisons.}
  \label{table:performance}
  \begin{tabular}{ccccccc}
    \hline
    Method                   & Entropy        & RMSE           & CORR          & Separability           \\
    \hline
    LP Band Selection        & 6.59           & 68.35          & 0.92          & 55.68                  \\
    \hline
    Stretched CMF            & 3.89           & 92.46          & 0.36          & 30.08                  \\
    \hline
    Bilateral Filtering      & 3.31           & 102.43         & 0.73          & 47.54                  \\
    \hline
    Bicriteria Optimization  & 5.75           & 98.47          & 0.63          & 52.43                  \\
    \hline
    Manifold Alignment       & \textbf{10.32} & \textbf{33.23} & 0.89          & 34.64                  \\
    \hline
    Ours                     & 8.25           & 49.99          & \textbf{0.98} & \textbf{64.89}         \\
    \hline
  \end{tabular}
\end{table}

\section{Conclusion}
\label{sec:conclusion}
In this paper we presented an unsupervised end-to-end VGAN for displaying spectral images in natural colors.
By minimizing the adversarial loss of the discriminator that is trained to differentiate between false-color images and natural-color images, VGAN is able to generate natural-looking visualizations.
In addition, we use a cycle-loss to overcome the ambiguity and structure inconsistent problem of classic GAN.
Visualization results show that VGAN is capable of producing visualizations with rich information and very natural color distributions.
Pixels in the spectral images are also well separated in the RGB space.


% conference papers do not normally have an appendix


% use section* for acknowledgment
%\section*{Future Works}


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{mybib}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)





% that's all folks
\end{document}


