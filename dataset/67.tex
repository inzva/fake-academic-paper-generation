

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\usepackage{hyperref}
\usepackage{url}
\usepackage{times}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[toc,page]{appendix}
\usepackage{enumitem}
\usepackage[toc,page]{appendix}
\setlist[itemize]{leftmargin=*}


\begin{document}
\pagestyle{headings}
\mainmatter
\title{Sherlock: Scalable Fact Learning in Images} % Replace with your title


\author{ \small Mohamed Elhoseiny$^{1,2}$,  Scott Cohen$^{1}$,  Walter Chang$^{1}$,  Brian Price$^{1}$, Ahmed  Elgammal$^{2}$ \normalsize }
\institute{$^{1}$Adobe Research $\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,$ $^{2}$Department of Computer Science, Rutgers University}




 \graphicspath{{./retrieval_qual3_seen_crop/}{./retrieval_qual3_unseen_crop/}{./KEx_qual2_crop/}{.//KEx_qual2_unseen_crop/}{./ret_qual3_gt5_seen_crop/}
 }




\maketitle




\begin{abstract}


We study scalable and uniform understanding of facts in images. Existing visual recognition systems are typically modeled differently for each fact type such as objects, actions, and interactions. We propose a setting where all these facts can be modeled simultaneously with a capacity to understand unbounded number of facts in a structured way.  The training data comes as structured facts in images,
including (1) objects (e.g.,  $<$boy$>$), (2)  attributes (e.g., $<$boy, tall$>$), (3) actions (e.g., $<$boy, playing$>$), and (4) interactions (e.g.,  $<$boy, riding, a horse $>$).  Each fact has a semantic language view (e.g., $<$ boy, playing$>$) and a visual view (an image with this fact). %This opens the door for new applications such as structured tagging (e.g. $<$person, riding , horse$>$) which is a more meaningful representation of information in images and is helpful for retrieval.
We show that learning visual facts in a structured way enables not only a uniform but also  generalizable visual understanding.  We propose and investigate recent and strong approaches from the multiview learning literature and also introduce two learning representation models as potential baselines. We applied the investigated methods on several datasets that we augmented with structured facts and a large scale dataset of more than 202,000 facts and 814,000 images. Our experiments show the advantage of relating facts by the structure by the proposed models compared to the designed baselines on bidirectional fact retrieval.


\end{abstract}




\vspace{-8mm}
\section{Introduction}
\label{lbl_intro}
\vspace{-2mm}
\begin{comment}
\begin{quote}
\emph{I had come to an entirely erroneous conclusion, my dear Watson, how dangerous it always is to reason from insufficient data.}
~-Sherlock Holmes
\end{quote}
\end{comment}\begin{comment}
\begin{quote}
\emph{It is a capital mistake to theorize in advance of the facts.}
~-Sherlock Holmes (The Adventure of the Second Stain)
\end{quote}
\vspace{-2mm}
\end{comment}%\noindent\emph{It is a capital mistake to theorize in advance of the facts.}~-Sherlock Holmes~(\cite{DoyleSecondStrain})%Associating natural language and image is currently an active area of research due to its wide range of applications.  One approach to assigning language to images is simply that of classification where an algorithm such as a neural net is used to provide a natural language tag to the dominant object in an image~\cite{}.  This can be extend to the tagging problem where a number of tags are created for an image corresponding to objects or scene properties~\cite{}.  These tags may or may not be localized in the image.  Another approach to associating images and natural language is that of captioning~\cite{}.  Captioning methods allow free-form descriptions of the image scene and contents therein.  There are also approaches like CCA~\cite{} that try to model the problem by embedding the language and images into a joint space.


Despite recent significant advances in recognition, image captioning, and visual question answering (VQA), there is still a large gap between humans and machines in the deep image understanding of objects, their attributes, actions, and interactions with one another. The human visual system is able to efficiently gain visual knowledge by learning different types of facts in a never ending way from many or few examples, aided by the ability to generalize from other known facts with related structure. We believe that the most effective and fastest way to close this gap are with methods that possess that following key characteristics:
\begin{itemize}
	\item \textbf{Uniformity}:  The method should be able to handle objects (``dog''), attributes (``brown dog''), actions (``dog running'') and interactions between objects (``dog chasing cat'').
	\item \textbf{Generalization}:  The method should be able to generalize to facts
that have zero or few examples during training.
	\item \textbf{Scalability}:  The method should handle an unbounded number of facts.
	\item \textbf{Bi-directionality}:  The method should be able to retrieve a language description for an image, and images that show a given language description of a fact.
	\item \textbf{Structure}:  The method should provide a structured understanding of facts, for example that ``dog'' is the subject and has an attribute of ``smiling''. %, detailed later. %; see Fig.~\ref{fig:imageknowledge}.
\end{itemize}%We approach this understanding task by models that can associate  language representation of facts to visual examples.


Existing visual understanding systems may be categorized into two trends: (1)  fact-level systems and (2) high-level systems. Fact level  systems include object recognition, action recognition, attribute recognition, and interaction recognition  (e.g.,~\cite{simonyan2014very},~\cite{zhang2014panda}, ~\cite{chen2014inferring}, ~\cite{zhou2014learning}, ~\cite{gkioxari2014finding},~\cite{Antol2014}). These systems are usually evaluated
separately for each fact type (e.g., objects, actions, interactions, attributes, etc.) and are therefore not uniform.
Typically, these systems have a fixed dictionary of facts, assuming that facts are seen during training by at least tens of examples, and treat facts independently. Such methods cannot generalize to learn facts outside of the dictionary and will not scale to an unbounded number of facts, since model size scales with the number of facts.
Furthermore, these recognition systems are typically uni-directional, only able to return the conditional probability of a fact given an image.
The zero/few-shot learning setting (e.g., ~\cite{romera2015embarrassingly,lampert2009learning}),  where only a few or even zero examples per fact are available, is typically studied apart from the traditional recognition setting.
We are not aware of a unified recognition/few shot learning system that learns unbounded set of facts.


In the second trend, several researchers study tasks like image captioning~\cite{karpathy2014deep,vinyals2015show,xu2015show,mao2015deep}, image-caption similarity~\cite{karpathy2014deep,kiros2014unifying}, and visual question answering~\cite{antol2015vqa,malinowski2015ask,ren2015exploring} with very promising results.
These systems are typically learning high-level tasks but their evaluation does not answer whether
these systems relate captions or questions to images by fact-level understanding.
Captioning models output sentences and thus can mention different types of facts and, in principle, any fact.
However, Devlin et al.~\cite{devlin2015language,devlin2015exploring} reported that 60-70\% of the generated captions by LSTM-based captioning methods actually exist in the training data and show that nearest neighbor methods have very competitive performance in captioning.
These results call into question both the core understanding and the generalization capabilities of the state-of-the-art caption-level systems.  %of state-of-the-art captioning approaches.  %that train on image-caption pairs. %This result questions whether these systems caption a test image based on the captions of training images most similar to it. %%%%%%%%% SAVE FOR LATER DISCUSSION ON IMAGE-SENTENCE-SIMILARITY RESULTS\begin{comment}
Psychologists has also studied how people caption images~\cite{chaplin2006convention,kaufman2007captions}, where they showed that caption usually reflect what a person see discriminative about a scene which might be subjective. This means that  different captions of the same images might convey multiple different facts and still be correlated to the same scene in existing image-caption similarity systems.  We show in our experiments that image-caption similarity systems trained on MS COCO dataset~\cite{lin2014microsoft} are confused when evaluated on fact level understanding even with tens of facts.
\end{comment}%This might confuse these systems to understand visual information on the visual fact level. We study in our experiments how image-caption trained systems perform on fact understanding tasks.  \begin{figure}[t!]
\centering
      \vspace{-3mm}
   \includegraphics[width=0.8\textwidth]{knowledge_in_image.jpg}
     \vspace{-2mm}
    \caption{Visual Facts in Images}
    \label{fig:imageknowledge}
      \vspace{-7mm}
\end{figure}%Studying this setting enables we are making progress in understanding general fact in images. %In contrast, our work provides the machinery to develop fact localization (by applying our model to areas of an image) which can describe a scene in a richer way by locating facts in it.


The limitations of prior settings motivated  us to study a fact-level understanding setting, which is more related to the first trend but unified to any fact type and able to learn an unbounded  number of facts. This setting allows measuring the gained  visual knowledge represented by the facts learnt by any proposed system to solve this task.
Our goal is a method that achieves a more sophisticated understanding of the objects, actions, attributes, and interactions between objects, and possesses the
desireable properties of scalability, generalization, uniformity, bi-directionality, and structure.
Our approach is to learn a common embedding space in which the language and visual views of a fact are mapped to the same location. The key to our solution achieving the desireable characteristics is to make the basic unit of understanding a structured fact as shown in Fig.~\ref{fig:imageknowledge} and to have a structured embedding space in which different dimensions record information about the subject S, predicate P, and object O of a fact.


Using an embedding space approach allows our method to scale as we can submit any ($<$S,P,O$>$, image), ($<$S,P$>$, image), or ($<$S$>$, image)  facts to train our embedding network. At test time, it allows for bi-directional retrieval, as we can search for language facts that embed near a given image fact and vice-versa.
Retaining the structure of a fact in the embedding space gives our method the chance to generalize to understand an S/SP/SPO from training data on its S, P, and O components, since this information is kept separate.
To obtain uniformity, we introduce wildcards ``*'' into our structured fact representation, e.g.~$<$man,smiling,*$>$ or $<$dog,*,*$>$ and use a wildcard training loss which ignores the unspecified components of embedded second and first order visual and language facts.
Carefully designed experiments show that our uniform method achieves state-of-the-art performance in fact-level bidirectional
view retrieval over existing image-sentence correlation methods, other view embedding methods, and a version of our
method without structure, while also scaling and generalizing better.


We investigate several baselines that were never uses in this task and also propose two methods to recognize  objects, object attributes, scenes, actions, and object interactions jointly as part of a generative model that is generalizable, scalable, and bi-directional.  We do so by organizing our language representation into structured facts, and our model learns to associate these facts with images.  In this way, we can think of the language description as a language view of the fact and a an image with this fact only as a visual view of the fact.
\end{comment}\textbf{Contributions:}  (1) We propose a new problem setting to study fact-level visual understanding of unbounded number of facts while considering the aforementioned characteristics. (2) We design and investigate several baselines from the multiview learning literature and
apply them on this task. (3) We propose two learning representation models that relate different fact types using the structure exemplified in Fig~\ref{fig:imageknowledge}. (4) Both the designed baselines and the proposed models embed language views and visual views (images) of facts in a joint space that allows uniform representation of different fact types. We show the value of relating facts by structure in the proposed  models compared to the designed baselines on several datasets on bi-directional fact retrieval.








\vspace{-3mm}
\section{Related Work}
\vspace{-2mm}
In order to make the contrast against the related work clear, we start by stating the scale of facts we are modeling in this work. Let's assume that $|$S$|$, $|$P$|$, and $|$O$|$ denotes the number of  unique subjects, unique predicates, and unique objects, respectively; see Fig~\ref{fig:imageknowledge}.  The scale of unique second and third order facts is bounded by $|$S$|\times |$P$|$ and  $|$S$| \times |$P$| \times |$O$|$ possibilities respectively,  which can easily reach millions of facts. The data we collected in this work has thus far reached 202,000 unique facts (814,000 images). We cover five lines of related research (first three are from  fact-level  recognition literature). %, which continue to grow. %The scale of unique second and third order facts could easily reach millions of unique facts and needs careful attention while designing a model maintaining the structure.%\begin{comment}%\begin{figure}[h!]% \centering%  \vspace{-5mm}%  \includegraphics[width=0.8\textwidth]{rel_fact_methods.png}% \caption{Our setting in contrast to the studied fact recognition settings in the literature. Scalability means the number of facts studied in these works. Uniformity mean if the setting is applied for multiple fact types. Generalization means the performance of this methods on facts of zero/few images}% \label{fig_relwork}% \vspace{-5mm}%\end{figure}% \end{comment}\textbf{(A) Modeling Visual facts in Discrete Space: }
Recognition of objects or activities has been typically modeled  as a mapping function $g: \mathcal{V} \to \cal{Y}$, where $\cal{Y}$ is discrete set of classes. %$1 \to K$.
The function $g$  has recently been learned using deep learning (e.g.,~\cite{simonyan2014very,Szegedy_2015_CVPR}). Different systems are built to recognize each fact type in images by modeling  a different $g: \mathcal{V} \to \cal{Y}$ , where $\cal{Y}$ is constrained to objects, (e.g.,~\cite{simonyan2014very}),  attributes (e.g.~\cite{zhang2014panda}), attributed objects ($<$car, red$>$)~\cite{chen2014inferring}, scenes (e.g., ~\cite{zhou2014learning}), human actions  (e.g.,~\cite{gkioxari2014finding}), and  interactions~\cite{Antol2014}. There are several limitations for  modeling  recognition as
\textbf{(1) Scalability:} Adding a new fact leads to changing the architecture, meaning adding thousands of parameters
and re-training the model (e.g., for adding a new output node). For example, if  VGGNet~\cite{simonyan2014very} is used on the scale of 202,000 facts, the number of parameters in the softmax layer alone is close to 1 billion. % GPU memory does not fit this scale of parameters and makes learning a big challenge. \textbf{(2) Uniformity:} Modeling each group of facts by a different $g$
requires
maintaining different systems,  retrain several models as new facts are added, and also doesn't allow learning the correlation among different fact types.
However, we aim to uniformally model visual  perception. \textbf{(3) Generalization:} While most of the existing benchmarks for this\begin{wrapfigure}{r}{0.58\textwidth}
 \centering
 \vspace{-8mm}
 \includegraphics[width=0.63\textwidth]{rel_fact_methods2.png}
 \caption{Our setting in contrast to the studied fact recognition settings in the literature. Scalability means the number of facts studied in these works. Uniformity means if the setting is applied for multiple fact types. Generalization means the performance of this methods on facts of zero/few images.}
 \label{fig_relwork}
 \vspace{-7mm}
 \end{wrapfigure}setting have at least tens of examples per fact (e.g., imageNet~\cite{deng2009imagenet}),  a more realistic assumption is  that  there    might not be enough examples %for the newly added examples
to learn the new class (the long-tail problem). %, which introduces a learning problem. %This is known as the long-tail distribution  and
 Several works have been proposed to deal this problem in object recognition settings  ~\cite{zipf1935psycho,salakhutdinov2011learning}. However, they suffer from the aforementioned scalability problems as facts increase.  \textbf{(4) Bi-directionality:} These models are uni-directional from $\cal{V}$ to $\cal{Y}$. Fig~\ref{fig_relwork} shows representatives settings of these methods. The three axes
are  Scalability, Uniformity, and Generalization.  These methods typically study seen classes and hence do not generalize to unseen classes.% as shown in Fig~\ref{fig_relwork}.%Based in the most recent imageNet challenge results in object recognition, several researchers argue that the visual recognition problem is solved. Even though object recognition performance is rapidly reaching maturity, this does not mean that existing machine learning methods is able to recognize complex facts in the visual world. %There are indeed other research problems that aim to explicitly understand facts about an image.%The main objective in object (e.g.,~\cite{simonyan2014very}), scene (e.g., ~\cite{zhou2014learning}), and activity categorization (e.g.,~\cite{gkioxari2014finding}) methods is to have a set of discrete categories that the system can recognize.  These methods face  a scalability problem since adding a new category means changing the architecture %of the model by adding thousands of parameters %and re-training the model (e.g., needed for adding a new output node). This drawback is also shared with recent attribute based methods (e.g., ~\cite{chen2014inferring}) that realize that attribute appearance is dependent on the class,  unlike earlier models (e.g.,~\cite{lampert2009learning}).  %This motivates them to jointly learn a classifier for every class attribute pair, which suffers from  scalability problems  as the number of the classes and attributes grows.  Our goal is to model structured facts in a way that is scalable, i.e., to avoid the need to change the model in order to add more facts. Furthermore, we aim that the model can gain structured visual knowledge by being continuously fed with instances of structured  facts,  that could be of different types (e.g.,  $<$woman,  smiling$>$, or $<$person, playing, soccer$>$, $<$ man, tall$>$, $<$kid$>$) with example image(s). At any point the machine does not have a fixed dictionary of trained object, scene, activity categories.%We then follow another two lines methods from Multiview literature that are applicable in our setting.\textbf{(B)  Modeling zero/few shot fact learning by semantic representation of classes (e.g., attributes):} One of the most successful ideas for learning from few examples per class is by using semantic output codes like attributes as an intermediate layer between features and classes. Formally,  $g$ is a composition of two function $g={h}({a}(\cdot))$, where $a: \cal{V} \to \cal{A}$, and $h: \cal{A} \to \cal{Y}$~\cite{palatucci2009zero}. The main idea is to collect data that is sufficient to learn an  intermediate attribute layer, where classes are then represented by these attributes to facilitate zero-shot/few-shot learning. However, Chen \emph{et al}.~\cite{chen2014inferring} realized that attribute appearance is dependent on the class,  as opposed to these earlier models~\cite{palatucci2009zero,lampert2009learning,farhadi2009describing}. Although~\cite{chen2014inferring}'s assumption is more realistic, they propose learning different classifiers for each category-attribute pair, which   suffers from the same scalability and learning problems pointed out in (A) and is restricted to certain groups of facts (not uniform).




More recent attribute-based zero-shot learning methods embed both images and attributes into a shared space (e.g.,  Attribute Embedding~\cite{akata2013label}, ESZSL~\cite{romera2015embarrassingly}). These methods were mainly studied in the case of zero-shot learning and have shown strong performance. In contrast, we aim at studying the setting where one system that can learn from both facts with many training images and facts with  few/no training images. Fig~\ref{fig_relwork} shows the contrast between our setting (white circle) and this setting. Although these methods were mainly studied using attributes as a semantic representation and at a much smaller scale of facts, we apply the state of the art ESZSL~\cite{romera2015embarrassingly} in order to study the capacity of these models at a much larger scale.


Recent works in language and vision involve using  unannotated text to improve object recognition and to facilitate zero-shot learning. The following group of  approaches model object recognition as a function
where $s(\cdot,\cdot)$ is a similarity function between image $v$ and class $y$ represented by text.
In~\cite{NIPS13DeViSE},~\cite{norouzi2014zero} and~\cite{NIPS13CMT},  word  embedding language models (e.g.,~\cite{mikolov2013distributed}) were adopted to represent class names as vectors.
In their setting, the imageNet dataset has 1000 object facts with thousands of examples per class.
Our setting has two orders of magnitude more facts with a long-tail distribution. Conversely, other works model the mapping of unstructured text descriptions for classes  into a visual classifier~\cite{elhoseiny2013write,ba2015predicting}. %In contrast, our goal is to gain visual knowledge for not only  first order facts $<$S$>$ but also for second  $<$S$,$P$>$, and third order facts $<$S$,$P$,$O$>$, and also maintain the structure of the facts.
We are extending the visual recognition task to unbounded scale of facts,
not only object recognition but also attributes, actions, and interactions in one model;
see Fig~\ref{fig_relwork} for contrast to our setting. %Our labels are also structured, where we handle missing parts like P and O in  \textless dolphin, *,*\textgreater\ examples and O in  \textless girl, playing,*\textgreater, which enables our method to learn facts of different types by the wild-card loss we introduce. Furthermore, we propose the visual modifiers notion, which motivates us to learn separate branches of convolutional filters that outperforms more straightforward convolutional baselines that we designed for this task; see Sec~\ref{sec_shmdls}. \textbf{(D) Image-Caption Similarity Methods:}  As we illustrated earlier, our goal is fact-level understanding. However, image-caption similarity methods such as~\cite{karpathy2014deep,kiros2014unifying} are relevant as multi-view learning methods. Although it is a different setting, we found two interesting aspects of these methods to study in our setting.  First, how image-caption similarity system trained on image-caption level performs on fact-level understanding. Second, these systems could be retrained in our setting by providing them with fact-level annotation, where every example is  a phrase representing the fact and an image (e.g., ``person riding horse'' and an image  with this fact).


\textbf{(E) MV-CCA : } MV-CCA is a recent multiview, scalable, and robust version of the famous CCA embedding method~\cite{gong2014multi}. We apply MV-CCA as a baseline in our setting. %in the setting we study as we detail later in our experiments. %Let's assume that $|$S$|$, $|$P$|$, and $|$O$|$ denotes the number of  subjects, predicates, and objects, respectively.  %The scale of unique second and third order facts is bounded by $|$S$|\times |$P$|$, and  $|$S$| \times |$P$| \times |$O$|$ possibilities respectively that could easily reach millions of unique facts and needs careful attention while designing a model maintaining the structure. %we aim at.  %The scale of unique second and third order facts could easily reach millions of unique facts and needs careful attention while designing a model maintaining the structure.%however, they suffer from the %aforementioned %scalability limitation. % Do we need to argue about \cite{sadeghi2015viske}






\vspace{-2mm}
\section{Problem Definition: Representation and Visual Modifiers}
\vspace{-2mm}
We deal with three groups of facts; see  Fig.~\ref{fig:imageknowledge}.  First Order Facts \textless S,*,*\textgreater\are object and scene categories (e.g., \textless baby,*,*\textgreater, \textless girl,*,*\textgreater, \textless beach,*,*\textgreater). Second Order Facts \textless S,P,*\textgreater\are objects performing actions or attributed objects (e.g., \textless baby, smiling,*\textgreater, \textless baby, Asian,*\textgreater). Third Order Facts \textless S,P,O\textgreater\are interactions and positional information (e.g. \textless baby, sitting\_on, high\_chair\textgreater, \textless person, riding, horse\textgreater ).  By allowing wild-cards in this structured representation (\textless baby,*,*\textgreater and \textless baby, smiling,*\textgreater), we can not only allow  uniform representation of  different fact types but also relate them by structure.  We propose to model these facts by  embedding them into a structured fact space that has three continuous hyper-dimensions $\phi_S$, $\phi_P$, and $\phi_O$%of dimensionality $d_S$, $d_P$, and $d_O$, respectively \begin{description}
\item [  $\phi_S \in \mathbb{R}^{d_S}$:]  The space of object categories or scenes S.


\item [  $\phi_P \in \mathbb{R}^{d_P}$:]  The space of actions, interactions, attributes, and positional relations.


\item [ $\phi_O \in \mathbb{R}^{d_O}$:]  The space of interacting objects, scenes that interact with S for SPO facts.
  \end{description}%We define ``structured fact space'' as a learning  representation of the  $\phi_S \in \mathbb{R}^{d_S}$, $\phi_P \in \mathbb{R}^{d_P}$, and $\phi_O \in \mathbb{R}^{d_O}$  hyper-dimensions (Fig.~\ref{fig:probdef}).
where ${d_S}$, ${d_P}$, and ${d_O}$ are the dimenstionalities corresponding to $\phi_S$, $\phi_P$, and $\phi_O$, respectively.
As shown in Fig.~\ref{fig:vidmod},
first order facts like \textless woman,*,*\textgreater, \textless man,*,*\textgreater, \textless person,*,*\textgreater\  live in a hyper-plane in the $\phi_P \times \phi_O$ space. Second order facts (e.g., \textless man, walking,*\textgreater, \textless girl, walking,*\textgreater) live as a hyper-line that is parallel to $\phi_O$ axis. Finally, a third order fact like \textless man, walking, dog\textgreater\is a point in the  $\phi_S \times \phi_P \times \phi_O$ visual perception space.
Inspired from the concept of language modifiers, the $\phi_S$,  $\phi_P$, and  $\phi_O$  could be viewed as what we call ``visual modifiers''. For example, the second order fact \textless baby, smiling,* \textgreater\is a $\phi_P$ visual modifier for \textless baby,*,*\textgreater, and the third order fact \textless person, playing, flute\textgreater\is the fact \textless person, *, *\textgreater\ visually modified on both $\phi_P$ and $\phi_O$ axes.  By embedding all language and images into this common space, our algorithm can scale efficiently.  Further, this space can be used to retrieve a language view of an image as well as a visual view of a language description, making the model bi-directional. We argue that modeling visual recognition based on this notion gives it a generalization capability.  For example is if the model learned the facts $<$boy$>$, $<$girl$>$,  $<$boy, petting, dog$>$, $<$girl,  riding,  horse$>$, we would aim at  recognizing an unseen fact $<$boy, petting, horse$>$.  We show these capabilities quantitatively later in our experiments. %We observe this in our results in several unseen cases.% that came as a result from introducing this structure.  \begin{figure}[t!]
\vspace{-4mm}
  \centering
    \includegraphics[width=0.9\textwidth]{vidmod_new.png}%{vis_modifier_fig2.jpg}
      \vspace{-3mm}
    \caption{Unified Fact Representation and Visual Modifiers Notion}
          \vspace{-2mm}
    \label{fig:vidmod}
        \vspace{-5mm}
\end{figure}% This hyper-space allows our algorithm to generalize to second- and third-order facts that were not present in the training data, e.g. even if \textless man, walking, cat\textgreater\ is never seen in the training data that point will exist in the space if S:man, P:walking, and O:cat are in the training data.  Further, since we are using GloVE to vectorize our langauge, subject (first-order facts), predicates, and objects not seen in the training data will have a mapping into the embedding space and can be generalized as well.
We model this setting as a problem with two views, one in the visual domain $\mathcal{V}$ and one in the language domain $\mathcal{L}$.
Let $\mathbf{f}$ be a structured fact,  $\mathbf{f}_v \in \mathcal{V}$ denoting the visual view   of $\mathbf{f}$  and $\mathbf{f}_l \in \mathcal{L}$ denoting the language view of $\mathbf{f}$. For instance, an annotated fact with language view $\mathbf{f}_l = <$S:girl, P:riding, O:bike$>$ would have a corresponding visual view $\mathbf{f}_v$ as an image where this fact occurs; see Fig.~\ref{fig:probdef}.


Our goal is to learn a representation that covers all the three orders of facts.
We denote the embedding functions  from a visual view  to  $\phi_S$, $\phi_P$, and $\phi_O$ as  $\phi^{\mathcal{V}}_S(\cdot)$, $\phi^{\mathcal{V}}_P(\cdot)$, and $\phi^{\mathcal{V}}_O(\cdot)$, and the structured visual embeddings of a fact  $\mathbf{f}_v$ by  $\mathbf{v}_S=\phi^{\mathcal{V}}_S(\mathbf{f}_v)$, $\mathbf{v}_P=\phi^{\mathcal{V}}_P(\mathbf{f}_v)$, and $\mathbf{v}_O=\phi^{\mathcal{V}}_O(\mathbf{f}_v)$, respectively. Similarly, we  denote the embedding functions from a language view to  $\phi_S$, $\phi_P$, and $\phi_O$ as  $\phi^{\mathcal{L}}_S(\cdot)$, $\phi^{\mathcal{L}}_P(\cdot)$, and $\phi^{\mathcal{L}}_O(\cdot)$, and the structured language embeddings of a fact  $\mathbf{f}_l$ as $\mathbf{l}_S=\phi^{\mathcal{L}}_S(\mathbf{f}_l)$, $\mathbf{l}_P=\phi^{\mathcal{L}}_P(\mathbf{f}_l)$, and $\mathbf{l}_O=\phi^{\mathcal{L}}_O(\mathbf{f}_l)$.
\centering\vspace{-7mm}
   \hspace{-4mm} \includegraphics[width=0.52\textwidth,height=0.35\textwidth]{problem3.jpg}
      \vspace{-4mm}
    \caption{Structured Embedding }
      \vspace{-2mm}
    \label{fig:probdef}
    \vspace{-5mm}
\end{wrapfigure} We denote the concatenation of the visual view  hyper-dimensions' embedding as $\mathbf{v}$, and the language view   hyper-dimensions' embedding  as $\mathbf{l}$; see Eq.~\ref{eq1}\begin{comment}
\begin{equation}
\small
\begin{split}
\mathbf{v} = [\mathbf{v}_S, \mathbf{v}_P,   \mathbf{v}_O ],
\mathbf{l}  = [\mathbf{l}_S, \mathbf{l}_P, \mathbf{l}_O]
\end{split}
\label{eq1}
\end{equation}
\end{comment}% $\phi^{\mathcal{V}}_S(\mathbf{f}_v)$, $\phi^{\mathcal{V}}_P(\mathbf{f}_v)$, and $\phi^{\mathcal{V}}_O(\mathbf{f}_v)$as $\phi^{\mathcal{V}}(\mathbf{f}_v)$ = [$\phi^{\mathcal{V}}_S(\mathbf{f}_v)$, $\phi^{\mathcal{V}}_P(\mathbf{f}_v)$,  $\phi^{\mathcal{V}}_O(\mathbf{f}_v)$ ]. Similarly, we denote the concatenation of $\phi^{\mathcal{L}}_S(\mathbf{f}_l)$, $\phi^{\mathcal{L}}_P(\mathbf{f}_l)$, and $\phi^{\mathcal{L}}_O(\mathbf{f}_l)$ as $\phi^{\mathcal{L}}(\mathbf{f}_l)$ = [$\phi^{\mathcal{L}}_S(\mathbf{f}_l)$, $\phi^{\mathcal{L}}_P(\mathbf{f}_l)$,  $\phi^{\mathcal{L}}_O(\mathbf{f}_l)$]. %%This opens the question of what is a training example in Universal visual recognition. %%\ignore{ For example if during training a model learned the facts $<$boy, petting, a dog$>$, $<$girl,  riding, a horse$>$, and $<$boy$>$, it should be able to recognize an unseen fact fact $<$boy, petting, a horse$>$ even though it did not see that fact before}
Third-order facts $<$S,P,O$>$ can be directly embedded in the structured fact space by Eq.~\ref{eq1} with $\mathbf{v} \in  \mathbb{R}^{d_S} \times \mathbb{R}^{d_P} \times \mathbb{R}^{d_O}$ for the image view and $\mathbf{l}  \in  \mathbb{R}^{d_S} \times \mathbb{R}^{d_P} \times \mathbb{R}^{d_O}$ for the language view. %Next, we detail how we embed first- and second-order facts and how it related to the visual modifiers  and the wild-card representation.%A question remains how first-  and second-order facts be represented in Eq.~\ref{eq1} so that we have a unified fact learning model that covers facts of all orders. In the remainder of this section, we present the notion of fact modifiers and how they are reflected in our proposed learning representation.  %Hence, we propose what we call wild-card representation, which we elaborate next.%%\subsection{High order facts as modifiers of lower order facts}%%\textbf{High order facts as modifiers:}%%First-order facts are facts that indicate an object like $<$S: person$>$. Second-order facts gets more specific about the subject (e.g. $<$S: person, P: playing$>$). Third-order facts get even more specific ($<$S: person, P: playing, O: piano$>$). %Inspired by the concept of modifiers in language grammar, we propose to define higher order facts as lower order facts with an additional modifier applied to it. %%(i.e., first order fact is the lowest order). %For example, adding the modifier P: eating to the fact $<$S: kid$>$, constructs the fact $<$S: kid, P: eating$>$. Further applying the modifier O: ice cream to the fact  $<$S: kid, P: eating$>$, constructs the fact $<$S: kid, P: eating, O: ice cream$>$. %%Similarly, attributes could be seen as modifiers to a subject. %(e.g., applying P: smiling to fact $<$S: baby$>$ constructs the fact $<$S: baby, P: smiling$>$). %%%\subsection{Lower order facts as wild-card representation of higher order facts}%\textbf{Wild-card representation:}%
Based on our ``fact modifier'' observation, we propose to represent both
second and first-order
facts as wild cards ``$*$'', as illustrated in Eq.~\ref{eq2},~\ref{eq3}; see Fig~\ref{fig:probdef},~\ref{fig:vidmod}.
\begin{align}
\textbf{Third-Order Facts $<$S,P,O$>$:}\;\; &
\mathbf{v} = [\mathbf{v}_S, \mathbf{v}_P,   \mathbf{v}_O ] &&
\mathbf{l}  = [\mathbf{l}_S, \mathbf{l}_P, \mathbf{l}_O] \label{eq1} \\
\textbf{Second-Order Facts $<$S, P,*$>$:}\;\; &
\mathbf{v}  = [\mathbf{v}_S, \mathbf{v}_P ,  \mathbf{v}_O = *] &&
\mathbf{l} = [\mathbf{l}_S , \mathbf{l}_P ,  \mathbf{l}_O = *] \label{eq2} \\
\textbf{First-Order Facts $<$S,*,*$>$:}\;\; &
\mathbf{v} = [\mathbf{v}_S, \mathbf{v}_P = *,  \mathbf{v}_O = *]  &&
\mathbf{l} = [\mathbf{l}_S , \mathbf{l}_P = *,  \mathbf{l}_O = *] \label{eq3}
\end{align}
}\begin{comment}
 \vspace{-2mm}
\noindent\textbf{First-Order Facts wild-card representation $<$S,*,*$>$}
\begin{equation}
\small
\begin{split}
\mathbf{v}  = [\mathbf{v}_S, \mathbf{v}_P = *,  \mathbf{v}_O = *],
\mathbf{l} = [\mathbf{l}_S , \mathbf{l}_P = *,  \mathbf{l}_O = *]
\end{split}
\label{eq2}
 \vspace{-3mm}
\end{equation}
\noindent\textbf{Second-Order Facts wild-card representation  $<$S, P,*$>$}
 \vspace{-3mm}
\begin{equation}
\small
\begin{split}
\mathbf{v}  = [\mathbf{v}_S, \mathbf{v}_P ,  \mathbf{v}_O = *],
\mathbf{l} = [\mathbf{l}_S , \mathbf{l}_P ,  \mathbf{l}_O = *]
\end{split}
\label{eq3}
 \vspace{-2mm}
\end{equation}
\end{comment}
Setting $\phi_P$ and $\phi_O$  to $*$ for first-order facts means that the $P$ and $O$ modifiers are not of interest for first-order facts, which is intuitive. Similarly, setting $\phi_O$ to $*$ for second-order facts indicates that the $O$ modifier is not of interest for single-frame actions and attributed objects.
\begin{comment}
Lower order facts do not necessarily mean that a higher order fact does not exist in it. For example, $<$person$>$ fact in an image does not mean that he is not performing an action or has a particular attribute like tall. It rather means that we don't know. Hence, the wild cards (i.e. $*$) of the lower order facts are not penalized during training in our loss, as we illustrate later at the end of Sec.~\ref{sec_shmdls}. We name both first and second-order facts as wild-card fact.  %Since modeling structured facts in visual data potentially allows logical reasoning over facts from images, we denote the described problem as the Sherlock problem.
\end{comment}
If an image contains  lower order fact such as $<$man$>$, then higher order facts such as $<$man, tall$>$ or $<$man, walking, dog$>$ may also be present. Hence, the wild cards (i.e. $*$) of the first- and second-order facts are not penalized during training.
\begin{figure*}[t!]


   \centering
    \begin{subfigure}[b]{0.42\textwidth}
        \includegraphics[width=\textwidth]{sherlock_model0.png}
        \vspace{-8mm}
        \caption{Model 1}
        \label{fig:mdl1}
    \end{subfigure}
\rulesep
      \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{sherlock_model1.png}
        \vspace{-8mm}
        \caption{Model 2}
        \label{fig:mdl2}
    \end{subfigure}
    \vspace{-2mm}
 \caption{Sherlock Models}
 \label{fig:mdls}
\end{figure*}
\end{comment}%A straightforward way to model facts in images is to learn a classifier for each separate fact. Although, there is a clear scalability limitation in this technique as the number of facts is in the complexity of $|S|\times |P| \times |O|$, where $|S|$, $|P|$, and $|O|$  are  the number of subjects, predicates, and objects respectively. It is not hard to see that $|S|\times |P| \times |O|$ could easily reach millions for possible facts in the real world, even in our scale. In additional to scalability problems,  this technique discards  semantic relationship between facts, which is the key property to allow generalization to unseen facts or facts with very few examples. For instance, during training, there might be a second-order fact like $<$S: boy, P: playing$>$, and  a first-order fact like $<$S: girl$>$, $<$S: boy$>$ . In test time, we expect that a good model would understand an image with the fact $<$girl, playing$>$, even if this fact is not seen during training, which is clearly not captured by learning a model for each fact in the training.  The aforementioned arguments  also further justify why we aim to modeling structured facts as a two view embedding problem.




\vspace{-2mm}
\section{Models}
\label{sec_shmdls}
We propose a two-view structured fact embedding model  with five properties mentioned in Sec~\ref{lbl_intro}.
Satisfying the first four  properties can be achieved by using a generative model  $p(\mathbf{f}_v, \mathbf{f}_l)$ that connects the visual and the language views of $\mathbf{f}$, where more importantly $\mathbf{f}_v$ and $\mathbf{f}_l$ inhabit a continuous space. We model  $p(\mathbf{f}_v, \mathbf{f}_l) \propto s ( \mathbf{v},  \mathbf{l})$, where $s(\cdot, \cdot)$ is a similarity function defined over the structured fact space. We satisfy the fifth property by building our models over  the aforementioned structured wild card representation.
Our objective is that two views of the same fact should be embedded so that they are close to each other; see Fig~\ref{fig:probdef}. The question now is how to model and train $\phi^{\mathcal{V}}(\cdot)$ visual functions ($\phi_S^{\mathcal{V}}(\cdot),\phi_P^{\mathcal{V}}(\cdot),\phi_O^{\mathcal{V}}(\cdot)$) and $\phi^{\mathcal{L}} (\cdot)$ language functions ($\phi_S^{\mathcal{L}}(\cdot),\phi_P^{\mathcal{L}}(\cdot),\phi_O^{\mathcal{L}}(\cdot)$) . We model $\phi^{\mathcal{V}}(\cdot)$ as a CNN encoder (e.g.,~\cite{krizhevsky2012imagenet,simonyan2014very}), and $\phi^{\mathcal{L}} (\cdot)$ as  RNN encoder (e.g.,~\cite{mikolov2013distributed,pennington2014glove}) due to their recent success as encoders for images and words, respectively. We  propose two models for learning facts, denoted by Model 1 and Model 2.  Both models share the same structured fact language embedding/encoder but differ in the structured fact image encoder.


We start by defining an activation operator $\psi(\theta, a)$, where $a$ is an input, and $\theta$ is a series of one or more neural network layers (may include different layer types, e.g.,  convolution,  pooling, then another convolution and pooling). The operator $\psi(\theta, a)$ applies $\theta$  parameters layer by layer to compute the final activation of $a$ using $\theta$  subnetwork. %We will use the operator $\psi(\cdot, \cdot)$ to define Model 1 and Model 2 structured fact image encoders. \noindent\textbf{Model 1 (structured fact CNN image encoder): } In Model 1, a structured fact is visually encoded by sharing convolutional layer parameters (denoted by $\theta_c$), and fully connected layer parameters  (denoted by $\theta_u$); see
Fig.~\ref{fig:mdls2}(a).
Then $W^S$,  $W^P$, and $W^O$ transformation matrices are applied to produce $\mathbf{v}_S = \phi_S^\mathcal{V}(\mathbf{f}_v)$,$\mathbf{v}_P =\phi_P^\mathcal{V}(\mathbf{f}_v)$ , and $\mathbf{v}_O =\phi_O^\mathcal{V}(\mathbf{f}_v)$. If we define $b = \psi( \theta_u, \psi(\theta_c, \mathbf{f}_v))$, then
\begin{comment}
\begin{equation}
\small
\begin{split}
&\phi_S^\mathcal{V}(\mathbf{f}_v) =  W^S \psi( \theta_u, \psi(\theta_c, \mathbf{f}_v)),
\phi_P^\mathcal{V}(\mathbf{f}_v) =  W^P\psi( \theta_u, \psi(\theta_c, \mathbf{f}_v)), \\ &
\phi_O^\mathcal{V}(\mathbf{f}_v) =  W^O\psi( \theta_u, \psi(\theta_c, \mathbf{f}_v))
\end{split}
\label{eq_mdl1}
\end{equation}
\begin{equation}
\small
b = \psi( \theta_v^u, \psi(\theta_v^c, \mathbf{f}_v)), \;\;
 \phi_S^\mathcal{V}(\mathbf{f}_v) =  W_v^S \; b,  \;\;
\phi_P^\mathcal{V}(\mathbf{f}_v) =  W_v^P \; b,   \;\;
\phi_O^\mathcal{V}(\mathbf{f}_v) =  W_v^O \; b.
\label{eq_mdl1}
\end{equation}
\end{comment}\begin{equation}
\small
\mathbf{v}_S = \phi_S^\mathcal{V}(\mathbf{f}_v) =  W^S \; b,  \;\;
\mathbf{v}_P= \phi_P^\mathcal{V}(\mathbf{f}_v) =  W^P \; b,   \;\;
\mathbf{v}_O= \phi_O^\mathcal{V}(\mathbf{f}_v) =  W^O \; b.
\label{eq_mdl1}
\end{equation}\begin{figure}[t!]
\begin{center}
\vspace{-10mm}
 \includegraphics[width=1.0\textwidth]{sherlock_models_new_one_row_2.jpg}
\end{center}
 \vspace{-4mm}
 \caption{Sherlock Models. See Fig.~\ref{fig:probdef} for the full picture.}
 \vspace{-5mm}
\label{fig:mdls2}
\end{figure}\vspace{-1mm}\noindent\textbf{Model 2 (structured fact CNN image encoder): } In contrast to Model 1, we use different convolutional layers for $S$ than that for $P$ and $O$, inspired by the idea that $P$ and $O$ are modifiers to $S$%(Fig.~\ref{fig:mdl2}).
(Fig.~\ref{fig:mdls2}(b)).
Starting from $\mathbf{f}_v$, there is a common set of convolutional layers, denoted by $\theta_{c}^{0}$, then the network splits into two branches, producing two sets of convolutional layers $\theta_c^{S}$ and $\theta_c^{{PO}}$, followed by two sets of fully connected layers $\theta_u^{S}$ and $\theta_u^{{PO}}$.
\begin{comment}
Finally $\phi_S^\mathcal{V}(\mathbf{f}_v)$,$\phi_P^\mathcal{V}(\mathbf{f}_v)$ , and $\phi_O^\mathcal{V}(\mathbf{f}_v)$ are computed by applying $W^S$,  $W^P$, and $W^O$ transformation matrices:
\begin{equation}
\small
\begin{split}
&\phi_S^\mathcal{V}(\mathbf{f}_v) =  W^S \psi( \theta_u^{S}, \psi(\theta_{c}^{S},  \psi( \theta_{c}^{0}, \mathbf{f}_v))),
\phi_P^\mathcal{V}(\mathbf{f}_v) =  W_v^P\psi( \theta_u^{{PO}}, \psi( \theta_{c}^{{PO}},  \psi( \theta_{c}^0, \mathbf{f}_v))), \\ &
\phi_O^\mathcal{V}(\mathbf{f}_v) =  W_v^O\psi( \theta_u^{{PO}}, \psi(\theta_{c}^{{PO}},  \psi( \theta_{c}^0, \mathbf{f}_v)))
\end{split}
\label{eq_mdl2}
\end{equation}
\end{comment}%\begin{comment}
If we define the output of the common S,P,O layers as $d = \psi( \theta_{c}^0, \mathbf{f}_v)$ and the output of the P,O column as $e = \psi( \theta_u^{{PO}},\psi(\theta_{c}^{{PO}}, d))$, then
\small
\mathbf{v}_S= \phi_S^\mathcal{V}(\mathbf{f}_v) =  W^S \; \psi( \theta_u^{S},  \psi(\theta_{c}^{S}, d)), \; \mathbf{v}_P=
\phi_P^\mathcal{V}(\mathbf{f}_v) =  W^P \; e, \;\;\mathbf{v}_O=
\phi_O^\mathcal{V}(\mathbf{f}_v) =  W^O \; e.
\label{eq_mdl2}
\end{equation}%\end{comment}%In both models,\noindent\textbf{Structured fact RNN language encoder: }
The structured fact language view is encoded using RNN word embedding vectors for $S$, $P$ and, $O$ separately.
Hence
\begin{equation}
\small
\mathbf{l}_S =\phi_S^\mathcal{L}(\mathbf{f}_l) =  \mbox{RNN}_{\theta_l}(\mathbf{f}_l^S), \mathbf{l}_P =\phi_P^\mathcal{L}(\mathbf{f}_l) =  \mbox{RNN}_{\theta_l}(\mathbf{f}_l^P), \mathbf{l}_O=\phi_O^\mathcal{L}(\mathbf{f}_l) =  \mbox{RNN}_{\theta_l}(\mathbf{f}_l^O)
\label{eq_lang_enc}
\end{equation}
where $\mathbf{f}_l^S$, $\mathbf{f}_l^P$, and $\mathbf{f}_l^O$ are the Subject, Predicate, and Object parts of $\mathbf{f}_l \in \mathcal{L}$. For each of them, the literals are dropped. In our experiments, $\theta_l$ is fixed to a pre-trained word vector embedding model (e.g.~\cite{mikolov2013distributed,pennington2014glove})  for $\mathbf{f}_l^S$, $\mathbf{f}_l^P$, and $\mathbf{f}_l^O$; see Fig~\ref{fig:mdls2}(c).


One way to model $p(\mathbf{f}_v, \mathbf{f}_l)$ for Model 1 and Model 2 is to assume that $p(\mathbf{f}_v, \mathbf{f}_l) \propto = \mbox{exp}(-\mbox{loss}_w(\mathbf{f}_v, \mathbf{f}_l))$ and minimize the distance $\mbox{loss}_w(\mathbf{f}_v, \mathbf{f}_l)$ defined as
\begin{comment}
 We define the loss on both Model 1 and 2 as follows
\begin{equation}
\begin{split}
\mbox{loss}(\mathbf{f}_v, \mathbf{f}_l) =&\|\mathbf{v}_S -  \mathbf{l}_S \|^2  + \\& \|\mathbf{v}_P -  \mathbf{l}_P \|^2  + \\& \|\mathbf{v}_O -  \mathbf{l}_O \| ^2,
\end{split}
\end{equation}
Thus we minimize the distances between the embedding of the visual view to be close the language view as possible. A question remains how to penalize wild-card facts (i.e. first- and second-order facts that have wild cards). Our solution is to ignore the wild-card modifiers in the loss, which we defined as follows
\begin{equation*}
\small
\begin{split}
\mbox{loss}_w(\mathbf{f}_v, \mathbf{f}_l) = w_S^{\mathbf{f}} \cdot \| \mathbf{v}_S -  \mathbf{l}_S  \|^2  + w_P^{\mathbf{f}} \cdot  \|\mathbf{v}_P -  \mathbf{l}_P  \|^2  +  w_O^{\mathbf{f}} \cdot \|\mathbf{v}_O -  \mathbf{l}_O  \|^2.
\end{split}
\end{equation*}
\end{comment}%We define the loss on both Model 1 and 2 as follows\begin{equation}
\small
\begin{split}
\mbox{loss}_w(\mathbf{f}_v, \mathbf{f}_l) = w_S^{\mathbf{f}} \cdot D( \mathbf{v}_S , \mathbf{l}_S )  + w_P^{\mathbf{f}} \cdot D(\mathbf{v}_P ,  \mathbf{l}_P ) +  w_O^{\mathbf{f}} \cdot D(\mathbf{v}_O , \mathbf{l}_O).
\end{split}
\label{eq_euc}
\end{equation}
where $D(\cdot,\cdot)$ is a distance function.
Thus we minimize the distance between the embeddings of the visual view and the language view. Our solution to penalize wild-card facts is to ignore their wild-card modifiers in the loss. Here $w_S^{\mathbf{f}}=1$, $w_P^{\mathbf{f}}=1$, $w_O^{\mathbf{f}}=1$ for $<$S,P,O$>$ facts , $w_S^{\mathbf{f}}=1$, $w_P^{\mathbf{f}}=1$, $w_O^{\mathbf{f}}=0$ for $<$S,P$>$ facts, and $w_S^{\mathbf{f}}=1$, $w_P^{\mathbf{f}}=0$, $w_O^{\mathbf{f}}=0$ for $<$S$>$ facts.  Hence $\mbox{loss}_w$ does not penalize the $O$ modifier for second-order facts or the $P$ and $O$ modifiers for first-order facts, which follows our definition of wild-cards.  In this paper, we used $D(\cdot,\cdot)$ as the standard Euclidean distance. %over L2 normalized vectors.  In order to introduce more discriminating power for the model,%The loss function could be further developed to include negative pairs that%should be far from each other. Equation~\ref{eq_rankimg} shows a more discriminative version of the loss%\begin{equation}%\small%\begin{split}%\mbox{loss}_w^d(\mathbf{f}_v, \mathbf{f}_l) =& w_S^{\mathbf{f}} \cdot ( y_S^{\mathbf{f}} \cdot D( \mathbf{v}_S ,  \mathbf{l}_S  )^2 + (1- y_S^{\mathbf{f}}) \cdot max(m- D(  \mathbf{v}_S ,  \mathbf{l}_S ) , 0)  ) + \\%& w_P^{\mathbf{f}} \cdot ( y_P^{\mathbf{f}} \cdot D (\mathbf{v}_P ,  \mathbf{l}_P ) + (1- y_P^{\mathbf{f}}) \cdot max(m-D(  \mathbf{v}_P ,  \mathbf{l}_P ) , 0)  ) +%\\% & w_O^{\mathbf{f}} \cdot ( y_O^{\mathbf{f}} \cdot D( \mathbf{v}_O ,  \mathbf{l}_O  ) + (1- y_O^{\mathbf{f}}) \cdot max(m- D( \mathbf{v}_O ,  \mathbf{l}_O  ) , 0)  )%\end{split}%\label{eq_rankimg}%\end{equation}%where $m$ is the margin, $y_O^{\mathbf{f}}=1, y_P^{\mathbf{f}}=1,$ and $y_P^{\mathbf{f}}=1$  for all ground truth pairs. For the negative sample pairs, $y_S^{\mathbf{f}}=1$ if the negative fact shares the same subject, $0$ otherwise.  $y_P^{\mathbf{f}}=1$ if the negative fact shares the same predicate, $0$ otherwise.  $y_O^{\mathbf{f}}=1$ if the negative fact shares the same object, $0$ otherwise.  \textbf{Testing (Two-view retrieval):} After training  a  model (either Model 1 or 2), we embed all the testing $\mathbf{f}_v$s (images) by  the learnt models, and similarly embed all the test $\mathbf{f}_l$s as shown in Eq~\ref{eq_lang_enc}. For language view retrieval (retrieve relevant facts in language given an image), we compute the distance between the structured embedding of an image $\mathbf{v}$ and all the  facts structured language embeddings $\mathbf{l}$s, which indicates relevance for each fact $\mathbf{f}_l$ for the given image. For visual view  retrieval (retrieve relevant images given fact in language form), we  compute the distance  between the structured embedding of the given fact  $\mathbf{l}$ and all structured visual embedding of images $\mathbf{v}$s in the test set.  For first and second order  facts, the wild-card part is ignored while computing the distance.


So far we have argued that a set of structured knowledge tuples is a good representation for images and we have proposed to use NLP methods to build a large training set from free-form text descriptions of images. Now we discuss how we model the relationship between structured textual information and visual information.


We build a joint probabilistic model P(<Subject,Attribute>,Image I), P(<Subject,Predicate,Object>,Image I) to output the probability that image I and structured text <Subject,Attribute> or <Subject,Predicate,Object> represent the same real world concept visually and textually. We choose our model to generalize well to unseen or rarely seen combinations of subjects, attributes, predicates, and objects, and to NOT require explicitly reducing from a large vocabulary of individual words to a small, pre-defined set of concepts. Text-based image search is mapping a text query (represented as a set of structured knowledge using an NLP tuple extraction method) to images, and is supported by our joint model by looping over images I and checking which gives a high probability P(structured text <S,P,O>, image I) for a given fact <S,P,O>. Knowledge extraction/tagging is supported by our model by looping over possible facts <S,P,O> and checking which gives a high probability P(structured text <S,P,O>, image I) for a given image or image region I.


The question is how to correlate text information and visual information. There are two parts to the modeling: (i) the feature representation for the structured text <S,P,O>, <S,A,->,<S,-,-> (where - indicates an unused slot to represent all facts as triples) and for images, and (ii) the model for correlating text feature t and image feature x: P(t,x).


Text Feature


C1. To support generalization and the use of a large vocabulary, we represent <S,P,O> and <S,A> tuples such that similar structured knowledge concepts have nearby and related representations, for example as vectors in a vector space. For example, the feature representations of <road,curvy> and <road,winding> should be similar and the representations between <dog,walking> and <person,walking> should be related by the common action of 'walking'.


There has been a lot of recent work such as word2vec and GLOVE in developing representations for single words that capture the semantic content and context in which a word is used. In both the word2vec and GLOVE work, single words are mapped into a vector space (based on the context in which a word is used) such that similar words are nearby in the space and the vector space captures some relationships between words. For example, vec(man)+(vec(queen)-vec(woman))=vec(king). We extend semantic text embedding to structured knowledge.


C1.1. Build upon existing semantic vector representations of single words to develop a vector representation of knowledge tuples which captures the relationship between two facts <S1,P1,O1> and <S2,P2,O2>. Specifically we build a feature vector for an <S,P,O> triple as a function of  the single word representations vec(S), vec(P), and vec(O). We use the state-of-the-art GLOVE vector embedding for single words. We build vec(<S,P,O>) as the concatenation of the individual word vectors: vec(<S,P,O>)=[vec(S) vec(P) vec(O)]. When an <S,P,O> element is missing, for example the Object O when representing a <Subject, Attribute> or both the Predicate P and Object O when representing just a <Subject>, we fill in the corresponding vector slot with all zeros. Thus the vector representation for just a Subject would lie along the S axis in SPO space. We think of visual attributes as modifiers for an unadorned Subject that move the representation of an <S,P> into the SP plane of SPO space. [MORE JUSTIFICATION IN MOHAMEDs TECHNICAL WRITE-UP TO BE DONE LATER]


Another option here is to sum the vector representations of the individual words, but we want to preserve as much information as possible.


The description in C1.1 above for our text feature representation is still incomplete because any of the three S, P, and O components can actually be a compound phrase instead of a single word, for example P=running toward or P=running away from or P=next to or P=close to.


C1.2. For a compound S or P or O, we average the vector representation for each individual word in the phrase to insert a single vector into the target slot of our [vec(S) vec(P) vec(O)] representation. For example, vec(running toward)=0.5*(vec(running)+vec(toward)). A non-uniform weight average may also be used when some words in the phrase carry more meaning than others.


An even better idea that we did not explore yet for this problem is


C1.2.1. Learn a semantic representation (vector or probability distribution) directly for compound phrases such as running toward or running away from by treating these phrases atomically as new vocabulary elements in an existing semantic word embedding model. For example, we can use the same context idea of predicting the central vocabulary element in a multiple element sentence as done in word2vec.


Image Feature


There are many choices of image features to capture semantics in the computer vision literature (e.g. SIFT or HOG), but the current state-of-the-art in image modeling is to use a deep network with many levels of features which are learned directly from the data for a particular task such as image classification. In particular, convolution neural networks (CNNs) with convolution, pooling, and activation layers (e.g. Rectified Linear Units or ReLus that threshold activity) have been proven to be state-of-the-art for image classification. Examples include AlexNet, VGGNet, and GoogLeNet. Perhaps equally importantly, classification features from deep classification nets have been shown to give high quality results on other tasks (e.g. segmentation), especially after fine tuning these features for the other task. Starting from features learned for classification and fine tuning these features for another image understanding task is also much more efficient in terms of training than starting training from scratch for a new task.


For the reasons above, we adopt CNN features as fixed features in our baseline linear CCA model and we fine tune from a CNN in our deep network for correlating text and images (both models discussed below) in our image description/knowledge extraction task. For a fixed image feature in the CCA model, we use AlexNet POOL5 layer. For our task, we found using layers that still capture some positional information to be better than using later/higher layers where most or all of the positional information is absent. It is also possible to concatenate multiple layers of features such as in the ZoomOut and HyperColumn work to form an image feature vector x that captures low, mid, and high level image features.


Modeling Correlation between Text and Images


Our main idea is to map text features t and image features x into a common vector space and penalize differences in the mapped features when the same or similar concepts are represented by t and x.


Linear CCA Model for Correlating Text and Images


Our baseline method is a linear mapping called Canonical Correlation Analysis (CCA). We apply this general method on text and image features. In CCA, we discover matrices T and X that map feature vectors t and x respectively into a common vector space: $t= Tt$ and $x=Xx$. If we map into a common space of dimension D, and t is a vector in D\_t-dimensional space, and x is a vector in D\_x-dimensional space, then T is (D by D\_t) matrix, X is a ($D$ by $D_x$) matrix, and the mapped representations t and x are D-dimensional vectors. We have experimented with mapping into D=300 dimensional space. We can use loss functions for model fitting using training pairs (t,x) based on squared Euclidean distance $||t-x||_2^2$ or the cosine similarity $dot\_product(t,x)$ or the $angle\_between(t,x)$ which removes the vector length from the cosine similarity measure.


When we use the dot product, then the CCA correlation function is


$f(t,x) = f_CCA_dp(t,x) = tr(Tt) * Xx = tr(t)*M*x = sum_{i,j} t_i M_{ij} x_j,$


where $tr=transpose, M=tr(T)*X$ is $(D_t$ by $D_x)$, and subscripts indicate vector components.


Such a simple form may enable faster than exhaustive search for images or text given the other. For example, in text-based image search we need to find images with feature vectors x such that $dot_prod(v,x)$ is large, where $v=tr(t)*M$.


For the squared Euclidean loss, the CCA correlation function is


$f(t,x) = f\_CCA\_E(t,x) = ||T t  X x||_2^2$.


Again, the simple closed form of the correlation function may enable faster than exhaustive search for images or text given the other. For example, in text-based image search we need to find images with feature vectors x such that f\_CCA\_E(t,x) is small for a given text vector t. Given (T,X) from fitting the CCA model and the query t, linear algebra provides the set of vectors that minimize f(t,x) and we need to find images with feature vector x close to this set.


The advantage of the linear CCA model over a deep network is that CCA is a relatively small model that is very fast to train and it provides close form correlation functions that may enable faster than linear search for the closest images to a given text query or the closest structured knowledge for a given image. But the disadvantage compared to a deep network is the modeling power, where the deep network has a highly non-linear correlation function f(t,x). Our preliminary results for knowledge extraction and search show that our deep network that fine-tunes image classification features is better than the CCA method. In a small scale experiment with 12 human actions, the linear CCA model was able to obtain the ground truth text tuple for a given input image as the nearest neighbor for 31.5% percent of query image regions while an initial deep, fine-tuned model had a top-1 accuracy of 63.5%. For search in this setting, CCA achieved an area under the ROC curve (AUC) of false positive rate versus false negative rate of 0.56 while the deep model had an AUC of 0.91. Our preferred model is a deep network described below.


When the linear CCA model does not work well, there are two potential reasons. (i) The required information is captured in the text and image feature vectors, but a single global matrix M or pair of matrices T and X is not enough to capture the correlations over the whole cross product space (t,x) of all textual and visual data. (We have also thought about a piecewise CCA model with different M in different areas of the (t,x) space, but we did not implement it because we suspected that the deep model would be as easy to implement and would work better.) (ii) The second reason for failure in our CCA experiment is that we kept the text and image feature vectors fixed, and the GLOVE or AlexNet features may not have captured the required information that needs to be correlated.


The more likely candidate for missing information useful for our correlation task between facts about object attributes and interactions is the AlexNet features because those features were computed to do classification. If detecting certain attributes (e.g. smiling) is useful for discriminating a certain class or set of classes from other classes, then that information will be present in the learned image feature vector. If not, then the network would spend its parameters on another discriminative feature for classification. For this reason, our deep model fine tunes the CNN features for classifications to improve the ability to correlate text and image features.


Deep Network for Correlating Text and Images


Our preferred modeling idea is


D1. Use a two column deep network to learn the correlation f(<S,P,O>,I) between structured text knowledge <S,P,O> and image or image region I by non-linear mapping into a common space.
The text column starts with a semantic text vector representation t as described in (3a) and passes through sets of fully connected and activation layers to output a non-linear mapping t->t.
The image column is a deep convolutional net (such as AlexNet or VGGNet or GoogLeNet) (with the final layers mapping to probabilities of class removed) that starts from image pixels and outputs a feature vector x. The image column is initialized as the training result of an existing CNN (currently we use AlexNet) and the image features are fine tuned to correlate images with structured text capturing image attributes and interactions (instead of just object class discrimination as in the original CNN).
The additional layers in the text column above the fixed semantic text representation allow that representation to be adapted according to a non-linear function to map it into a common space with image features representing the same concept.
A loss layer at the top joins the columns and penalizes differences in the outputs t and x of the text and image columns to encourage mapping into a common space for the same concept. A discriminative loss function such as a ranking loss may be used to ensure that mismatched text and images have smaller correlation than correctly matched text and images. For example, a simple ranking loss function may require correlations dot\_prod($t_i$,$x_i$) > dot\_prod($t_j$,$x_i$) for a training example ($t_i$,$x_i$) and where the original tuple for training tuple $t_j$ does not match training image $x_i$. A ranking loss may also use the semantic text similarity from (3a) or an external object hierarchy such as ImageNet to formulate the loss to non-uniformly penalize different mismatches.
As example deep network architecture is shown below. Other loss functions and architectures are possible, for example with fewer or more adaptation layers between the semantic text representation t=[vec(S),vec(P),vec(O)] and the embedding space t or with connections between text and image layers before the common embedding space.








(We have also considered using a deep Boltzmann machine for learning a generative model, but have not yet explored an implementation.)


(4) The Knowledge Extraction Problem: Putting it all together


Thus far we have discussed the problem of extracting a fact relevant to an image region.


E1. We can apply the modeling above for P(Fact <S,P,O>,Image I) to extract all high probability facts about a region.


That is, we do not have to choose just the most probable fact. Consider an image region that contains a smiling man who is wearing a blue shirt. The image pixel data I for this region will have high correlation with both <man, smiling> and <man, wearning, blue shirt>, we extract both of these facts for the same image region.


Recall that our goal is to extract a set of structure knowledge facts about a given input image.


E2. We can solve this knowledge extraction task by applying the above model with image pixel data from regions identified by an object proposal algorithm or object regions identified by the R-CNN algorithm or even in a sliding window approach that more densely samples image regions. For capturing object interactions, we can generate bounding boxes from pairs of object proposals or pairs of R-CNN object regions. One approach is to try all pairs of potential object regions. Another approach is to apply some heuristics to be more selective. For example, we might not examine pairs that are distant in image.


E3. Since we apply our model to extract zero, one, or more high probability facts about an image region, we can localize the extracted <S,P,O> facts to image regions that provide the corresponding visual data.


\end{comment}








\vspace{-2mm}
\section{Experiments}
\label{sec_exp}
\vspace{-2mm}\subsection{Data Collection of Structured Facts}\vspace{-2mm}\label{sec_data}
In order to train a
model that connects the structured fact language view in $\mathcal{L}$ with its visual view in $\mathcal{V}$,  we need to collect large scale data in the form of ($\mathbf{f}_v$, $\mathbf{f}_l$) pairs. Large scale data collection is  challenging in our setting since it  relies on the localized association of a structured language fact $\mathbf{f}_l$ with an image $\mathbf{f}_v$ when such facts occur. In particular, it is a complex task to collect annotations for second-order facts and third-order facts.  %Also, multiple structured language facts could be assigned to the same image (e.g., $<$S:  man, P: smiling$>$ and $<$S: man, P: wearing, O: glasses$>$). If these facts refer to the same man, the same image example could be used to learn about both facts. \begin{comment}
\begin{wraptable}{r}{0.6\textwidth} \vspace{-9mm}
  \centering
  \caption{Coverage of facts in the Large Scale Setting}
  \vspace{-3mm}
    \label{tab_lsc_setting}%
    \scalebox{0.8}{
    \begin{tabular}{|c|c|c|c|c|}
  \hline
          & \textbf{S} & \textbf{SP} & \textbf{SPO} & \textbf{Total} \\
  \hline
    \textbf{Training All facts} & 6116  & 57681 & 107472 & 171269 \\
    \textbf{Testing All facts} & 2733  & 22237 & 33447 & \textbf{58417} \\
    \textbf{Training/Test Intersection} & 1923  & 13043 & 11774 & 26740 \\
    \hline
    \textbf{Test unseen facts } & \textbf{810} & \textbf{9194} & \textbf{21673} & \textbf{31677} \\
    \hline
    \end{tabular}%
    }
    \vspace{-3mm}
\end{wraptable}




\begin{wraptable}{r}{0.6\textwidth} \vspace{-7mm}
\centering \caption{Our fact augmentation of six  datasets}
\vspace{-4mm}
  \label{tbl_6DS}
  \scalebox{0.55}{
    \begin{tabular}{|c|ccc|c|ccc|c|}
 \hline
          & \multicolumn{3}{c}{\textbf{Unique language views $\mathbf{f}_l$}} & \textbf{} & \multicolumn{4}{c|}{\textbf{Number of ( $\mathbf{f}_v, \mathbf{f}_l$) pairs}} \\
   \hline
          &\small \textbf{$<S>$} \normalsize. & \small \textbf{$<S,P>$}\normalsize. & \small \textbf{$<S,P,O>$} \normalsize. & \textbf{total} & \small \textbf{$<S>$} \normalsize & \small \textbf{$<S,P>$}\normalsize & \small \textbf{$<S,P,O>$} \normalsize & \textbf{total images} \\
   \hline
    \textbf{INTERACT} & 0     & 0     & 60    & 60    & 0     & 0     & 3171  & 3171 \\
    \textbf{VisualPhrases} & 11    & 4     & 17    & 32    & 3594  & 372   & 1745  & 5711 \\
    \textbf{Stanford40} & 0     & 11    & 29    & 40    & 0     & 2886  & 6646  & 9532 \\
    \textbf{PPMI} & 0     & 0     & 24    & 24    & 0     & 0     & 4209  & 4209 \\
    \textbf{SPORT} & 14    & 0     & 6     & 20    & 398   & 0     & 300   & 698 \\
    \textbf{Pascal Actions} & 0     & 5     & 5     & 10    & 0     & 2640  & 2663  & 5303 \\
    \hline
    \textbf{Union} & \textbf{25} & \textbf{20} & \textbf{141} & \textbf{186} & \textbf{3992} & \textbf{5898} & \textbf{18734} & \textbf{28624} \\
 \hline
    \end{tabular}%
    }  \vspace{-7mm} \end{wraptable}
\end{comment}\begin{table}[b!]
\vspace{-4mm}
\begin{minipage}{.6\linewidth}
\centering \caption{Our fact augmentation of six  datasets}
  \label{tbl_6DS}
  \scalebox{0.7}{
    \begin{tabular}{|c|ccc|c|ccc|c|}
 \hline
          & \multicolumn{3}{c}{\textbf{Unique language views $\mathbf{f}_l$}} & \textbf{} & \multicolumn{4}{c|}{\textbf{Number of ( $\mathbf{f}_v, \mathbf{f}_l$) pairs}} \\
   \hline
          &\small \textbf{S} \normalsize. & \small \textbf{SP}\normalsize. & \small \textbf{SPO} \normalsize. & \textbf{total} & \small \textbf{S} \normalsize & \small \textbf{SP}\normalsize & \small \textbf{SPO} \normalsize & \textbf{total images} \\
   \hline
    \textbf{INTERACT} & 0     & 0     & 60    & 60    & 0     & 0     & 3171  & 3171 \\
    \textbf{VisualPhrases} & 11    & 4     & 17    & 32    & 3594  & 372   & 1745  & 5711 \\
    \textbf{Stanford40} & 0     & 11    & 29    & 40    & 0     & 2886  & 6646  & 9532 \\
    \textbf{PPMI} & 0     & 0     & 24    & 24    & 0     & 0     & 4209  & 4209 \\
    \textbf{SPORT} & 14    & 0     & 6     & 20    & 398   & 0     & 300   & 698 \\
    \textbf{Pascal Actions} & 0     & 5     & 5     & 10    & 0     & 2640  & 2663  & 5303 \\
    \hline
    \textbf{Union} & \textbf{25} & \textbf{20} & \textbf{141} & \textbf{186} & \textbf{3992} & \textbf{5898} & \textbf{18734} & \textbf{28624} \\
 \hline
    \end{tabular}%
    }
\end{minipage}
\begin{minipage}{.45\linewidth}
      \caption{Large Scale Dataset}
    \label{tab_lsc_setting}%
    \scalebox{0.7}{
    \begin{tabular}{|c|c|c|c|c|}
  \hline
          & \textbf{S} & \textbf{SP} & \textbf{SPO} & \textbf{Total} \\
  \hline
    \textbf{Training facts} & 6116  & 57681 & 107472 & 171269 \\
    \textbf{Testing facts} & 2733  & 22237 & 33447 & \textbf{58417} \\
    \textbf{Train/Test Intersection} & 1923  & 13043 & 11774 & 26740 \\
    \hline
    \textbf{Test unseen facts} & \textbf{810} & \textbf{9194} & \textbf{21673} & \textbf{31677} \\
    \hline
    \end{tabular}%
    }
\end{minipage}
\vspace{-1mm}
\end{table}
We began our data collection by augmenting existing datasets with fact language view labels $\mathbf{f}_l$: PPMI~\cite{yao2010grouplet}, Stanford40~\cite{yao2011human}, Pascal Actions~\cite{pascal_voc_2012}, Sports~\cite{abhinavg09}, Visual Phrases~\cite{sadeghi2011recognition}, INTERACT ~\cite{antol2014zero} datasets. The union of these 6 datasets resulted in 186 facts with 28,624 images as broken out in Table~\ref{tbl_6DS}.
We also extracted structured facts from the Scene Graph dataset~\cite{johnson2015image} with 5000 manually annotated images in a graph structure from which first-, second-, and third-order relationships can be extracted. We extracted 110,000 second-order facts and 112,000 third-order facts.  The majority of these are positional relationships. We also added to the aforementioned data,  380,000 second and third order fact annotation  collected from MSCOCO and Flickr30K Entities datasets using a language approach as detailed in~\cite{safa2016_acl} in the supplementary. We show later in this section how we use this data to perform several experiments varying in scale to validate our claims. Table~\ref{tab_lsc_setting} shows the unique facts in the large scale dataset.


\begin{table}[t!]
\vspace{-5mm}
  \centering
  \caption{Our fact augmentation of six datasets}
  \label{tbl_6DS}
  \scalebox{0.5}{
    \begin{tabular}{|c|ccc|c|ccc|c|}
 \hline
          & \multicolumn{3}{c}{\textbf{Unique Facts}} & \textbf{} & \multicolumn{4}{c|}{\textbf{Number of Images}} \\
   \hline
          &\small \textbf{$<S>$} \normalsize. & \small \textbf{$<S,P>$}\normalsize. & \small \textbf{$<S,P,O>$} \normalsize. & \textbf{total facts} & \small \textbf{$<S>$} \normalsize & \small \textbf{$<S,P>$}\normalsize & \small \textbf{$<S,P,O>$} \normalsize & \textbf{total images} \\
   \hline
    \textbf{INTERACT} & 0     & 0     & 60    & 60    & 0     & 0     & 3171  & 3171 \\
    \textbf{VisualPhrases} & 11    & 4     & 17    & 32    & 3594  & 372   & 1745  & 5711 \\
    \textbf{Stanford40} & 0     & 11    & 29    & 40    & 0     & 2886  & 6646  & 9532 \\
    \textbf{PPMI} & 0     & 0     & 24    & 24    & 0     & 0     & 4209  & 4209 \\
    \textbf{SPORT} & 14    & 0     & 6     & 20    & 398   & 0     & 300   & 698 \\
    \textbf{Pascal10} & 0     & 5     & 5     & 10    & 0     & 2640  & 2663  & 5303 \\
    \hline
    \textbf{Union} & \textbf{25} & \textbf{20} & \textbf{141} & \textbf{186} & \textbf{3992} & \textbf{5898} & \textbf{18734} & \textbf{28624} \\
 \hline
    \end{tabular}%
    }
    \vspace{-8mm}
\end{table}%
\end{comment}\begin{comment}
\subsection{Structured Fact annotations from existing datasets}
\label{ss:dc_1}
\noindent\textbf{Our structured fact augmentation of six existing action datasets:}
We first manually augmented six existing datasets with structured facts that included activity in images using PPMI~\cite{yao2010grouplet}, Stanford40~\cite{yao2011human}, Pascal10~\cite{yao2010grouplet}, Sports~\cite{abhinavg09}, Visual Phrases~\cite{sadeghi2011recognition}, INTERACT ~\cite{antol2014zero} datasets. We manually augmented each of the labels in these datasets in a structured form. %, for example \SPO{person}{holding}{instrument} from PPMI.
For the INTERACT dataset, we included only real images (excluding cartoon images). Table~\ref{tbl_6DS} shows the coverage of different types of facts included in these datasets, and the number of images in each. In conclusion, the union of these six datasets ended up covering only 186 facts with 28,624 images, which is too limited to cover the space of visual facts that might appear in an image.








\noindent\textbf{Scene Graph dataset:}
Johnson et al.~\cite{johnson2015image} explored a scene retrieval approach in a setting where each scene is provided with a graph annotation for retrieval. Using Amazon's Mechanical Turk, they manually augmented 5000 images selected from the intersection of MSCOCO ~\cite{lin2014microsoft} and YFCC100m~\cite{thomee2015new} datasets with 5000 graph annotation (one for each scene). While it is a different setting, each graph on average has a mean of 18.9 attributes and 21.9 relationships per image, which we think is a useful resource to collect structured fact annotations for second- and third-order facts. This is since attributes are one type of second-order fact $<S,P>$ and relations could include third-order facts $<S,P,O>$. We extracted 110,000 second-order fact annotations from attribute instances, and 112,000 third-order facts annotations. %Accordingly,  we managed to add 110,000 second-order fact annotations from attribute instances, and 112,000 third-order fact annotations.~\cite{johnson2015image}.




While the scene graph dataset provides a valuable source of fact annotations, we found that relationship annotations are dominated by positional information (e.g., ``on'', ``in front of'', etc.). For example, there are 24,484 relationships indicating that "object 1" is "on" "object 2", 7,709 relations for ``next to'', 7,632 ``behind'' , and 5,340 ``in front'' (the four most common positional facts). In exploring the dataset, we found examples where there were cases of tens of people and a sky, and then a relationship for each person indicating that ``sky'' is ``on top of'' each person. We further found that several thousands of annotations had major spelling errors of which most were easily corrected.


This dataset supplemented the six action datasets well as the latter contained largely object actions and interactions while the former contained largely position information.  However, the information was still limited, with the action dataset covering only 186 actions and the scene graph dataset covering facts in only 5000 scenes.


\subsection{Structured Fact annotations from existing datasets}
\label{ss:dc_1}
\noindent\textbf{Our structured fact augmentation of six existing action datasets:}
We first manually augmented six existing datasets with structured facts that included activity in images using PPMI~\cite{yao2010grouplet}, Stanford40~\cite{yao2011human}, Pascal10~\cite{pascal_voc_2012}, Sports~\cite{abhinavg09}, Visual Phrases~\cite{sadeghi2011recognition}, and INTERACT ~\cite{antol2014zero} datasets. We manually augmented each of the labels in these datasets in a structured form. %, for example \SPO{person}{holding}{instrument} from PPMI.
For the INTERACT dataset, we included only real images (excluding cartoon images). Table~\ref{tbl_6DS} shows the coverage of different types of facts included in these datasets, and the number of images in each. In conclusion, the union of these six datasets ended up covering only 186 facts with 28,624 images, which is too limited to cover the space of visual facts that might appear in an image.






\begin{table}[htbp]
  \centering
  \caption{Our fact augmentation of six existing datasets}
  \label{tbl_6DS}
  \scalebox{0.5}{
    \begin{tabular}{|c|ccc|c|ccc|c|}
 \hline
          & \multicolumn{3}{c}{\textbf{Unique Facts}} & \textbf{} & \multicolumn{4}{c|}{\textbf{Number of Images}} \\
   \hline
          &\small \textbf{$<S>$} \normalsize. & \small \textbf{$<S,P>$}\normalsize. & \small \textbf{$<S,P,O>$} \normalsize. & \textbf{total facts} & \small \textbf{$<S>$} \normalsize & \small \textbf{$<S,P>$}\normalsize & \small \textbf{$<S,P,O>$} \normalsize & \textbf{total images} \\
   \hline
    \textbf{INTERACT} & 0     & 0     & 60    & 60    & 0     & 0     & 3171  & 3171 \\
    \textbf{VisualPhrases} & 11    & 4     & 17    & 32    & 3594  & 372   & 1745  & 5711 \\
    \textbf{Stanford40} & 0     & 11    & 29    & 40    & 0     & 2886  & 6646  & 9532 \\
    \textbf{PPMI} & 0     & 0     & 24    & 24    & 0     & 0     & 4209  & 4209 \\
    \textbf{SPORT} & 14    & 0     & 6     & 20    & 398   & 0     & 300   & 698 \\
    \textbf{Pascal10} & 0     & 5     & 5     & 10    & 0     & 2640  & 2663  & 5303 \\
    \hline
    \textbf{Union} & \textbf{25} & \textbf{20} & \textbf{141} & \textbf{186} & \textbf{3992} & \textbf{5898} & \textbf{18734} & \textbf{28624} \\
 \hline
    \end{tabular}%
    }
\end{table}%


\noindent\textbf{Scene Graph dataset:}
Johnson et al.~\cite{johnson2015image} explored a scene retrieval approach in a setting where each scene is provided with a graph annotation for retrieval. Using Amazon's Mechanical Turk, they manually augmented 5000 images selected from the intersection of MS COCO ~\cite{lin2014microsoft} and YFCC100m~\cite{thomee2015new} datasets with 5000 graph annotations (one for each scene). Each graph on average had a mean of 18.9 attributes and 21.9 relationships per image; we think this is a useful resource for collecting structured fact annotations for second- and third-order facts since attributes are one type of second-order fact $<S,P>$ and relations can include third-order facts ($<S,P,O>$). We extracted 110,000 second-order fact annotations from attribute instances, and 112,000 third-order fact annotations. %Accordingly,  we managed to add 110,000 second-order fact annotations from attribute instances, and 112,000 third-order fact annotations.~\cite{johnson2015image}.




While the scene graph dataset provides a valuable source of fact annotations, we found that relationship annotations are dominated by positional information (e.g., ``on'', ``in front of'', etc.). For example, there are 24,484 relationships indicating that "object 1" is "on" "object 2", 7,709 relations for ``next to'', 7,632 ``behind'' , and 5,340 ``in front'' (these were the four most common positional facts). In exploring this dataset, we found examples where there were cases of tens of people and a sky, and then a relationship for each person indicating that ``sky'' is ``on top of'' each person. We further found that several thousands of annotations had major spelling errors of which most were easily corrected.


This dataset supplemented the six action datasets well as the latter contained largely object actions and interactions while the former contained largely position information.  However, the information was still limited, with the action dataset covering only 186 actions and the scene graph dataset provided facts for only 5000 scenes.




\end{comment}%Furthermore, scene graph annotations are collected for only 5000 scene images, which may not cover facts that might be present for an arbitrary image.%\noindent\textbf{Coverage of the collected data so far and their limitations:}%We have discussed two sources of fact annotations: (a) our manual fact augmentation of six existing action image datasets, and (b) the facts we extracted from the scene graph dataset.  The first source has the advantage that the six datasets covered second- and third-order facts that are actions and interactions, which are important and interesting facts to model for images. In contrast, these facts lack attribute and positional information which is provided by the second source (scene graph). Although, the coverage of facts is still limited for activities beyond the union of the six action datasets we included, i.e., which covered only 186 action and interaction facts, and the facts collected from scene graph were mainly positional and provided facts for only 5000 scenes.  %Although the scene graph dataset is a valuable source of data, we aim to cover more annotation actions and interaction relationships that are not positional. %Although we investigate structured fact data collection from their dataset of 5000 scenes, our observation is that most of the annotations by the Turkers are positional facts (e.g., "glass on table"), see figure~\ref{}. While the dataset dataset is still very rich with a mean of 13.8 objects, 18.9 attributes and 21.9 relationships per image.%About 100 on fact specifying that the sky on top of every object in the scene.  Scene graph dataset, like several existing dataset, %\textbf{CONCLUSION:} coverage is limited, we need to explore other way to gain structured data  that are not expenseive which motivates automatic data structured fact collection %Furthermore, scene graph annotations are collected for only 5000 scene images, which may not cover facts that might be present for an arbitrary image.%\noindent\textbf{Coverage of the collected data so far and their limitations:}%We have discussed two sources of fact annotations: (a) our manual fact augmentation of six existing action image datasets, and (b) the facts we extracted from the scene graph dataset.  The first source has the advantage that the six datasets covered second- and third-order facts that are actions and interactions, which are important and interesting facts to model for images. In contrast, these facts lack attribute and positional information which is provided by the second source (scene graph). Although, the coverage of facts is still limited for activities beyond the union of the six action datasets we included, i.e., which covered only 186 action and interaction facts, and the facts collected from scene graph were mainly positional and provided facts for only 5000 scenes.  %Although the scene graph dataset is a valuable source of data, we aim to cover more annotation actions and interaction relationships that are not positional. %Although we investigate structured fact data collection from their dataset of 5000 scenes, our observation is that most of the annotations by the Turkers are positional facts (e.g., "glass on table"), see figure~\ref{}. While the dataset dataset is still very rich with a mean of 13.8 objects, 18.9 attributes and 21.9 relationships per image.%About 100 on fact specifying that the sky on top of every object in the scene.  Scene graph dataset, like several existing dataset, %\textbf{CONCLUSION:} coverage is limited, we need to explore other way to gain structured data  that are not expenseive which motivates automatic data structured fact collection \vspace{-2mm}\subsection{Setup of our Models and the designed Baselines}\vspace{-2mm}\label{sec_setup}\begin{comment}
In all of our Model 1 and Model 2  experiments, $\theta_l$ is defined by the GloVE840B RNN model~\cite{pennington2014glove}, which is used for encoding structured facts in the language view  for both Model 1 and Model 2 (i.e., $\phi_S^{\mathcal{L}}(\mathbf{f}_l)$, $\phi_P^{\mathcal{L}}(\mathbf{f}_l)$, and $\phi_O^{\mathcal{L}}(\mathbf{f}_l)$). We now enumerate the setup that are specific to Model2,  Model1 and other  methods from multiview literature that we found  applicable in our setting.
\end{comment}


In our Model 1 and Model 2, $\theta_l$ is the GloVE840B RNN model~\cite{pennington2014glove} to encode structured facts in the language view. %Unless otherwise is mentioned, Model 1 and Model 2 were trained by the loss in Eq~\ref{eq_euc}. We now give the setup for models we compare.\begin{enumerate}
\item{\textbf{Model 1}:} Model 1 is constructed from VGG-16, where  $\theta_c$ is built from the layer \texttt{conv\char`_1\char`_1} to \texttt{pool5}, and  $\theta_u$ is the two following fully connected layers \texttt{fc6} and  \texttt{fc7} in VGG-16~\cite{simonyan2014very}. Similar to Model 2,  $W^S$, $W^P$, and  $W^O$  are initialized randomly  and the rest of the parameters are initialized from VGG-16 trained on ImageNet~\cite{deng2009imagenet}.
\item \noindent\textbf{Model 2:} The shared layers $\theta_c^{0}$ match the architecture of the convolutional layers and pooling layer in VGG-16 named  \texttt{conv\char`_1\char`_1} until \texttt{pool3}, and have seven convolution layers. The subject layers $\theta_c^{S}$ and predicate-object layers $\theta_c^{{PO}}$ are two branches of convolution and pooling layers with the same architecture as VGG-16 layers  named  \texttt{conv\char`_4\char`_1} until \texttt{pool5} layer, which makes six convolution-pooling layers in each branch. Finally, $\theta_u^{S}$  and  $\theta_u^{{PO}}$ are two instances of \texttt{fc6} and \texttt{fc7} layers in VGG-16 network.
$W^S$, $W^P$, and  $W^O$  are initialized randomly and the rest are initialized from VGG-16 trained on ImageNet.
	\item \textbf{Multiview CCA IJCV14 ~\cite{gong2014multi} (MV CCA) }: MV CCA expects features from both views. For visual view features, we used VGG16 (FC6).
For the language view features, we used GloVE.
Since MV CCA does not support wild-cards, we fill the wild-card parts of $\Phi^\mathcal{L}(\mathbf{f}_l)$ with zeros for First Order and Second order facts.
	\item \textbf{ESZSL ICML15 Baseline~\cite{romera2015embarrassingly} (ESZSL)}: ESZSL also expects both visual and semantic features for a fact.
As in MV CCA,  we used VGG16 (FC6) and GloVE.
     \item \textbf{Image-Sentence Similarity (TACL15~\cite{kiros2014unifying}) (MS COCO pretrained)}: We used the theano implementations of this method that were made publically available by the authors \cite{tacl15_implementation}.
The purpose of applying MS COCO pretrained image-caption models is to show how image-caption trained models perform when applied to fact level recognition in our setting. In order to use these models to measure simility between image and facts in our setting, we provide them with the image and a phrase constructed from the fact language representation. For example $<$person, riding, horse $>$ is converted to ``person riding horse''.
\item \textbf{Image-Sentence Similarity (TACL15~\cite{kiros2014unifying} (retrained)}:  In contrast to the previous setting, we  retrain these models by providing them our image-fact training pairs where facts are converted to phrases.
The results show the value of learning models on the fact level instead of the caption level. %It is worth mentioning that a very recent paper~\cite{vendrov2015order}, accepted to  ICLR16, has proposed a new order-embedding loss that can improve existing methods. This is shown in~\cite{vendrov2015order} for the method in~\cite{kiros2014unifying}  by  replacing the distance loss with the order-embedding loss.  We also expect similar improvement by applying their order loss on our structured fact space instead of the but this is future work as~\cite{vendrov2015order} is a very recent work (not yet presented).
\begin{comment}
Note that ICLR16~\cite{vendrov2015order} is a very recent work (not yet presented) and although both our models and ~\cite{vendrov2015order} build uses VGG Network, the comparison is complicated since their main contribution is to use order-embedding loss instead of distance loss.  ICLR16~\cite{vendrov2015order} applies the method in TACL15~\cite{kiros2014unifying} but using their proposed order-embedding loss instead of the distance loss used in~\cite{kiros2014unifying}. We keep both the TACL15 and ICLR16 results in this paper as a guide since they are firstly applied in this setting while considering applying order-embedding loss to Model1 and 2 as future work.
\end{comment}
\end{enumerate}%Since our Sherlock models  are continuous in both language and visual views of facts, we can perform two way retrieval from the visual view given the language view and vice versa. %We start by presenting evaluation metrics used in our small, medium, and large scale experiments for both language view retrieval and visual view retrieval.% We evaluate retrieval results in both directions by embedding either language facts or images into the structured fact space. Specifically, having trained either Model 1 or Model 2, the embedding $\phi^{\mathcal{V}}$ is applied to all the test images  to embed them to the structured fact space. Similarly,  $\phi^{\mathcal{L}}(\mathbf{f}_l)$ is applied to embed all the language views of facts into the same space.  Then, we compute the cosine similarity as the final similarity measure between images and facts.  \vspace{-6mm}\subsection{Evaluation Metrics}\vspace{-2mm}
We present evaluation metrics for both language view retrieval and visual view retrieval.
\noindent\textbf{Metrics for  visual view retrieval  (retrieving $\mathbf{f}_v$ given $\mathbf{f}_l$): }  To retrieve an image (visual view) given a language view (e.g.  $<$S: person, P: riding, O: horse$>$), we measure the performance by mAP (Mean Average Precision).% and ROC AUC performance. %on the test set of each designated dataset in this section.
An image $\mathbf{f}_v$ is considered positive only if there is a pair $(\mathbf{f}_l, \mathbf{f}_v)$ in the annotations. Even if the retrieved image is relevant but such pair does not exist, it is considered incorrect.  We also use  mAP10, mAP100 variants
that compute the mAP based on only the top 10 or 100 retrieved images, which is useful for evaluating large scale experiments.% which has billions of combinations of  $(\mathbf{f}_v,\mathbf{f}_v )$ pairs. \noindent\textbf{Metrics for language view retrieval (retrieving $\mathbf{f}_l$ given $\mathbf{f}_v$): } To retrieve fact language views given an image. we use top 1, top 5, top 10 accuracy for evaluation.
We also used MRR (mean reciprocal ranking) metric which is basically $1/r$ where $r$ is the rank of the correct class. An important issue with our setting is that there might be multiple facts in the same image. Given that there are $L$ correct facts in the given image to achieve top 1 performance these $L$ facts must all be in the top $L$ retrieved facts. Accordingly, top K means the $L$ facts are in the top $L+K-1$ retrieved facts.
A fact language view $\mathbf{f}_l$ is considered correct only if there is a pair $(\mathbf{f}_l, \mathbf{f}_v)$ in the annotations.


It is not hard to see that the aforementioned metrics are very harsh, especially in the large scale setting. For instance, if the correct fact for an image is $<$S:man,P:  jumping$>$, then an answer $<$S:person, P:jumping$>$ receives zero credit. Also, the evaluation is limited to the ground truth fact annotations. There might be several facts in an image but the provided annotations may miss some facts. Qualitatively we found the metrics harsh for our large scale experiment.
Defining better metrics is future work.
\vspace{-10mm}
\centering
 \caption{Small and Medium Scale Experiments}
  \label{ttbl_smDS}
  \scalebox{0.65}{
    \begin{tabular}{|c|c|ccc|ccc|}
\hline
          &       & \multicolumn{3}{c|}{\textbf{Language View retrieval}} & \multicolumn{3}{c|}{\textbf{Visual View retrieval}} \\
\hline
    \textbf{} & \textbf{} & \textbf{Top1} & \textbf{Top 5} & \textbf{MRR} & \textbf{mAP} & \textbf{mAP10} & \textbf{mAP100}  \\
\textbf{Standord40 (40 facts)}    & \textbf{Model2} & \textbf{74.46} & \textbf{92.01} & \textbf{82.26} & 73    & 98.35 & 92     \\
  (11 SP,  29 SPO)      & \textbf{Model1} & 71.22 & 90.98 & 82.09 & \textbf{74.57} & \textbf{99.72} & \textbf{92.62}  \\
  & \textbf{MV CCA IJCV14} & 67.74 & 88.32 & 76.80 & {66.00} & {96.86} & {86.66} \\
          & \textbf{ESZSL ICML15~\cite{romera2015embarrassingly}} & 40.89 & 74.93 & 56.08 & 50.9  & 93.87 & 78.35 \\
          & \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (COCO pretrained)} & 33.73 & 62.62 & 47.70 & 26.29 & 59.68 & 44.2  \\
                  & \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (retrained)} & 60.86 & 87.82 & 72.51 & 51.9 & 88.13 & 74.55   \\
        \textbf{} & \textbf{Chance} & 2.5 & - & - & - & - & - \\
    \hline
  \textbf{Pascal Actions (10 facts)}        & \textbf{Model2} & \textbf{74.760} & \textbf{95.750 }& \textbf{83.680} & \textbf{80.950} & \textbf{100.000} & \textbf{97.240}  \\
     (5 SP,  5 SPO)       & \textbf{Model1} & 74.080 & 95.790 & 83.280 & 80.530 & 100.000 & 96.960 \\
                    & \textbf{MV CCA IJCV14} & 59.82 & 92.78 & 73.16 & 33.45 & 66.52 & 53.29 \\
          & \textbf{ESZSL ICML15~\cite{romera2015embarrassingly}} & 44.846 & 88.864 & 63.366 & 54.274 & 89.968 & 82.273  \\
       & \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (COCO pretrained)} & 46.050 & 86.907 & 62.796 & 40.712 & 88.694 & 71.078  \\
              & \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (retrained) } & 60.27 & 94.66 & 74.77 & 50.58 & 84.65 & 71.61  \\
              \textbf{} & \textbf{Chance} & 10 & - & - & - & - & - \\
                  \hline
   \textbf{VisualPhrases (31 facts)}    & \textbf{Model2} & \textbf{34.367} & \textbf{76.056} & \textbf{47.263} & \textbf{39.865} & \textbf{61.990} & \textbf{48.246} \\
 (14 S, 4 SP, 17 SPO)   & \textbf{Model1} & 28.100 & 75.285 & 42.534 & 38.326 & 65.458 & 46.882  \\
     & \textbf{MV CCA IJCV14~\cite{gong2014multi}} & 28.94 & 70.61 & 88.92 & {28.27} & {49.30} & {34.48} \\
 &   \textbf{ESZSL ICML15~\cite{romera2015embarrassingly}} & 33.830 & 68.264 & 44.650 & 33.010 & 57.861 & 41.131  \\
  &  \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (COCO pretrained)} & 30.111 & 64.494 & 42.777 & 26.941 & 49.892 & 33.014 \\
      &  \textbf{Image-Sentence TACL15   ~\cite{kiros2014unifying} (retrained)} & 32.32 & 94.72 & 50.7 & 28.0 & 49.89 & 33.21 \\
                 \textbf{} & \textbf{Chance} & 3.2 & - & - & - & - & - \\
        \hline
 \textbf{6DS (186 facts)}  &   \textbf{Model2} & \textbf{69.63} & \textbf{80.32} & \textbf{70.66} &\textbf{ 34.86} & \textbf{61.03 }& \textbf{50.68 }\\
  (25 S, 20 SP, 141 SPO)  &  \textbf{Model1} &{ 68.94} & {78.74} & 70.74 & 34.64 & 56.54 & 47.87 \\
    &  \textbf{MV CCA IJCV14~\cite{gong2014multi}} & 29.84 & 39.78 & 32.00 & 23.93 & 46.43 & 36.44 \\
  &  \textbf{ESZSL ICML15~\cite{romera2015embarrassingly}} & 27.53 & 47.4 & 58.2 & 30.7 & 60.97  & 47.58 \\
   & \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (COCO pretrained)} & 15.71 & 26.84 & 19.65 & 9.37  & 21.58 & 15.88 \\
      & \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (retrained)} & 26.13 & 41.10 & 30.94 & 26.17  & 56.10 & 40.4  \\
      \textbf{} & \textbf{Chance} &0.54 & - & - & - & - & - \\
\hline
    \end{tabular}
    }
\end{table}\begin{comment}
\begin{table}[b!]
\vspace{-10mm}
\centering
 \caption{Small and Medium Scale Experiments (metric2)}
  \label{ttbl_smDS}
  \scalebox{0.65}{
    \begin{tabular}{|c|c|ccc|cccc|}
\hline
          &       & \multicolumn{3}{c|}{\textbf{Language View retrieval}} & \multicolumn{4}{c|}{\textbf{Visual View retrieval}} \\
\hline
    \textbf{} & \textbf{} & \textbf{Top1} & \textbf{Top 5} & \textbf{MRR} & \textbf{AP} & \textbf{AP10} & \textbf{AP100} & \textbf{AUC} \\
       \textbf{Standord40 (40 facts)} &  \\
   (11 SP,  29 SPO) & \textbf{Model2} & \textbf{74.46} & \textbf{92.01} & \textbf{82.26} & 73    & 98.35 & 92    & \textbf{0.97} \\
          & \textbf{Model1} & 71.22 & 90.98 & 82.09 & \textbf{74.57} & \textbf{99.72} & \textbf{92.62} & 0.97 \\
  & \textbf{MV CCA IJCV14~\cite{gong2014multi}} & 67.74 & 88.32 & 76.80 & {66.00} & {96.86} & {86.66} & 0.95 \\
          & \textbf{ESZSL ICML15~\cite{romera2015embarrassingly}} & 40.89 & 74.93 & 56.08 & 50.9  & 93.87 & 78.35 & 0.89 \\
          & \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (COCO pretrained)} & 33.73 & 62.62 & 47.70 & 26.29 & 59.68 & 44.2  & 0.83 \\
                  & \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (retrained)} & 60.86 & 87.82 & 72.51 & 51.9 & 88.13 & 74.55  & 0.939 \\
        \textbf{} & \textbf{Chance} & 2.5 & - & - & - & - & - & -\\
    \hline
    \textbf{Pascal Actions (10 facts)} &\\
        (5 SP,  5 SPO)     & \textbf{Model2 c} & \textbf{75.240} & \textbf{96.430} & \textbf{84.210} & \textbf{81.760} & \textbf{100.000} & \textbf{98.140} & \textbf{0.950} \\
          & \textbf{Model2} & \textbf{74.760} & \textbf{95.750} & \textbf{83.680 }& \textbf{80.950} & \textbf{100.000} & \textbf{97.240} &\textbf{ 0.950} \\
          & \textbf{Model1} & 74.080 & 95.790 & 83.280 & 80.530 & 100.000 & 96.960 & 0.950 \\
                    & \textbf{MV CCA IJCV14~\cite{gong2014multi}} & 59.82 & 92.78 & 73.16 & 33.45 & 66.52 & 53.29 & 0.76 \\
          & \textbf{ESZSL ICML15~\cite{romera2015embarrassingly}} & 44.846 & 88.864 & 63.366 & 54.274 & 89.968 & 82.273 & 0.853 \\
       & \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (COCO pretrained)} & 46.050 & 86.907 & 62.796 & 40.712 & 88.694 & 71.078 & 0.782 \\
              & \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (retrained) } & 60.27 & 94.66 & 74.77 & 50.58 & 84.65 & 71.61 & 0.883 \\
                  \hline
    \textbf{VisualPhrases (31 facts)} & \\
     (14 S, 4 SP, 17 SPO)   & \textbf{Model2} & {34.367} & {76.056} & {47.263} & \textbf{39.865} & \textbf{61.990} & \textbf{48.246} & 0.942 \\
   & \textbf{Model1} & 28.100 & 75.285 & 42.534 & 38.326 & 65.458 & 46.882 & 0.936 \\
     & \textbf{MV CCA IJCV14~\cite{gong2014multi}} & 28.94 & 70.61 & 88.92 & {28.27} & {49.30} & {34.48} & 0.92 \\
 &   \textbf{ESZSL ICML15~\cite{romera2015embarrassingly}} & 33.830 & 68.264 & 44.650 & 33.010 & 57.861 & 41.131 & 0.916 \\
  &  \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (COCO pretrained)} & 30.111 & 64.494 & 42.777 & 26.941 & 49.892 & 33.014 & 0.918 \\
      &  \textbf{Image-Sentence TACL15   ~\cite{kiros2014unifying} (retrained)} & 32.32 & 94.72 & 50.7 & 28.0 & 49.89 & 33.21 & 0.927 \\
        \hline
    \textbf{6DS (186 facts)} \\
 (25 S, 20 SP, 141 SPO) &   \textbf{Model2} & \textbf{69.63} & \textbf{80.32} & \textbf{70.66} & 34.86 & 61.03 & 50.68 & 0.94 \\
  &  \textbf{Model1} & 68.94 & 78.74 & 70.74 & 34.64 & 56.54 & 47.87 & 0.95 \\
    &  \textbf{MV CCA IJCV14~\cite{gong2014multi}} & 29.84 & 39.78 & 32.00 & 23.93 & 46.43 & 36.44 & {0.94} \\
  &  \textbf{ESZSL ICML15~\cite{romera2015embarrassingly}} & 33.83 & 68.26 & 44.65 & 33.01 & 57.7  & 41.1  & 0.91 \\
   & \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (COCO pretrained)} & 15.71 & 26.84 & 19.65 & 9.37  & 21.58 & 15.88 & 0.76 \\
      & \textbf{Image-Sentence TACL15  ~\cite{kiros2014unifying} (retrained)} & 26.13 & 41.10 & 30.94 & 26.17  & 56.10 & 40.4 & 0.94 \\
\hline
    \end{tabular}%
    }
\end{table}
\end{comment}\vspace{-2mm}\subsection{Small and Mid scale Experiments}\vspace{-2mm}%\textbf{Experiments Overview: }%We performed experiments on small, medium and large scale setting.  %We performed small and medium scale experiments in Sec~\ref{sec_pascal_voc_2012} an~\ref{smmed_exp}, and large scale experiments in Sec~\ref{sec_lsc_exp} to evaluate our work. % that could be also applied in our setting. %We contrast Model 1 and 2 on small, medium, and large scale experiments and show that Model 2 is better.%We show that Model 2 is better on the small, medium, and large scale experiments.%In the large scale experiment, the collected data form is more than 816,000 $(\mathbf{f}_v,\mathbf{f}_l)$ pairs, coveringmore than 202,000 unique facts in the language view $\mathbf{f}_l$. The training-testing split is performed by randomly splitting all the pairs into  80\% training pairs and 20\% testing pairs. This results in 168,691 testing $(\mathbf{f}_v,\mathbf{f}_l)$ pairs with   58,417  unique $\mathbf{f}_l$ and at test time, where 31,677 out of them are \textit{unseen} during training. We contrast Model 1 and 2 on small, medium, and large scale experiments and show that Model 2 is better.%\subsubsection{Small and Mid-scale Experiments}%\label{smmed_exp}%\vspace{-1mm}
We  performed experiments on several  datasets ranging in scale: Stanford40~\cite{yao2011human}, Pascal Actions~\cite{yao2010grouplet}, Visual Phrases~\cite{sadeghi2011recognition}, and the union of six datasets described earlier in Table~\ref{tbl_6DS} in Sec.~\ref{sec_data}. We used the training and test splits defined with those datasets. For the union of six datasets, we unioned the training and testing annotations to get the final split.  In all these training/testing splits, each fact language view $\mathbf{f}_l$ has corresponding tens of visual views $\mathbf{f}_v$ (i.e., images)
split into training and test sets.
So, each test image belongs to a fact that was seen by other images in the training set. %We show later in  our large scale experiment how the behavior of these methods change when the scale of the facts goes three order of magnitudes bigger and with tens of thousands of unseen test facts which is more common in real world setting. % \begin{wraptable}{r}{0.6\textwidth} \vspace{-4mm}%  \centering%    \vspace{-2mm}%  \caption{Small and Medium Scale Experiments}%  \vspace{-3mm}%  \label{ttbl_smDS}%  \scalebox{0.5}%  {%    \begin{tabular}{|c|c|ccc|cccc|}%\hline%          &       & \multicolumn{3}{c|}{\textbf{Language View retrieval}} & \multicolumn{4}{c|}{\textbf{Visual View retrieval}} \\%\hline%    \textbf{} & \textbf{} & \textbf{Top1 Acc} & \textbf{Top 5 Acc} & \textbf{MRR} & \textbf{AP} & \textbf{AP10} & \textbf{AP100} & \textbf{AUC} \\%    \textbf{Standord40 (40 facts)} & \textbf{Model 1} & 66.29 & 88.77 & 76.17 & 74.45 & 99.47 & 92.64 & 0.97 \\%    \textbf{} & \textbf{Model 2} & 68.80 & 89.41 & 77.87 & 72.81 & 98.18 & 91.93 & 0.97 \\%        \textbf{} & \textbf{Chance} & 2.5 & - & - & - & - & - & -\\%        \hline%    \textbf{VisualPhrases (31 facts)} & \textbf{Model 1} & 24.90 & 43.70 & 35.70 & 29.55 & 55.50 & 36.14 & 0.92 \\%    \textbf{} & \textbf{Model 2} & 25.30 & 43.80 & 36.48 & 30.27 & 50.43 & 36.77 & 0.93 \\%            \textbf{} & \textbf{Chance} & 3.2 & - & - & - & - & - & -\\%    \hline%    \textbf{Pascal Actions (10 facts)} & \textbf{Model 1} & 64.75 & 93.49 & 76.81 & 80.12 & 100.00 & 96.94 & 0.94 \\%    \textbf{} & \textbf{Model 2} & 64.97 & 93.79 & 77.04 & 80.68 & 100.00 & 97.21 & 0.95 \\%          \textbf{} & \textbf{Chance} & 10 & - & - & - & - & - & -\\%        \hline%    \textbf{6DS (186 facts)} & \textbf{Model 1} &       47.07 &     64.4  & 54.18       &    28.2   &      51.32 &  43.13     & 0.890 \\%    \textbf{} & \textbf{Model 2} & 48.5  &  64.59     & 54.32      &  32.40     &  54.44     & 45.07      & 0.94 \\%%     \textbf{} & \textbf{VGG-16 CNN} & 51.5  &  -     & -      &  -     &  -    & -     & - \\%        \textbf{} & \textbf{Chance} & 0.54 & - & - & - & - & - & -\\%\hline%    \end{tabular}%%    }%        \vspace{-5mm}%\end{wraptable}
Table~\ref{ttbl_smDS} shows the performance of our Model 1,  Model 2, and the designed baselines on these four datasets for  both view  retrieval tasks. We note that Model 2 works relatively  better than Model 1 as the scale  size increases as shown here when comparing results on Pascal dataset to larger datasets like Stanford40, Visual Phrases, and  6DS. In the next section, we show that Model2 is clearly better than Model 1 in the large scale setting.
Our intuition behind this result is that Model 2 learns a different set of convolutional filters
in the PO branch
to understand action/attributes and interactions
which is different from the filter bank learned to discriminate between different subjects for the S branch.
In contrast, Model 1 is trained by optimizing one bank of filters
for SPO altogether, which might conflict to optimize for both S and PO together; see Fig~\ref{fig:mdls2}.


Learning from image-caption pairs even on big dataset like MSCOCO does not help discriminate between tens of facts as shown in these experiments. However, retraining these models by providing them image-fact pairs makes them perform much better as shown in Table~\ref{ttbl_smDS}. Compared to other methods on language view retrieval, we found Model 1 and 2 perform significantly better than  TACL15~\cite{kiros2014unifying}  even when retrained for our setting, especially on PASCAL10, Stanford40, and 6DS datasets which are dominated by SP and SPO facts; see Table~\ref{tbl_6DS}. %The performance is competitive between our models and ICLR16~\cite{vendrov2015order} on Visual Phrases dataset. %Our explanation is that the %The test examples in this dataset are dominated by objects (more than 75\% of the training/ testing examples) and~\cite{vendrov2015order} performs well on these cases in the small scale.
For visual view retrieval, performance is competitive in some of the datasets.
We think the reason is due to the structure that makes our models relate all fact types by the visual modifiers notion.


Although ESZSL is applicable in our setting, it is among the worst performing methods in Table~\ref{ttbl_smDS}.
This could be because ESZSL is mainly designed for Zero-Shot Learning, but each fact has some training examples in these experiments. Interestingly, MV CCA with the chosen visual and language features is among the best methods.  Next we compare these methods when number of facts becomes three orders of magnitudes larger and with tens of thousands of testing facts that are unseen in training.% Finally, we observed an little increase in the performance while using  the loss in~\ref{eq_rankimg} from 74.76\% to 75.24\%.%It is worth mentioning that the order embedding loss proposed in ICLR16~\cite{vendrov2015order} has shown promising results in multiple tasks and we consider applying the proposed loss as a part of our future work to  improve the performance of proposed models.  %\noindent \textbf{MV CCA and ESZSL:} %The performances of Model 1 and 2 are very similar in small datasets like Pascal Actions.  %In the 6DS experiment, we also performed the CNN classification but using VGG-Net for this experiment. This leads to the same conclusion we discussed in the previous experiment in Sec.~\ref{sec_pascal_voc_2012}. \vspace{-2mm}\subsection{Large Scale Experiment}\label{sec_lsc_exp}\vspace{-2mm}%In the large scale experiment, the collected data is more than 816,000 $(\mathbf{f}_v,\mathbf{f}_l)$ pairs, covering more than 202,000 unique facts in the language view $\mathbf{f}_l$, 31,677 unique facts are unseen at test time. %\begin{figure*}[ht!]%  \centering%    \includegraphics[width=1.0\textwidth]{fact_int_freq.eps}%     \caption{Intersecting facts between training and test (majority of training facts have one of few examples)}%\end{figure*}In this experiment, we used the union of all the data described in Sec.~\ref{sec_data}. We further augmented this data with 2000 images for each MS COCO object (80 classes) as first-order facts. We also used object annotations in the Scene Graph dataset as first-order fact annotations with a maximum of 2000 images per object.
Finally, we randomly split all the annotations into an 80\%-20\% split, constructing sets of 647,746 $(\mathbf{f}_v, \mathbf{f}_l)$ training pairs (with 171,269 unique fact language views $\mathbf{f}_l$) and 168,691 $(\mathbf{f}_v, \mathbf{f}_l)$ testing pairs (with 58,417 unique $\mathbf{f}_l$), for a total of $(\mathbf{f}_v, \mathbf{f}_l)$ 816,436 pairs, 202,946 unique  $\mathbf{f}_l$. Table~\ref{tab_lsc_setting} shows the coverage of  different types of facts. %in the training and the test split and their intersection. %There are a total of 31,677 unique unseen facts out of the 58,417 testing facts in the language view.
There are 31,677 language view test facts  that were unseen in the training set (851 $<$S$>$, 9,194 $<$S,P$>$, 21,673 $<$S,P,O$>$).
The majority of the facts have only one example; see the supplementary material.


Qualitative results are shown in Fig.~\ref{fig:ex1},~\ref{fig:ex2} (with many more in the supplementary).  In Fig.~\ref{fig:ex1}, our model's ability to generalize can be seen in the red facts.  For example, for the leftmost image our model was able to correctly identify the image as $<$dog, riding, wave$>$ despite that fact never being seen in our training data.  The left images in Fig.~\ref{fig:ex2} show the variety of images we can retrieve for the query $<$airplane, flying$>$.  In the right images in Fig.~\ref{fig:ex2}, note how our model learns to visually distinguish gender (``man'' versus ``girl''), and group versus single. It can also correctly retrieve images for facts that were never seen in the training set ($<$girl, using, racket$>$). Highlighting the harshness of the metric, Fig.~\ref{fig:ex2} also shows that   $<$airplane, flying$>$ has zero AP10 value giving us zero credit since the top images were just annotated as an $<$ airplane$>$.


\vspace{-3mm}
\begin{center}
\setlength{\tabcolsep}{1pt}
\begin{tabular}{cccc}
\includegraphics[height=0.70in]{KEx_dog_riding_Wave.jpg} &
\includegraphics[height=0.75in]{KEx_INTERACT_man_on_sand.jpg} & \includegraphics[height=0.68in]{boat_pulling_boat.jpg} &
\includegraphics[height=0.70in]{KEx_banana.jpg} \\
{\scriptsize
\begin{tabular}{ll}
\textcolor{red}{$<$dog, riding, wave$>$} & 0.88 \\ %0.876
\textcolor{red}{$<$one, riding, wave$>$} & 0.62 \\ %0.623
$<$guy, riding, wave$>$ & 0.60\\ % 0.595
\end{tabular}} &
{\scriptsize
\begin{tabular}{ll}
$<$man, on, sand$>$ & 0.52 \\ %0.517
$<$man,on, beach$>$ & 0.51 \\ %0.514
$<$man, pushing, boat$>$ & 0.48
\end{tabular}} &
{\scriptsize
\begin{tabular}{ll}
{$<$boat, behind, boat$>$} & 0.775 \\
{$<$boat, beside, boat$>$} & 0.765 \\
\textcolor{red}{$<$ boat, pulling, boat$>$} & 0.753
\end{tabular}} &
{\scriptsize
\begin{tabular}{ll}
\textcolor{red}{$<$bananas, over,fruit$>$} & 0.68 \\ %0.681
\textcolor{red}{$<$bananas,behind, fruit$>$} & 0.68 \\ %0.681
$<$pineapple, on, table$>$ & 0.59 %  0.590
\end{tabular}}
\end{tabular}
\end{center}
\vspace{-6mm}
\caption{Language View Retrieval examples (\textcolor{red}{red}  means unseen facts)}
\vspace{-5mm}
 \label{fig:ex1}
\end{figure}\begin{figure}[t!]
   \centering
\includegraphics[width=0.49\textwidth]{kex_ret_Ex1_2.jpg}
\includegraphics[width=0.49\textwidth]{ret_ex3_1.jpg}
  \vspace{-2mm}
 \caption{ Visual View Retrieval Examples (\textcolor{red}{red} means unseen facts)}
  \vspace{-4mm}
 \label{fig:ex2}
\end{figure}


To  perform retrieval in both directions, we used the FLANN library~\cite{muja2009flann}%, and we restricted
to compute the (approximate) 100 nearest neighbors for $\mathbf{f}_l$ given $\mathbf{f}_v$, and vice-versa.  Details about the nearest-neighbor database creation and the large scale evaluation could be found in the supplementary.
The results in Table~\ref{tab:atbl} indicate that Model 2 is better than Model 1 for retrieval from  both views, which is consistent with our medium scale results and our intuition. Model 2 is also multiple orders of magnitude better than chance and is also significantly better than the competing methods. To test the value of structure, we ran an experiment where we averaged the S, P, and O parts of the visual and language embedding vectors instead of keeping the structure.
Removing the structure leads to a noticeable decrease in performance from 16.39\% to 8.1\% for the K1 metric; see Table~\ref{tab:atbl}.
improvement of Model 2 compared to the baselines in the large scale setting. Additionally, the performance of the image-caption similarity methods degrade substantially. We think this is due to both the large scale of the facts and that the majority of the facts have zero or very few training examples. Interestingly, MV CCA is among the best performing methods in the large scale setting. However, Model 2 and Model 1 outperform MV CCA on both Top1 and Top 5 metrics; see Table~\ref{tab:atbl}. On the language view retrieval, we have very competitive results to MV CCA but as we have notices several good visual retrieval results for which the metric gives zero-credit.


\begin{table}[b!]
\vspace{-7mm}
\centering
 \caption{Large Scale Experiment}
    \label{tab:atbl}
    \scalebox{0.8}{
    \begin{tabular}{|c|c|c|c|c|c|}
   \hline
          & \multicolumn{3}{c|}{\textbf{Language View retrieval \%}} & \multicolumn{2}{c|}{\textbf{Visual view Retrieval \%}} \\
\hline
    \textbf{} & \textbf{Top1 } & \textbf{Top 5 } & \textbf{Top 10 }  & \textbf{mAP100} & \textbf{mAP10} \\
            \textbf{Model 2} & \textbf{16.39 }& \textbf{17.62} & \textbf{18.41} &  0.90 & 0.90 \\
                \textbf{Model 1} & 13.27 & 14.19 & 14.80   & 0.73 & 0.73 \\
      \textbf{ Model 2 (Unstructured by SPO average)} & 8.1 & 12.4 & 14.00  & 0.61 & 0.62 \\
        \textbf{MV CCA IJCV14~\cite{gong2014multi}} & 12.28& 12.84 & 13.15  &  \textbf{1.0} & \textbf{1.0} \\
        \textbf{ESZSL ICML15~\cite{romera2015embarrassingly}} & 5.80 & 5.84 & 5.86   &  0.4 & 0.4 \\
  \textbf{Image-Sentence TACL15~\cite{kiros2014unifying} (COCO pretrained)} & 3.48 & 3.48 & 3.5  &  0.021 & 0.0087 \\
    \textbf{Image-Sentence TACL15~\cite{kiros2014unifying} (retrained)} & 5.87 & 6.06 & 6.15 & 0.29 & 0.29 \\
        \textbf{Chance} & 0.0017& -& -   &  - & -\\
\hline
    \end{tabular}
    }
\end{table}\begin{comment}
\begin{table}[b!]
\vspace{-7mm}
\centering
 \caption{Large Scale Experiment}
    \label{tab:atbl}
    \scalebox{0.8}{
    \begin{tabular}{|c|c|c|c|c|c|c|}
   \hline
          & \multicolumn{4}{c|}{\textbf{Language View retrieval \%}} & \multicolumn{2}{c|}{\textbf{Visual view Retrieval \%}} \\
\hline
    \textbf{} & \textbf{Top1 } & \textbf{Top 5 } & \textbf{Top 10 } & \textbf{MRR} & \textbf{mAP100} & \textbf{mAP10} \\
            \textbf{Model 2} & \textbf{16.39 }& \textbf{17.62} & \textbf{18.41} & \textbf{9.60} &  0.90 & 0.90 \\
                \textbf{Model 1} & 13.27 & 14.19 & 14.80 & 7.88  & 0.73 & 0.73 \\
      \textbf{ Model 2 (Unstructured by SPO average)} & 8.1 & 12.4 & 14.00 & 7.88  & 0.61 & 0.62 \\
        \textbf{MV CCA IJCV14~\cite{gong2014multi}} & 12.28& 12.84 & 13.15 & 7.87 &  \textbf{1.0} & \textbf{1.0} \\
        \textbf{ESZSL ICML15~\cite{romera2015embarrassingly}} & 5.80 & 5.84 & 5.86  & 3.32 &  0.4 & 0.4 \\
  \textbf{Image-Sentence TACL15~\cite{kiros2014unifying} (COCO pretrained)} & 3.48 & 3.48 & 3.5 & 0.4 &  0.021 & 0.0087 \\
    \textbf{Image-Sentence TACL15~\cite{kiros2014unifying} (retrained)} & 5.87 & 6.06 & 6.15 & 3.16 & 0.29 & 0.29 \\
        \textbf{Chance} & 0.0017& -& - & -  &  - & -\\
\hline
    \end{tabular}
    }
\end{table}
\end{comment}\begin{figure}[b!]
\vspace{-3mm}
  \includegraphics[width=0.22\textwidth,height=0.28\textwidth]{K1_PASCAL12_noICLR.eps}
        \includegraphics[width=0.22\textwidth,height=0.28\textwidth]{K1_Stanford40_noICLR.eps}
       \includegraphics[width=0.22\textwidth,height=0.28\textwidth]{K1_6DS_noICLR.eps}
             \includegraphics[width=0.31\textwidth,height=0.29\textwidth]{K1_LSC_v2_noICLR.eps}
                    \vspace{-1mm}
        \caption{K1 Performance Across Different Datasets. These graphs show the  advantage of the proposed models as the scale increases from  left to  right. (R) for TACL15  means the retrained version, (C) means COCO pretrained model; see Sec~\ref{sec_setup}}
        \label{fig:lsc_scalability2}
\end{figure}\begin{comment}
\begin{figure*}
  \includegraphics[width=0.5\textwidth]{K1_PASCAL12_noICLR.eps}
        \includegraphics[width=0.5\textwidth]{K1_Stanford40_noICLR.eps}
       \includegraphics[width=0.5\textwidth]{K1_6DS_noICLR.eps}
             \includegraphics[width=0.5\textwidth]{K1_LSC_v2_noICLR.eps}
        \caption{K1 Performance Across Different Datasets. These graphs show the  advantage of the proposed models as the scale increases from upper left to lower right. (R) for TACL15 and ICLR16 means their retrained versions; see Sec~\ref{sec_setup}}
        \label{fig:lsc_scalability2}
\end{figure*}


\end{comment}


Figure~\ref{fig:k1_as_Ex_inc} shows the Top10 large scale knowledge view retrieval (K10) results reported in Table~\ref{tab:atbl} broken out by fact type  and the number of images per fact. These results show that Model 2 generally behaves better with compared other models with the increase of facts. We noticed a slight increase for Model 1 over Model 2.
\begin{figure*}[t!]
  \includegraphics[width=0.5\textwidth]{Num_Facts_S_v2_noICLR.eps}
        \includegraphics[width=0.5\textwidth]{Num_Facts_SP_v2_noICLR.eps}
       \includegraphics[width=0.5\textwidth]{Num_Facts_SPO_v2_noICLR.eps}
             \includegraphics[width=0.5\textwidth]{Num_Facts_All_v2_noICLR.eps}
        \caption{K10 Performance ($y$-axis) versus the number of images per fact ($x$-axis). Top Left: Objects (S), Top Right: Attributed Objects and Objects performing Actions (SP), Bottom Left: Interactions (SPO), Bottom Right: All Facts.}
        \label{fig:k1_as_Ex_inc}
          \vspace{-5mm}
\end{figure*}%We further analyze the results where the number of examples per fact is very small.
It is desirable for a method to be able to generalize to understand an SPO interaction from training examples involving its components, even when there are zero or very few training examples for the exact SPO  with all its parts S,P and O. Table~\ref{tbl_gen_SPO} shows the K10 performance for SPOs where the number of training examples is $\le 5$.
For example, the column $\textbf{SP$\ge$15, O$\ge$15}$  means $\le 5$ examples of an SPO that has at least 15 examples for the SP part and for the O part. An example of this case is when we see zero or very few examples of $<$person, petting, horse$>$, but we see at least 15 examples of $<$person, petting, something$=$dog/cat/etc (not horse)$>$ and at least 15 examples of something interacting with a horse $<$*,*, horse$>$. Model2  performs the best in all the listed generalization cases in   Table~\ref{tbl_gen_SPO}.
We found a similar generalization behavior for SP facts that have no more than 5 examples during training. We add more figures and additional results in the supplementary materials.


\begin{table}[t!]
  \centering
  \caption{Generalization: SPO Facts of less than or equal 5 examples (K10 metric)}
  \label{tbl_gen_SPO}
  \scalebox{0.6}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \toprule
    \textbf{Cases} & \textbf{SP$\ge$15, O$\ge$15} & \textbf{PO$\ge$15, $\ge$15} & \textbf{SO$\ge$15, P$\ge$15} & \textbf{S$\ge$15, PO$\ge$15} & \textbf{SO$\ge$15, PO$\ge$15} & \textbf{SO$\ge$15, SP$\ge$15} & \textbf{S,P,O$\ge$15} & \textbf{S,P,O$\ge$100} \\
 \hline
    \textbf{NumFacts for this case} & \textbf{10605} & \textbf{9313} & \textbf{4842} & \textbf{4673} & \textbf{1755} & \textbf{3133} & \textbf{21616} & \textbf{12337} \\
    \hline
    \textbf{Model2} & \textbf{2.063} & \textbf{2.026} & \textbf{3.022} & \textbf{2.172} & \textbf{3.092} & \textbf{2.962} & \textbf{1.861} & \textbf{2.462} \\
    \textbf{Model1} & 1.751 & 1.357 & 1.961 & 1.645 & 1.684 & 2.097 & 1.405 & 1.666 \\
    \textbf{ESZSL} & 0.149 & 0.107 & 0.098 & 0.066 & 0.041 & 0.038 & 0.240 & 0.176 \\
\textbf{    TACL15 (COCO pretrained) }& 0.013 & 0.024 & 0.025 & 0.019 & 0.000 & 0.013 & 0.034 & 0.027 \\
\textbf{ TACL15 (retrained) }& 0.367 & 0.380 & 0.473 & 0.384 & 0.543 & 0.586 & 0.353 & 0.438 \\
\textbf{    MV CCA }& 1.221 & 1.889 & 1.462 & 1.273 & 1.786 & 1.109 & 1.853 & 1.838 \\
 \hline
    \end{tabular}%
    }
  \label{tab:gen_SPO}
  \vspace{-5mm}
\end{table}%%Fig.~\ref{fig:mdls} shows some qualitative examples using our model for both-way retrieval.% Fig.~\ref{fig:mdls} (right) shows the top retrieved facts for the shown image. Fig.~\ref{fig:mdls} (left) shows the top related scenes for a fact which was not seen during training, which illustrates the model's capability to recognize unseen classes. %\begin{table}[htbp]%  \centering%     \vspace{-3mm}%  \caption{Large Scale Experiment Model 1 and 2}%    \label{tab:atbl}%    \scalebox{0.6}%    {%    \begin{tabular}{|c|c|c|c|c|c|c|}%   \hline%          & \multicolumn{4}{c|}{\textbf{Language View retrieval \%}} & \multicolumn{2}{c|}{\textbf{Visual view Retrieval \%}} \\%\hline%    \textbf{} & \textbf{Top1 Acc} & \textbf{Top 5 Acc} & \textbf{Top 10 Acc} & \textbf{MRR} & \textbf{mAP100} & \textbf{mAP10} \\%    \textbf{Model 1} & 13.27 & 14.19 & 14.80 & 7.88  & 0.61 & 0.62 \\%    \textbf{Model 2} & 15.41 & 16.45 & 17.1 & 9.60 &  0.77 & 0.77 \\%        \textbf{Chance} & 0.0017& -& - & -  &  - & -\\%\hline%    \end{tabular}%    }%    \vspace{-7mm}%\end{table}%%%\begin{table}[htbp]%  \centering%  \caption{Large Scale Experiment Model 1 and 2}%    \label{tab:atbl}%    \scalebox{0.8}%    {%    \begin{tabular}{|c|c|c|c|c|c|c|}%   \hline%          & \multicolumn{4}{c|}{\textbf{Language View retrieval \%}} & \multicolumn{2}{c|}{\textbf{Visual view Retrieval \%}} \\%\hline%    \textbf{} & \textbf{Top1 Acc} & \textbf{Top 5 Acc} & \textbf{Top 10 Acc} & \textbf{MRR} & \textbf{mAP100} & \textbf{mAP10} \\%    \textbf{Model 1 (my metric)} & 13.27 & 14.19 & 14.80 & 7.88  & 0.62 & 0.61 \\%    \textbf{Model 2  (my metric)} & 15.41 & 16.45 & 17.1 & 9.60 &  0.77 & 0.77 \\%    \hline%    \textbf{Model 1 (metric Dr.Scott)} & 8.8 & 14.48 & 16.6& 11.3  & 0.62 & 0.61 \\%    \textbf{Model 2 (metric Dr.Scott)} & 10.95 & 16.96 & 19.12 & 13.58  &  0.77 & 0.77 \\%    \hline%        \textbf{Chance} & 0.0017& -& - & -  &  - & -\\%\hline%    \end{tabular}%    }%\end{table}%\begin{figure}[t!]%   \centering%\includegraphics[width=0.8\textwidth]{kex_ret_Ex1} % \caption{Examples for Language View retrieval and Visual View Retrieval ( red color facts means that fact is not seen during the training) }% \label{fig:ex1}% \vspace{-5mm}%\end{figure}%%\begin{figure}[t!]%   \centering%\includegraphics[width=0.8\textwidth]{kex_ret_Ex1} % \caption{Examples for Language View retrieval and Visual View Retrieval ( red color facts means that fact is not seen during the training) }% \label{fig:ex1}% \vspace{-5mm}%\end{figure}%\begin{figure}[t!]%   \centering%\includegraphics[width=0.35\textwidth]{man_on_sand}%\includegraphics[width=0.50\textwidth]{ret_ex3} % \caption{Examples for Language View retrieval (left) and Visual View Retrieval (right)}  %  \vspace{-5mm}% \label{fig:ex2}%\end{figure}\begin{comment}
\begin{figure*}
  \includegraphics[width=0.5\textwidth]{Scalability_Pascal12.eps}
        \includegraphics[width=0.5\textwidth]{Scalability_Stanford40.eps}
       \includegraphics[width=0.5\textwidth]{Scalability_6DS.eps}
             \includegraphics[width=0.5\textwidth]{Scalability_LSC.eps}
        \caption{K1 Model2 Improvement over baselines Across Different Datasets}
        \label{fig:lsc_scalability2}
 \label{fig:mdls}
\end{figure*}
\end{comment}%\begin{figure}[t!]%   \centering%\includegraphics[width=0.4\textwidth]{examples.png}    % \caption{Examples for fact and image retrieval tasks}% \label{fig:mdls}%\end{figure}\begin{comment}
\begin{figure}[t!]
   \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{knowledge_extraction.png}
        \caption{Language View retrieval}
        \label{fig:ret_q1_ex}
    \end{subfigure}
      \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{image_retrieval.png}
        \caption{Visual View retrieval (unseen fact)}
        \label{fig:kn_ex_qn}
    \end{subfigure}
    \vspace{-2mm}
 \caption{Examples for fact and image retrieval tasks}
 \label{fig:ex2}
\end{figure}
\end{comment}




\section{Conclusion}


We introduce new setting  for learning unbounded number of facts in images, which could be referred to as a model for gaining visual knowledge. The facts could be of different types like objects, attributes, actions, and interactions.  While studying this task, we consider  Uniformity, Generalization, Scalability, Bi-directionality, and Structure. We investigated several baselines from multi-view learning literature which were adapted to the proposed setting. We proposed learning representation methods that outperform the designed baseline mainly by the advantage of relating facts by structure.




\clearpage


\bibliographystyle{splncs03}
\bibliography{egbib}


\clearpage
\section*{Supplementary Materials}


The supplementary include the following materials




7) Large Scale Experiment Data




8) Training and Implementation Details






9) More Details about the metric in the Large Scale Experiment




10) Additional Language Retrieval Result (for each fact type separately)


11) Language View Retrieval Qualitative Results




12) Visual View Retrieval Qualitative Results


13) Qualitative Results for Language View Retrieval Generalization


\section{Large Scale Experiment Data}
\label{app:lsc_data}




\subsection{Training images of the test facts (related to Fig~9 in the paper)}


Fig.~\ref{fig:numfacts} shows more details about the number of training image for each test fact. The $x-$ axis shows different ranges for each. The $y-$ axis shows the number of test facts whose number of training examples falls between the specified range of examples.


Note that the $x-$ axis here is the same as the $x-$ axis in Fig~9 in the paper.








\begin{figure*}[b!]
             \vspace{-3mm}
  \includegraphics[width=0.8\textwidth]{Num_facts_stats_All.eps}
        \includegraphics[width=0.8\textwidth]{Num_facts_stats_SPO.eps}
       \includegraphics[width=0.8\textwidth]{Num_facts_stats_SP.eps}
             \includegraphics[width=0.8\textwidth]{Num_facts_stats_S.eps}
             \vspace{-3mm}
        \caption{Number of Facts per each ``Number of Images Range''. $x-$axis shows ranges of number of images per fact. $y-$ axis is the number of facts whose number of images fall in the corresponding Range}
        \label{fig:numfacts}
\end{figure*}








\clearpage
\subsection{Test images}




We show the test examples in two groups.


\begin{itemize}
	\item The first group is the group of facts that has at least one training image (seen fact) in Fig~\ref{fig:facts_seen} .
\item The second group of facts is the group where there is no training images at all; see Fig.~\ref{fig:facts_unseen}.
\item The $x-$axis  shows the "fact identifier" where facts are sorted from largest to smallest number of images.
\item $y-$axis the number of test images in each of them. Each figure has three plot, one for each fact type $<$S$\ge$, $<$S,P$\ge$,$<$S,P,O$\ge$.
\item The figures show that  in both cases, the majority of the facts have at most one test example.
\item This quantitatively shows the difficulty of the evaluation especially for the large scale setting for those facts.
\end{itemize}














\begin{figure*}[ht!]
  \centering
    \includegraphics[width=0.8\textwidth]{fact_int_freq.pdf}


     \caption{26,740 unique test facts that have at least one training example (seen facts), total of 136,040  images ($x-$ axis shows these facts, $y-$ axis shows the number of test images per each fact)}
     \label{fig:facts_seen}
\end{figure*}
\begin{figure*}[ht!]
  \centering
    \includegraphics[width=0.8\textwidth]{fact_unseen_freq.pdf}
     \caption{31,677 unique unseen test facts, total of 32,651 images ($x-$ axis shows  these facts, $y-$ axis shows the number of test images per each fact)}
     \label{fig:facts_unseen}
\end{figure*}
















\clearpage
\section{Training and Implementation Details}
\textbf{GPU Framework} Our implementation was based on Caffe~\cite{jia2014caffe} for our implementation.


\textbf{Training Parameters} Model 1 and Model2 were trained by back propagation with stochastic gradient descent. The base learning rate is assigned $0.5 \times 10^{-4}$. For fine-tuning, the learning rate of the randomly initialized parameters are assigned to be ten times faster than the learning rate of the remaining parameters(  initialized from the pretrained CNN). Please refer to the paper where randomly initialized parameters are specified for each of Model 1 and Model 2.




The decay of the learning rate $\gamma$ is 0.1. While training our CNNs, we drop the learning rate by a factor of $\gamma$ every 5000 iterations. The momentum and the weight decay were assigned to 0.9 and 0.0001 respectively. Training images are randomly shuffled before feeding the CNN for training. The training batch size was 100 images.


\textbf{Training and testing batches}  At training time, we randomly sample 224x224 batches from the down-scaled 256x256 images. At test time the center 224x224 batches are taken.


\textbf{Normalization of GloVE word vectors~\cite{pennington2014glove}:} For the structured fact language encoder (Fig 4 c in the paper), we normalize the S, P, and O vectors to L2 norm 1 individually. Then we subtract the mean of all the training vectors. This is similar to subtracting the mean of the image for encoding the visual view of the image. For first-order, we fill the P and O parts with zeros. For second-order facts, we fill the O part with zeros.




As illustrated in the paper,   Model 1 and Model 2 do not penalize first order facts for P and O, and do not penalize second order facts for O; see Eq 7 in the paper (lines 333 to 334, page 8).












\clearpage
\section{Details about the Language View Retrieval Metric in the Large Scale Experiment used in our experiments}


In the language view retrieval metric used in our submission, we created a database for all facts of 900 dimensional vectors (300 dimensions for S, followed by 300 dimensions for P, followed by 300 dimensions for O).  It may not be hard to see that, our methods leans to produce more specific facts ( higher order facts compared to lower order facts; highest order fact is the third order facts).  This is because lower order facts have zeros in the unspecified fact components.


In order  to avoid incorrectly penalize a method for being more specific, we do not penalize more specific facts of the ground truth facts. This cases only happens for ground truth first and second-order facts.


For first-order ground truth facts $<$S$>$,  the retrieved second  $<$S,P$>$ and third order facts $<$S,P,O$>$ that have exactly the same ground truth subject S are not penalized. For example if the ground truth fact is $<$car$>$ and the retrieved fact is $<$car, red$>$.


For second-order ground $<$S,P$>$ truth facts, the retrieved third order facts that have exactly the same ground truth subject S and predicate P are not penalized. For example if the ground truth is $<$person, playing$>$ and the retrieved fact is $<$person, playing, guitar$>$.




We attach the code that performs the evaluation in our experiments from which our results could be reproduced.   It could also be used to evaluate any other method for comparison to our work.  We name this metric as ``Metric 1''


\textbf{Attachment: }The implementation of Metric 1 could be found in the attached  \textbf{GetSherlockResults\_ANN\_metric1.m}.






We also report all the results that perform tagging for each fact type separately in Sec~\ref{sherlock_metric2} in this supplementary.






\clearpage
\section{Language Retrieval Result with Metric2 Defined below ( uses a KD Tree database for Each Fact Type)}
\label{sherlock_metric2}
In this section, we present more results, where we deal with each fact order separately at test time. There is no change during training; it is exactly the same model. At test time,  we build three KD Tree databases, one for each fact order; i.e., one for first-order, one for second-order, and one for third order facts. At test time, we are given a test image that might  include first-,second-, and/or third-order facts. We then use the given model to check the closest 100 facts in each database depending on its type. So, this step produces three lists, which are (1) relevant first-order facts,(2) second-order facts, (3) third-order facts given the image. Then, the rank for each  ground truth facts  is checked in its corresponding  list based on its type.


For instance, if an image has two facts (such as  $<$ man, riding, horse $>$, $<$man, Asian$>$)). We look-up the rank for $<$ man, riding, horse $>$ in the list of third-order facts and we
look-up the rank for $<$man, Asian$>$) in the list of second-order facts.


In order to compute top K-performance, the rank of the ground truth facts are checked. Accordingly, top K means the $L$ facts are in the top $L+K-1$ retrieved facts, where $L$ is the number of ground truth facts in the image of the same order.  However, it is checked for each fact type separately  for Metric 2 .


\textbf{This metric tests accuracy for a given order of specificity (i.e. which order fact are you interested in to describe a given image). We name this metric as \textbf{``Metric 2''}.}
The metric in the paper has a bias toward tagging with the most specific fact (third order). We name this metric  in the main paper (pages 13 and 14) as \textbf{``Metric 1''}.




Accordingly, we can produce here a new set of results based on \textbf{``Metric 2''} as opposed to \textbf{``Metric 1''}  which is reported in the paper (pages 13 and 14).


\textbf{Attachment:} The implementation of Metric 2 could be found in the attached  \textbf{GetSherlockResults\_ANN\_metric2.m}.




\begin{table}[htbp]
  \centering
  \caption{ (Metric 2) Language View Retrieval Summary Results}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
          & \textbf{K1} & \textbf{K5} & \textbf{K10} & \textbf{MRR} \\ \hline
    \textbf{Model2} & \textbf{11.200} & \textbf{17.800} & \textbf{20.300} & \textbf{14.200} \\
    \textbf{Model1} & 9.220 & 15.400 & 18.000 & 12.000 \\
    \textbf{MV CCA} & 3.690 & 4.970 & 5.400 & 4.250 \\
    \textbf{ESZSL} & 2.350 & 3.070 & 3.300 & 2.680 \\
    \textbf{TACL15  ( retrained )} & 0.870 & 1.650 & 1.940 & 1.190 \\
    \textbf{TACL15  ( Coco pretrained )} & 0.080 & 0.169 & 0.282 & 0.153 \\ \hline
    \end{tabular}%
  \label{tab:k10_db_threedb123}%
\end{table}%\clearpage\subsection{Results as Number of Images per Fact Increases (Metric 2)}\begin{figure*}[b!]
             \vspace{-3mm}
  \includegraphics[width=0.8\textwidth]{Num_Facts_S_v3_noICLR.eps}
      \vspace{-2mm}
        \includegraphics[width=0.8\textwidth]{Num_Facts_SP_v3_noICLR.eps}
              \vspace{-2mm}
       \includegraphics[width=0.8\textwidth]{Num_Facts_SPO_v3_noICLR.eps}
             \vspace{-2mm}
             \includegraphics[width=0.8\textwidth]{Num_Facts_All_v3_noICLR.eps}
             \vspace{-3mm}
        \caption{ (Metric 2) K10 Performance ($y$-axis) versus the number of images per fact ($x$-axis). First from Top: Objects (S), Second from Top: Attributed Objects and Objects performing Actions (SP), Third from Top: Interactions (SPO), Fourth from Top: All Facts.}
        \label{fig:k1_as_Ex_inc}
          \vspace{-5mm}
\end{figure*}\clearpage\subsection{Generalization Results (Metric 2)}
We see in Table~\ref{tab_k10_metric2} that our Model 2 has better generalization capabilities using Metric 2 than other methods overall and for most specific generalization scenarios. In two of the generalization senarios, Model 2 was a close second behind our Model 1 and MV CCA. Recall that with Metric 1, our Model 2 was best in generalization across all scenarios.


\begin{table}[htbp]
  \centering
  \caption{ (Metric 2) Generalization Results (three KDTree databases, one for each fact type)  K10 metric}
  \scalebox{0.63}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
    \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\textbf{SP}} & \multicolumn{6}{c|}{\textbf{SPO}}              & \multicolumn{1}{c|}{\textbf{Total}} \\ \hline
    \multicolumn{1}{|c|}{\textbf{NumFacts for this case}} & \multicolumn{1}{c|}{\textbf{14448}} & \multicolumn{1}{c|}{\textbf{10605}} & \multicolumn{1}{c|}{\textbf{9313}} & \multicolumn{1}{c}{\textbf{4842}} & \multicolumn{1}{c|}{\textbf{4673}} & \multicolumn{1}{c|}{\textbf{1755}} & \multicolumn{1}{c|}{\textbf{3133}} & \multicolumn{1}{c|}{\textbf{48769}} \\ \hline
    \multicolumn{1}{|c|}{\textbf{Case}} & \multicolumn{1}{c|}{\textbf{S$\ge$15,P$\ge$15}} & \multicolumn{1}{c|}{\textbf{SP$\ge$15, O$\ge$15}} & \multicolumn{1}{c|}{\textbf{PO$\ge$15, S$\ge$15}} & \multicolumn{1}{c|}{\textbf{SO$\ge$15, P$\ge$15}} & \multicolumn{1}{c|}{\textbf{SP$\ge$15, PO$\ge$15}} & \multicolumn{1}{c|}{\textbf{SO$\ge$15, PO$\ge$15}} & \multicolumn{1}{c|}{\textbf{SO$\ge$15, SP$\ge$15}} & \multicolumn{1}{c|}{\textbf{All}} \\ \hline
    \textbf{Model2} & \textbf{3.656} & {1.991} & 1.802 & \textbf{2.709} & \textbf{1.854} & \textbf{2.635} & \textbf{1.162} & \textbf{2.476} \\
    \textbf{Model1} & 3.041 & \textbf{2.066} & 1.546 & 2.346 & 1.891 & 2.248 & 1.009 & 2.205 \\
    \textbf{MV CCA} & 2.199 & 1.382 & \textbf{1.907} & 1.320 & 1.480 & 1.372 & 0.737 & 1.686 \\
    \textbf{ESZSL} & 1.098 & 0.258 & 0.214 & 0.211 & 0.165 & 0.204 & 0.201 & 0.479 \\
    \textbf{TACL15 (retrained)} & 1.619 & 1.140 & 1.385 & 1.114 & 1.454 & 1.457 & 1.215 & 1.372 \\
    \textbf{TACL15 (COCO pretrained)} & 0.121 & 0.067 & 0.040 & 0.051 & 0.050 & 0.000 & 0.038 & 0.070 \\ \hline
    \end{tabular}
}
  \label{tab_k10_metric2}%
\end{table}%


\clearpage
\section{ Language View  Retrieval Qualitative Results}
\label{app_kex}
Facts colored in \textcolor{red}{red}  were never seen during training. Facts colored in \textcolor{blue}{blue}  have at least one training example.


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.48\textwidth]{dog_chasing_dog.jpg}
    \includegraphics[width=0.48\textwidth]{bear_laying_head.jpg}
        \includegraphics[width=0.48\textwidth]{bear_resting_head.jpg}
            \includegraphics[width=0.48\textwidth]{cat_sitting_on_chair.jpg}
\includegraphics[width=0.48\textwidth]{cat_sitting_in_sink.jpg}
\includegraphics[width=0.48\textwidth]{men_riding_bike.jpg}
   \end{figure}\begin{figure}[h!]
   \includegraphics[width=0.48\textwidth]{boat_pulling_boat_unseen.jpg}
\includegraphics[width=0.48\textwidth]{airplane_behind.jpg}
\includegraphics[width=0.48\textwidth]{airplane_on_land.jpg}
\includegraphics[width=0.48\textwidth]{boy_playing_ball.jpg}
\includegraphics[width=0.48\textwidth]{bus_behind_bus.jpg}
\includegraphics[width=0.48\textwidth]{bus_driving_street.jpg}
   \end{figure}\begin{figure}[h!]
\includegraphics[width=0.48\textwidth]{man_riding_motorcycle.jpg}
\includegraphics[width=0.48\textwidth]{man_riding_skateboard.jpg}
\includegraphics[width=0.48\textwidth]{man_riding_waves.jpg}
\includegraphics[width=0.48\textwidth]{man_riding_waves2.jpg}
   \end{figure}\begin{figure}[h!]
\includegraphics[width=0.48\textwidth]{people_riding_horse_conf.jpg}
\includegraphics[width=0.48\textwidth]{person_riding_horse_conf.jpg}
\includegraphics[width=0.48\textwidth]{person_wearing_sth.jpg}
\includegraphics[width=0.48\textwidth]{riding_biks.jpg}\\
\includegraphics[width=0.48\textwidth]{riding_horse.jpg}
\includegraphics[width=0.48\textwidth]{sheep_standing_side.jpg}
  \end{figure}\begin{figure}[h!]
\includegraphics[width=0.48\textwidth]{tower_building.jpg} \includegraphics[width=0.48\textwidth]{woman_ball.jpg}\\
\end{figure}


\clearpage
\section{ Visual View Retrieval Qualitative Results}
\label{app_ret}


Facts colored in \textcolor{red}{red}  were never seen during training. Facts colored in \textcolor{blue}{blue}  have at least one training example. \textcolor{green}{Green} boxes in the retrieved images indicate the images that were annotated by the query fact. It is easy to see that the method retrieves a lot of relevant examples for which it was not given credit, which opens the door to explore better metrics for Sherlock Problem. As illustrated in the experiments section, our large scale setting has hundreds of thousands of facts with the majority of them have one or few examples. The following examples show how our model managed to retrieve these examples to the top of the list given the fact in the language view.


\subsection{Unseen Facts during training}\begin{figure}[h!]
  \centering
    \includegraphics[width=0.48\textwidth]{airplane_ocean.jpg}
   \includegraphics[width=0.48\textwidth]{birds_eating_from_ocean.jpg}
     \includegraphics[width=0.48\textwidth]{boy_toothbrushjpg.jpg}
   \includegraphics[width=0.48\textwidth]{brococlo_bowl.jpg}  \includegraphics[width=0.48\textwidth]{dog_truck.jpg}
   \includegraphics[width=0.48\textwidth]{drawer_next_oven.jpg}  \includegraphics[width=0.48\textwidth]{elephant_nearing_food.jpg}
   \includegraphics[width=0.48\textwidth]{giraff_walking_road.jpg}
   \end{figure}\begin{figure}[h!]
    \includegraphics[width=0.48\textwidth]{girl_umbrealla.jpg}
   \includegraphics[width=0.48\textwidth]{girl_using_racket.jpg}  \includegraphics[width=0.48\textwidth]{man_urinating_pathroom.jpg}
   \includegraphics[width=0.48\textwidth]{snow_ontop_mountainsjpg.jpg}
      \includegraphics[width=0.48\textwidth]{stove_leftof_sink.jpg}
            \includegraphics[width=0.48\textwidth]{stove_leftof_sink.jpg}      \includegraphics[width=0.48\textwidth]{stove_leftof_sink.jpg}      \includegraphics[width=0.48\textwidth]{text_above_picjpg.jpg}
            \end{figure}\begin{figure}[h!]
             \includegraphics[width=0.48\textwidth]{train_x_stationjpg.jpg}      \includegraphics[width=0.48\textwidth]{tree_behind_ramp.jpg}
\end{figure}%\begin{figure}[h!]%  \caption{Fact }%  \centering%    \includegraphics[width=0.5\textwidth]{gull}%\end{figure}\clearpage\subsection{Seen Facts during training}\begin{figure}[h!]
  \centering
    \includegraphics[width=0.45\textwidth]{people_riding_horses.jpg}
   \includegraphics[width=0.45\textwidth]{person_riding_bike.jpg}
     \includegraphics[width=0.45\textwidth]{person_riding_horse.jpg}
   \includegraphics[width=0.45\textwidth]{woman_riding_wave.jpg}
         \includegraphics[width=0.45\textwidth]{bus_driving_street_ret.jpg}  \includegraphics[width=0.45\textwidth]{man_riding_wave.jpg}
            \end{figure}\begin{figure}[h!]
   \includegraphics[width=0.48\textwidth]{person_riding_bike.jpg}
   \includegraphics[width=0.48\textwidth]{man_riding_skateboardjpg.jpg}  \includegraphics[width=0.48\textwidth]{man_flying_kite.jpg}
   \includegraphics[width=0.48\textwidth]{woman_flying_kite.jpg}
   \includegraphics[width=0.5\textwidth]{dog_on_sofa.jpg}
               \end{figure}\begin{figure}[h!]
     \includegraphics[width=0.48\textwidth]{girl_carrying_luggage.jpg}
   \includegraphics[width=0.48\textwidth]{girl_hold_surfboard.jpg}  \includegraphics[width=0.48\textwidth]{girl_walking_dog.jpg}
   \includegraphics[width=0.48\textwidth]{group_cover_mountain.jpg}  \includegraphics[width=0.48\textwidth]{group_riding_surfboard.jpg}
   \includegraphics[width=0.48\textwidth]{group_riding_wave.jpg}
   \end{figure}\begin{figure}[h!]


    \includegraphics[width=0.48\textwidth]{people_Cake.jpg}
   \includegraphics[width=0.48\textwidth]{people_skiiing_hill.jpg}  \includegraphics[width=0.48\textwidth]{person_behind_person.jpg}
   \includegraphics[width=0.48\textwidth]{dog_chase_dog.jpg}
\end{figure}\clearpage




\clearpage
\section{Qualitative Results for Language View Retrieval Generalization}
In all the following figures, the number of training images are less than 5 examples, but the model shows some generalization cases that we discussed in Table~5 in the main paper and Table 2  in this document. In the following examples, the ground truth is  on the top of the list.




\begin{figure}[h!]
\centering
   \caption{SPO$\le$5,  PO$\ge$15 and S$\ge$15 }
   \includegraphics[width=0.8\textwidth]{Gen_SPO_PO_S_ex1.jpg}
    \includegraphics[width=0.8\textwidth]{Gen_SPO_PO_S_ex2.jpg}
\end{figure}
\clearpage




\begin{figure}[h!]
\centering
   \caption{SPO$\le$5, SO$\ge$15 and P$\ge$15 }
   \includegraphics[width=0.8\textwidth]{Gen_SPO_SO_P_ex3.jpg}
    \includegraphics[width=0.8\textwidth]{Gen_SPO_SO_P_ex2.jpg}
\end{figure}
\clearpage






\begin{figure}[h!]
\centering
   \caption{SPO$\le$5, SP$\ge$15 and O$\ge$15 }
   \includegraphics[width=0.8\textwidth]{Gen_SPO_SP_O_ex3.jpg}
   \includegraphics[width=0.8\textwidth]{Gen_SPO_SP_O_ex2.jpg}
\end{figure}




\clearpage


\begin{figure}[h!]
\centering
   \caption{SPO$\le$5, SO$\ge$15 and PO$\ge$15 }
   \includegraphics[width=0.8\textwidth]{Gen_SPO_SO_PO_ex3.jpg}
   \includegraphics[width=0.8\textwidth]{Gen_SPO_SO_PO_ex1.jpg}
\end{figure}


\clearpage












\begin{figure}[h!]
\centering
   \caption{SPO$\le$5, SP$\ge$15 and PO$\ge$15 }
   \includegraphics[width=0.8\textwidth]{Gen_SPO_SP_PO_ex2.jpg}
      \includegraphics[width=0.8\textwidth]{Gen_SPO_SP_PO_ex1.jpg}
\end{figure}








\clearpage




\begin{figure}[h!]
\centering
   \caption{SPO$\le$5, SO$\ge$15 and SP$\ge$15 }
   \includegraphics[width=0.8\textwidth]{Gen_SPO_SO_SP_ex1.jpg}
\end{figure}


\clearpage




\end{document}


