\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color}
\usepackage{makecell}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{float}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{1445} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\def\mypar#1{\vspace{1mm}{\bf #1.}\hspace{1mm}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi

\graphicspath{ {./figures/} }

\begin{document}

%%%%%%%%% TITLE
\title{Vector Image Generation by Learning Parametric Layer Decomposition}

\author{
Othman Sbai$^{1,2}$, Camille Couprie$^1$, Mathieu Aubry$^2$\\
$^1$Facebook AI Research, $^2$LIGM (UMR 8049) - \'Ecole des Ponts, UPE
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   Deep image generation is becoming a tool to enhance artists and designers creativity potential. In this paper, we aim at making the generation process easier to understand and interact with. Inspired by vector graphics systems, we propose a new deep generation paradigm where the images are composed of simple layers, defined by their color and a parametric transparency mask. This presents a number of advantages compared to the commonly used convolutional network architectures. In particular, our layered decomposition allows simple user interaction, for example to update a given mask, or change the color of a selected layer. From a compact code, our architecture also generates images with a virtually infinite resolution, the color at each point in an image being a parametric function of its coordinates. We validate the viability of our approach in the auto-encoding framework by comparing reconstructions with state-of-the-art baselines given similar memory resources on CIFAR10, CelebA and ImageNet datasets and demonstrate several applications. We also show Generative Adversarial Network (GAN) results qualitatively different from the ones obtained with common approaches.
\end{abstract}


\section{Introduction}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/teaser}
    \caption{Our system learns in an unsupervised manner a decomposition of images as superimposed masks, of uniform color.}
    \label{fig:my_label}
\end{figure}

%---------------------------------
% Motivation: interest of the task
%---------------------------------

Deep image generation models demonstrate breathtaking and inspiring results~\cite{Zhu2017cycleGANs,Zhu2017bicycleGAN}, but usually offer limited control and little interpretability. In contrast, we explore a new deep image generation paradigm, designed for simple interpretation and easy interaction. 
We aim at enabling designers to directly build on the results of deep image generation methods, by editing objects individually, changing their characteristics, or intervening in the middle of the generation process.
Following an approach similar to the one used in interactive design tools, we formulate image generation as the composition of successive parametric layers, each associated to a single color.
We argue that such a vector representation, allowing virtually infinite resolution, is a viable alternative to the exploration of always higher pixel resolution \cite{karras2017progressive,brock2018large}.

We believe the interpretability and interactivity of an approach are getting more important as generative models are considered in an increasing number of applications including inpainting~\cite{Ulyanov2018deepPrior}, medical imaging~\cite{frid2018gan,nie2017medical}, fashion image generation~\cite{sbai2018design}, image editing~\cite{perarnau2016invertible}. 

%--------------------------------------
% Description of traditional approaches
%--------------------------------------

Our attempt is in line with the long standing Computer Vision trend to look for a simplified and compact representation of the visual world. For examples, in 1971 Binford~\cite{binford1971visual} proposes to represent the 3D world using generalized cylinders and in 1987 the seminal work of Biederman~\cite{biederman1987recognition} aims at explaining and interpreting the 3D world and images using geons, a family of simple parametric shapes. These ideas have recently been revisited using Neural Networks to represent a 3D shape using a set of blocks~\cite{tulsiani2017learning} or, more related to our approach, a set of parametric patches~\cite{groueix2018atlasnet}. 
The idea of organizing these shapes in layers has been successfully applied to model images~\cite{adelson1991layered,SceneCollaging} and videos~\cite{wang1994representing}. 
A classical texture generation method, the dead leaves model~\cite{lee2001occlusion} which creates realistic textures by relying on the iteration of simple patterns addition is particularly related to our work. 
In particular, our GAN generation results are reminiscent of~\cite{alvarez2015exploring}, which relies on the exploitation of principles from abstract art~\cite{zheng2015layered} or from Gestalt theory to obtain visually meaningful image generations.

%-----------------------------------------
% Our approach
%-----------------------------------------

We build on this idea of composing layers of simple primitives in order to design a deep image generation method. The core ingredient of our method is a mask blending module which learns a family of parametric transparency masks, defined as continuous functions on the unit square. %We can thus generate mask and final images at arbitrary resolutions, simply by sampling positions position in a lattice of the target resolution. 
At each step of our generation process, a network predicts both parameters and color for one mask. Our final generated image is the result of blending a fixed number of colored masks. One of the advantages of this approach is that, differently to most existing deep generation setups where the generation is of fixed size, our generations are vector images defined continuously, and thus have virtually infinite resolution. Another key aspect is that the generation process is easily interpretable, allowing simple user interaction. %by a human, who can potentially interact with it. \\% that is decoded into the corresponding intensity of a pixel in a mask. %The inspiration comes from \cite{groueix2018shape} where transformations parameters are similarly learned from coordinates of 3D body meshes. 
%A unique color for each mask is also learned, and iteratively we may compose either a new image from scratch, or approximate an existing one.   


Our main contributions are the following:
\begin{itemize}
    \item We introduce a paradigm for image generation, based on a compact and vectorized layered representation.
    %propose an unsupervised, end-to-end trainable, novel image reconstruction approach that results in a vectorized, compact representation.
    \item We show the potential of our approach in an auto-encoding and generative adversarial framework.%Based on the same principle, we propose generations from random numbers leading to the first vector image GAN representations.  \todo{isn't that too strong given the results we currently have}
    \item We demonstrate the usefulness of our method for several applications, including image editing using generated masks.% Our iterative learning scheme allows user interaction at multiple levels.
\end{itemize}
    
%---------------------
\section{Related work}
%---------------------

As stated above, image generation models have a long history in computer vision. Here, we review in more detail recent deep learning based approaches to this task.
We first focus on some supervised deep image generation methods, then present the most related unsupervised strategies and finally deal with potential applications to image manipulation.

%-----------------------------------
\mypar{Supervised deep scene generation} 
%-----------------------------------
A layered image generation process is naturally employed in a number of recent methods, supervised by either segmentations~\cite{SceneCollaging}, bounding box and weaker localization~\cite{Reed2016LearningWhat}, or text~\cite{Yan2016Attribute2Image}. Isola and Liu~\cite{SceneCollaging} combine scene elements according to the semantic segmentation of a query image, resulting in  novel scene collages. Reed et al.~\cite{Reed2016LearningWhat}  synthesize images given instructions of the drawing location by the means of bounding boxes or point coordinates. Yan et al.~\cite{Yan2016Attribute2Image} extract attributes from text queries to reconstruct images.

Scene generation may also be conditioned on Scene Graphs, as in the work of Johnson et al.~\cite{Johnson2018Scene} that generates a layout of a scene by using a Cascaded Refinement Network (CRN)~\cite{Chen17Photographic}.
Qi et al.~\cite{Qi2018SemiParametric} improve over CRNs for generating images from semantic layouts using a semi-parametric approach. The authors first create a bank of image segments and learn to compose images from them. 

 From a foreground object segmentation and a background image,~\cite{Lin2018STGAN} composes an image looking as real as possible, using iterative warping of the object segments using a Spatial Transformer Network (STN)~\cite{Jaderberg2015STN}. Similarly, the Compositional GANs of~\cite{Azadi2018compositional} learn interactions between objects of different domains to compose novel scenes.
In contrast to these works, our approach does not specialize in scene generation and does not use semantic supervision.


%------------------------------------------
\mypar{Unsupervised deep sequential generative models}
%------------------------------------------
There are different ways to restrict the informational content of image representations when performing image reconstruction. For example,~\cite{Rolfe2013discriminative} uses a recurrent auto-encoder to reconstruct images iteratively, and employs a sparsity criterion to make sure that the image parts that are added at each iteration are simple. 
A second line of approaches~\cite{Gregor2015DRAW,Eslami2016AIR,gregor2016compression} optimize an upper bound of the log-likelihood {of the reconstructed image} in a Variational Auto-Encoder (VAE)~\cite{kingma14iclr} framework.
Deep Recurrent Attentive Writer (DRAW)~\cite{Gregor2015DRAW} frames a recurrent approach using reinforcement learning and a spatial attention mechanism to mimic human gestures. A potential application of DRAW arises in its extension to conceptual image compression~\cite{gregor2016compression}, where a recurrent convolutional and hierarchical architecture allows to obtain various levels of lossy compressed images.

Attend, Infer, Repeat (AIR)~\cite{Eslami2016AIR} models scenes by latent variables of object presence, content, and position. The parameters of presence and position are inferred by an RNN and a VAE decodes the objects one at a time to reconstruct images. 
Most related to our work, a third strategy for learning sequential generative models is to employ adversarial networks. Ganin et al.~\cite{ganin2018synthesizing} employ adversarial training in a reinforcement learning context. Specifically, their method dubbed SPIRAL, trains an agent to synthesize programs executed by a graphic engine to reconstruct images.    
Finally, the Layered Recursive GANs (LR-GAN) of~\cite{Yang2017LRGAN} learn to generate foreground and background images that are stitched together using STNs to form a consistent image. Although presented in a generic way that generalizes to multiple steps, the experiments are limited to foreground and background separation, made possible by the definition of a prior on the object size contained in the image. 
In contrast, our method (i) does not rely on STNs inducing a virtually infinite resolution at test time; (ii) extends to tens of steps as demonstrated in our experiments; (iii) relies on  simple architectures and losses, without the need of LSTMs or reinforcement learning.  


%------------------------------------------
% MODEL SCHEME
%------------------------------------------

\begin{figure*}[t!]
\begin{center}
\includegraphics[width=\linewidth]{figures/overview_figure.pdf}
\end{center}
    \vspace{-3mm}
    \caption{Our iterative generation pipeline for image reconstruction. The previous canvas $I_{t-1}$ ($I_0$ initialized to black) is concatenated with the target $I$ and forwarded through a ResNet feature extractor, to obtain a color $\textbf{c}_t$ and mask parameters $\textbf{p}_t$. 
    Our Mask Blending Module (in green) generates a parametric mask $M_t$ from pixelwise coordinates of a 2D grid and mask parameters $\textbf{p}_t$ via a Multi Layer Perceptron $f$.}
\label{fig:mask_generation_and_blending}
\end{figure*}


%-------------------------
\mypar{Image manipulation} 
%-------------------------
Some successful applications of deep learning to image manipulation have been demonstrated, but they are usually specialized and offer limited used interaction. Image colorization \cite{zhang2016colorful} and style transfer \cite{Gatys2016ImageStyleTransfer} are two popular examples. Zhu et al. \cite{Zhu2016manipulation} integrate user constraints in the form of brush strokes in GAN generations. More recently, \cite{bau2018gandissect} locates sets of neurons related to different visual components of images, such as trees or artifacts, and allows their removal interactively.
Fader Networks \cite{lample2017fader} or \cite{Mathieu16Disentangling} learn disentangled representations from datasets of images with attributes.  In contrast to these methods, our iterative scheme makes human interaction possible in the generation process at every step of the image composition.


%*********************************************************************************
\section{Layered Parametric Image Generation}
%*********************************************************************************

We frame image generation as an alpha-blending composition of a sequence of layers.
More precisely, we define our image generation procedure in a recurrent manner, given a fixed budget of $T$ iterations. We apply it  

in two settings: (i) image reconstruction, where a model is trained to output an image $I_{T}$ similar to a given target image $I$ by minimizing a reconstruction loss; (ii) adversarial generation, where a model is trained to map an input noise distribution to the distribution of images in a reference dataset. In both settings, we start from an empty (black) canvas $I_0$, and iteratively blend a total of $T$ generated colored masks onto it.

In this section, we first present the core idea of our approach in the context of the reconstruction setting, in particular our Mask Blending Module.
We then explain how it can be applied in the adversarial generation setting.
Finally, we detail our training losses for both settings. 

%---------------------------------
\subsection{Iterative generation with Mask Blending}
%---------------------------------

The core idea of our approach is visualized in Fig. \ref{fig:mask_generation_and_blending} for the reconstruction setting. At each iteration $t\in \left\lbrace 1...T\right\rbrace$, our model takes as input the concatenation of the target image $I \in \mathbb{R}^{3\times W\times H}$ and the result of the previous iteration $I_{t-1}$, to generate the updated image $I_t$ by
\begin{equation}
I_t = g(I_{t-1}, I),
\label{eq:aeset}
\end{equation} where $g$ consists of (i) 
a Residual Network (ResNet) which predicts mask parameters $\mathbf{p}_t\in \mathbb{R}^K$ with a corresponding color triplet $\mathbf{c}_t\in \mathbb{R}^3$, and (ii) our Mask Blending Module. 

This Mask Blending Module can be decomposed in two parts: first, it generates a mask $M_t$ using the predicted parameters $\mathbf{p}_t$, then blends it with the image $I_{t-1}$ and the predicted color $\mathbf{c}_t$.

We represent the function $f$ generating the mask as a standard Multi-Layer Perceptron (MLP), which takes as input the concatenation of the mask parameters $\mathbf{p}_t$ and the two spatial coordinates $(x,y)$ of a point in image space. At iteration $t$, this MLP $f$ defines the continuous 2D function of the mask $M_t$ by:
\begin{equation}
    M_t(x,y) = f(x,y,\mathbf{p}_t).
\end{equation}

In practice, we evaluate the mask at discrete spatial locations corresponding to the desired resolution to produce a discrete image.
We then update $I_{t}$ at each spatial location $(x,y)$ using the following blending: 
\begin{equation}
I_{t}(x,y) = I_{t-1}(x,y).(1-M_{t}(x,y)) + \mathbf{c}_t.M_{t}(x,y),
\label{alpha_blending}
\end{equation}
where $I_{t}(x,y)\in \mathbb{R}^3$ is the RGB value of resulting image $I_t$ at position $(x,y)$.

\begin{figure}[htb]
\centering
\includegraphics[width=0.5\textwidth]{figures/gan_setup.jpg}
\caption{Image generation from noise, using the Mask Blending Module of Fig. \ref{fig:mask_generation_and_blending} (green). Our model generates an image iteratively by predicting a mask from parameters $\mathbf{p}_t$, and color $\mathbf{c}_t$ which are blended on the previous canvas.}
\label{fig:GAN_setup} 
\end{figure}

%---------------------------------
\subsection{GAN setting}
%---------------------------------

In the Generative Adversarial Network (GAN) setting, illustrated in Fig.~\ref{fig:GAN_setup}, our input at each iteration consists of the previously generated image $I_{t-1}$, and a noise vector $\mathbf{\eta}_t \in \mathbb{R}^N$ sampled independently at each iteration: $\mathbf{\eta}_t \sim \mathcal{N}(0,1)$.
{The noise $\mathbf{\eta}_t$ is concatenated to the feature extracted from a ResNet. From this feature the mask and color parameters are predicted. They are then used by our Mask Blending Module to generate the next image as explained in the previous section.} 
Similar to the previous section this defines, for each iteration, a recursive relation:
\begin{equation}
I_t = g(I_{t-1}, \mathbf{\eta}_t).
\label{eq:ganset}
\end{equation}


%*********************************************************************************
\subsection{Training losses}
%*********************************************************************************
Performing $T$ iterations of the process described by $g$ (defined by equations \ref{eq:aeset} or \ref{eq:ganset} depending on the setting) defines an image generation model $G$.\\

In the GAN setting, we alternate the minimization of a loss over the generator $G$, $\mathcal{L}^{\textsc{adv}}_\textsc{G}$, and a loss over the discriminator $D$, $\mathcal{L}_\textsc{D}$.     

The discriminator $D$ is trained to recognize real images from generated ones, and we optimize our generator $G$ to fool this discriminator. We train $D$ to minimize the Wasserstein GAN with Gradient Penalty loss~\cite{gulrajani2017improved}

\begin{equation} \label{eq:disc_loss}
\begin{split}
\mathcal{L}_\textsc{D} =& -\mathbb{E}_{x \sim p_{d}}[D(x)] + \mathbb{E}_{\hat{x} \sim p_{g}}[D(\hat{x})]  \\
& + \gamma\mathbb{E}_{\hat{x} \sim p_{g}}[(||\nabla_{\bar{x}} D(\bar{x})||_2 - 1)^2],
\end{split}
\end{equation}
where $\gamma$ is a scale factor, and $\bar{x}$ is sampled uniformly between pairs of points sampled from the data and generated distributions $p_d$ and $p_g$.
The architecture of $D$ is the patch discriminator defined in~\cite{Isola2016ImageToImage} with 3 or 5 convolutional layers for image size of 32 or 128 respectively.
The generator adversarial loss is defined by
\begin{equation} \label{eq:gen_wgan_loss}
    \mathcal{L}^{\textsc{adv}}_\textsc{G}=-\mathbb{E}_{\hat{x} \sim p_{g}}[D(\hat{x})].
\end{equation}

In the image reconstruction setting, we learn $G$ by minimizing a reconstruction loss $\mathcal{L}^{\textsc{rec}}_G$ that consists of a weighted sum of a $\mathcal{L}_1$ loss and the adversarial loss $\mathcal{L}^{\textsc{adv}}_\textsc{G}$ defined above. 

We alternatively update the discriminator's weights to minimize $\mathcal{L}_\textsc{D}$ and the 
generator's weights to minimize
\begin{equation} \mathcal{L}^{\textsc{rec}}_G = \mathcal{L}_1 + \lambda \mathcal{L}^{\textsc{adv}}_\textsc{G},
\end{equation}

where $\lambda$ is a non-negative scalar parameter that controls the relative influence of the adversarial loss. 

%---------------------------------
\subsection{Discussion}
%---------------------------------
\paragraph{Differentiability.} The composition process is completely differentiable. 
Our generation can thus be modeled as a recurrent network that can be trained end-to end with standard optimizers. This is different from the recent SPIRAL~\cite{ganin2018synthesizing} approach, which uses an external painting library as a non-differentiable generator to update the drawn canvas, and thus requires a Reinforcement Learning (RL) formulation rewarding the agent with the final reconstruction accuracy.


\paragraph{Relation with Spatial Transformer Network \cite{Jaderberg2015STN}.}
In layered-GAN \cite{Yang2017LRGAN}, images are generated by combining a background and a foreground image deformed using Spatial Transformers \cite{Jaderberg2015STN}. In our case, explicitly using a Spatial Transformer Network is not necessary since our Mask Blending Module can itself generate masks at different positions in the image. %This is similar to our approach in the sense that However using spatial transformer networks 

\paragraph{Number of steps vs. size of the mask parameter.} Intuitively, our mask blending module iteratively adds strokes to the canvas to compose the final image. The size $K$ of the mask parameter $\mathbf{p}$ controls the complexity of the possible mask shapes, while the number of strokes controls the amount of different elements that can be used to compose the image. Since we aim at producing a set of layers that can easily be used and interpreted by a human, we use a limited number of strokes and masks. %  is small and a few tens of strokes are used, use i 
Interestingly, if the MLP generating the images from the parameter was very large, it could generate very complex patterns. In fact one could show using the universal approximation theorem \cite{cybenko1989approximation,hornik1991approximation} that, with a large hidden size of hidden units in the MLP $f$, an image could be approximated with only three iterations of our generation process, using one mask for each color channel. While this is one of the baseline we compare to, that has the advantages to build a parametric image from a rasterized input, we rather designed our network architecture to have each mask represent a relatively simple shape and use more layers.


%****************************
\section{Experiments}
%****************************

\subsection{Datasets and training details}

\paragraph{Datasets.}

Our models are trained on three datasets with different resolutions: CIFAR10~\cite{Krizhevsky2010cifar10} (60k images of 10 classes) at $32\times 32$, CelebA~\cite{celebA} (202k images of celebrity faces) and ImageNet~\cite{imagenet_cvpr09} (1.28M natural images of 1000 classes) at $128\times128$ in the reconstruction setting and $64\times 64$ images in the GAN setting. We keep 10k images for testing and train on the remaining ones.

% Mask Decomposition
\begin{figure}[ht]
    \centering
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{cccccc}
       Target & $M_1$  & $M_2$ & $M_4$ & $M_7$ & $M_{10}$ \\
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/imagenet_I.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/imagenet_m1.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/imagenet_m2.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/imagenet_m4.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/imagenet_m7.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/imagenet_m10.jpg}\\
        $I_0$  & $I_1$ & $I_2$ & $I_4$ & $I_7$ & $I_{10}$ \\
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/black_canvas.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/imagenet_i1.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/imagenet_i2.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/imagenet_i4.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/imagenet_i7.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/imagenet_i10.jpg}\\
       Target & $M_1$  & $M_2$ & $M_4$ & $M_7$ & $M_{10}$ \\
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/celeba_I.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/celeba_m1.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/celeba_m2.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/celeba_m4.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/celeba_m7.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/celeba_m10.jpg}\\
       $I_0$  & $I_1$ & $I_2$ & $I_4$ & $I_7$ & $I_{10}$ \\
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/black_canvas.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/celeba_i1.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/celeba_i2.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/celeba_i4.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/celeba_i7.jpg}&
       \includegraphics[width=.16\linewidth]{figures/mask_decomposition/celeba_i10.jpg}\\
    \end{tabular}
    \vspace{-2mm}
    \caption{Mask decomposition of images using 10 masks (full results in the supplementary).}
        \label{fig:mask_decomposition}
\end{figure}


% Applications figure

\setlength{\tabcolsep}{1pt}
\begin{figure*}[ht]
	\begin{subfigure}[t]{0.32\textwidth}
		\begin{tabular}{ccc}
	        Original & Mask & Edited \\
            \includegraphics[width=0.32\linewidth]{figures/applications/editing/1_original.jpg}&
            \includegraphics[width=0.32\linewidth]{figures/applications/editing/1_mask_face.jpg}&
            \includegraphics[width=0.32\linewidth]{figures/applications/editing/1_edited_luminosity.jpg}\\
            \includegraphics[width=0.32\linewidth]{figures/applications/editing/bird_original.jpg}&
            \includegraphics[width=0.32\linewidth]{figures/applications/editing/bird_mask_background.jpg}&
            \includegraphics[width=0.32\linewidth]{figures/applications/editing/bird_edited_original_grayed_background.jpg}
		\end{tabular}
		\caption{Editing the original image using extracted masks, by performing local modifications of luminosity (top), or color modification using a blending of masks (bottom).}
		\label{fig:grouped_applications:a}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.32\textwidth}
	
		\begin{tabular}{ccc}
		    Original & Rec. & Edited \\
            \includegraphics[width=0.32\linewidth]{figures/applications/editing_rec/nose_original.jpg}&
            \includegraphics[width=0.32\linewidth]{figures/applications/editing_rec/nose_reconstructed.jpg}&
            \includegraphics[width=0.32\linewidth]{figures/applications/editing_rec/nose_edited.jpg}\\
            \includegraphics[width=0.32\linewidth]{figures/applications/editing_rec/2_original.jpg} & % Billiard ball example
            \includegraphics[width=0.32\linewidth]{figures/applications/editing_rec/2_rec.jpg}&
            \includegraphics[width=0.32\linewidth]{figures/applications/editing_rec/edited_moving_white_ball.jpg}
          
		\end{tabular}
		\caption{Image editing using masks: Using chosen extracted mask(s) from image reconstruction, we apply   object removal (top) and color modifications with object removal (bottom).}
		\label{fig:grouped_applications:b}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.32\textwidth}
		\begin{tabular}{ccc}
		    Original & Bilinear & Ours \\
            \includegraphics[width=0.32\linewidth]{figures/applications/vectorization/7_original_1024.jpg}&
            \includegraphics[width=0.32\linewidth]{figures/applications/vectorization/7_bil_1024.jpg}&
            \includegraphics[width=0.32\linewidth]{figures/applications/vectorization/7_ours_1024.jpg} \\
            \includegraphics[width=0.32\linewidth]{figures/applications/vectorization/3_original_1024.jpg}&
            \includegraphics[width=0.32\linewidth]{figures/applications/vectorization/3_bil_1024.jpg}&
            \includegraphics[width=0.32\linewidth]{figures/applications/vectorization/3_ours_1024.jpg}
		\end{tabular}
		\caption{Image vectorization: Reconstruction results on MNIST images. Our model learns a vectorized mask representation of digits that can be generated at any resolution without interpolation artifacts.}
		\label{fig:grouped_applications:c}
	\end{subfigure}
     \caption{\textbf{Applications:} (a) and (b) demonstrate how the masks of our decomposition may be used for image editing. (c) illustrates vectorization example on MNIST digits.} 	\label{fig:grouped_applications}
\end{figure*}

\begin{figure}[!th]
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=0.333\linewidth]{figures/pix2pix/1_3.jpg}& \includegraphics[width=0.333\linewidth]{figures/pix2pix/1_1.jpg}&
\includegraphics[width=0.333\linewidth]{figures/pix2pix/1_2.jpg}\\
\includegraphics[width=0.333\linewidth]{figures/pix2pix/2_3.jpg}& \includegraphics[width=0.333\linewidth]{figures/pix2pix/2_1.jpg}&
\includegraphics[width=0.333\linewidth]{figures/pix2pix/2_2.jpg}\\
Original image & Our result & Our result enhanced \\  
\end{tabular}\\
\vspace{-5mm}

\end{center}
   \caption{Enhanced image reconstruction using image translation trained from our auto-encoded images domain toward real images one using Pix2Pix~\cite{Isola2016ImageToImage}.}
\label{fig:pix2pix_fish}
\end{figure}


\paragraph{Training details.}
% on 1 GPU Volta, 10 masks, 128x128 --> slow : 0.67s / batch of 16 --> 33min/1000iterations almost 8 days of training
% on 2 GPUs Volta, 10 masks, 128x128 -->     : 0/39s / batch of 32 --> 20 min/1000iterations
The parameters of $G$ are optimized using Adam~\cite{kingma2014adam} with a learning rate of $10^{-3}$ and $\beta_1=0.9$ for reconstruction setting and a learning rate of $2.10^{-4}$ and $\beta_1=0$ in the GAN setting. The batch size is set to 32. 
The training time depends on the number of masks, and the image size. 
For $128\times128$ pixel images, 1000 training iterations take 30min when using 10 masks on two Tesla V100 GPUs. Our training typically converges within 600k iterations, but reaches about $95\%$ of its final PSNR accuracy at 100k iterations.
Unless otherwise noted, we employ an $\mathcal{L}_1$ loss for image reconstruction to ease comparison to baselines. By default, we use a mask input parameter of 10 ($K = |\mathbf{p}| = 10$) and vary the number of masks $T$ between 5 and 20.

\paragraph{Networks architectures.} 
The mask generator $f$ consists of an MLP with three hidden layers of 128 units with group normalization \cite{wu2018group}, tanh non-linearities, and an additional sigmoid after the last layer. It takes as input a parameter vector $\mathbf{p}$ and coordinates $(x,y)$, and outputs a value between 0 and 1. The parameter $\mathbf{p}$ is predicted by a ResNet-18 network, taking as input either the current image in the GAN setting or the concatenation of the target and the current image in the auto-encoder setting.


%---------------------------------
\subsection{Qualitative analysis and applications}
%---------------------------------

After displaying examples of image decomposition obtained with our approach, we demonstrate in this section how it may serve different purposes such as image editing, vectorization and enhancement.


\paragraph{Layered decomposition}

Fig.~\ref{fig:mask_decomposition} shows two image reconstruction results with their corresponding masks. The learned masks capture various levels of smoothness and sharpness of the images. The final result is not a perfect reconstruction of the original image, but rather a simplified version with a compact and editable representation.

\paragraph{Image editing.}{
Image editing from raw pixels can be time consuming. Using our generated reconstruction masks, it is possible to alter the original image by blending for example luminosity or color modifications on the region specified by a mask, as demonstrated in Fig.~\ref{fig:grouped_applications:a}. This avoids going through the tedious process of defining a blending mask manually.
In Fig.~\ref{fig:grouped_applications:b}, the goal is to produce as output a vectorized image, which is essential in many design scenarios. Our pipeline outputs an initial suggestion, which can then be edited by modifying the masks, removing them, or changing their color.
}

\paragraph{Image vectorization.}

We demonstrate in Fig.~\ref{fig:grouped_applications:c} the potential of our approach at recovering a continuous vectorized image from a low resolution bitmap. We learned our network on the MNIST dataset in $32\times32$, but generate the output at $1024\times1024$ resolution. We display the original images, the upsampled ones by bilinear interpolation, and our result directly generated in the target resolution, $1024\times1024$. Compared to bilinear interpolation, the image we generate presents less artifacts.



\paragraph{Image enhancement using image translation.}

Our typical output images appear very smooth, with little details. Our approach can however easily be associated to approaches that hallucinate details. Fig.~\ref{fig:pix2pix_fish} displays such a result, where we trained an image-to-image translation network~\cite{Isola2016ImageToImage} to transform the regular images generated by our auto-encoder into the original images. The output images only depend on our compact representation, but show fine details.

\begin{table}[t]
\centering
\caption{Reconstruction accuracy in function of the total number of masks and of number of parameters per mask.}
\label{table:rec_losses_masks_params}
\begin{tabular}{ccccc}
\hline
$T$ masks  & $K$ param.  & CelebA & ImageNet & CIFAR10 \\
\hline
5 &  10   &  20.59 &  17.77 & 20.04\\
10 & 5   &  20.93 & 17.85  & 21.10 \\
10 &  10  &  22.03 &  18.68 & 22.36\\
10 & 20  &  23.54 & 19.55 & 23.41\\
20 &  10  & {\bf 23.80}  &  {\bf 19.83} & {\bf 24.15}\\
\hline
\end{tabular}
\end{table}


%---------------------------------
\subsection{Comparisons for Image reconstruction}
%---------------------------------

 In this section, we provide comparisons between different versions of our model and baselines. To ease the comparison, we only used an $\mathcal{L}_1$ reconstruction loss (i.e. $\lambda=0$).  

\paragraph{Number of iterations and parameter size.}
As expected the reconstruction accuracy in term of Peak Signal to Noise Ratio (PSNR) improves with the size of the parameter vector $K$ and the  number of masks $T$ (Table~\ref{table:rec_losses_masks_params}).
This is coherent with the visual quality of the images that we compare in the supplementary material.

\paragraph{Comparison with auto-encoder baselines}

We compare our results to different image reconstruction baselines. Each baseline consists of an auto-encoder where the encoder is a residual network (ResNet-18, same as our model) producing a latent code $z$. The different decoders are listed below, and architecture details are provided in the supplementary material.
\begin{itemize}
    \item {MLP baseline:} using as decoder an MLP with a $3\times W \times H$ output for an RGB image of size $W\times H$.
    \item {ConvT baseline:} using transposed convolutions in a DCGAN-like architecture~\cite{radford2015unsupervised}.
    \item {ResNet baseline:} using convolutions and residual connections with upsampling similarly to the architecture used in \cite{miyato2018spectral} and \cite{kurach2018gan}.
   \item {MLP-$xy$ baseline:} the decoder computes the pixel-wise color of the resulting image $\hat{I}$ as a function $f$ of the coordinates $(x,y)$ of a grid and the latent code $z$ as $\hat{I}(x,y) = f(x,y,z)$, where $f$ is an MLP similar to the one used in our mask generation module, differently having a 3-channel output instead of a 1-channel as for the mask. 
\end{itemize}


\begin{table}[!h]
\centering
\caption{Comparison of our model with baselines in terms of PSNR on each dataset (higher is better). Note that the MLP-$xy$ baseline is the only alternative to our method that offers a vectorized image result.}
\label{table:rec_losses_our_vs_baselines}
\begin{tabular}{lccc}

\hline
Setup / Dataset   & CelebA & ImageNet & CIFAR10 \\
\hline
$|z|$ = 260 or 20 masks \\
\hline
% 20 strokes models
MLP  & 20.69 &  17.81 & 19.97 \\
MLP-$xy$  & 19.74 & 17.16  & 19.09 \\
ConvT  & 24.08 &  19.78 & 21.99\\
ResNet  & \textbf{24.41} &  \textbf{19.88} & 19.47\\
Ours ($T$=20, $K$=10)  & 23.80 & 19.83  & \textbf{24.15} \\
\hline
$|z|$ = 130 or 10 masks  \\
\hline
% 10 strokes models
MLP  & 21.00 &  17.92 &  19.91 \\
MLP-$xy$  & 19.45 & 17.47  & 19.64 \\
ConvT  & 22.12 &  18.48 & 21.04\\
ResNet  & 22.35 &  18.52 & 18.88\\
Ours ($T$=10, $K$=10) & \textbf{23.77} & \textbf{18.68} & \textbf{22.36} \\
\hline
\end{tabular}
\end{table}


\begin{figure}[H]
\begin{center}
\begin{tabularx}{\linewidth}{@{} *5{>{\centering\arraybackslash}X}@{}}

 Target & MLP & MLP-$xy$ & ResNet & Ours \\ 
\end{tabularx}\\

\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=1\linewidth]{figures/baselines/celeba_baselines_less.jpg}
\caption{Comparison with baselines on CelebA.}
\label{fig:baselineComparison_20masks_celeba}
\end{subfigure}\\
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics[width=1\linewidth]{figures/baselines/baselines_imagenet_rec.jpg}
\caption{Comparison with baselines on ImageNet.}
\label{fig:baselineComparison_20masks_imagenet}
\end{subfigure}\\
\caption{Comparison with baselines for models with latent code size of 260 (20 masks with 10 parameters).}
\end{center}
\end{figure}

\setlength{\tabcolsep}{1pt}
\begin{figure}[b]
\begin{center}
\begin{tabular}{cccc}
Real & PNG Qu. & SPIRAL & Ours \\
\includegraphics[width=.245\linewidth]{figures/spiral/0r.jpg}&
\includegraphics[width=.245\linewidth]{figures/spiral/0r20-fs8.jpg}&
\includegraphics[width=.245\linewidth]{figures/spiral/0s.jpg}&
\includegraphics[width=.245\linewidth]{figures/spiral/0o_256_216.jpg}\\
\includegraphics[width=.245\linewidth]{figures/spiral/14r.jpg}&
\includegraphics[width=.245\linewidth]{figures/spiral/14r20-fs8.jpg}&
\includegraphics[width=.245\linewidth]{figures/spiral/14s.jpg}&
\includegraphics[width=.245\linewidth]{figures/spiral/14o_256_216.jpg}\\
\includegraphics[width=.245\linewidth]{figures/spiral/4r.jpg}&
\includegraphics[width=.245\linewidth]{figures/spiral/4r20-fs8.jpg}&
\includegraphics[width=.245\linewidth]{figures/spiral/4s.jpg}&
\includegraphics[width=.245\linewidth]{figures/spiral/4o_256_216.jpg}\\
\includegraphics[width=.245\linewidth]{figures/spiral/5r.jpg}&
\includegraphics[width=.245\linewidth]{figures/spiral/5r20-fs8.jpg}&
\includegraphics[width=.245\linewidth]{figures/spiral/5s.jpg}&
\includegraphics[width=.245\linewidth]{figures/spiral/5o_256_216.jpg}\\
\end{tabular}
\end{center}
\vspace{-4mm}
 \caption{Comparison with SPIRAL ~\cite{ganin2018synthesizing} and PNG quantization on CelebA. The PNG quantization \cite{pngquant} was obtained using 20 color levels.}
\label{fig:compSpiral}
\end{figure}


Table~\ref{table:rec_losses_our_vs_baselines} shows a quantitative comparison on the three datasets of results obtained by our model with results from the baselines described above. We consider two different latent code sizes $|z|$ of 130 and 260. These correspond to the number of parameters for our generation with $K=10$ parameters for each mask, three parameters for each of the colors, and respectively $T=10$ and 20 iterations.  


For a fixed number of parameters, our model obtains a reconstruction quality comparable with the baselines while providing other advantages. In particular, the only other parametric MLP-$xy$ baseline is by far outperformed by our iterative approach. 
Fig.~\ref{fig:baselineComparison_20masks_celeba} and ~\ref{fig:baselineComparison_20masks_imagenet} show the visual comparison of the models trained with latent code size of 260 (20 masks) on CelebA and ImageNet. As hinted by the quantitative results, our model and the ResNet baseline provide clearly better reconstruction. Interestingly, our method is the only one able to recover the uncommon detail of the plaster in the second face example, and provides finer details for the bicycle and bird examples. 


\paragraph{Comparison with SPIRAL and quantization.}

\begin{figure*}[htb]
\begin{center}
\includegraphics[width=\linewidth]{figures/recGAN/recGAN_4rows_arrows.jpg}
\end{center}
\vspace{-4mm}
   \caption{Influence of the adversarial loss in the reconstruction results. From left to right: Target images to reconstruct, reconstructions using 10 masks, and reconstructions using 20 masks, using an increasing weight $\lambda$ of the adversarial loss.}
\label{fig:advRec_IN_celeba}
\end{figure*}

We finally compare our model with SPIRAL~\cite{ganin2018synthesizing} on a few images from CelebA dataset published in \cite{ganin2018synthesizing}. SPIRAL is closely related to our approach in the sense that it is an iterative deep approach for reconstructing an image and extracting its structure only using a few color strokes and that it can produce vector results. We report SPIRAL results using 20-step episodes. In each episode, a tuple of 8 discrete decisions is estimated, resulting in a total of 160 parameters for reconstruction. Our results shown in Fig.~\ref{fig:compSpiral} are obtained with a model trained on $64\times64$ images using 20 masks and 10 parameters. We do not reproduce the stroke gesture for drawing each mask as it is the case in SPIRAL, but our results reconstruct the original images better. We also compare our results with color quantization \cite{pngquant} using 20 color levels. Compared to our smooth images, this baseline leads to quantization artifacts. 



%---------------------------------
\subsection{Effect of adversarial loss}
%---------------------------------

In this section, we focus on the effect of the adversarial training. We first compare results on image reconstruction with increasing importance of the adversarial term and then discuss results obtained in the generative adversarial setting. 

\paragraph{Adversarial reconstruction.}

Fig.~\ref{fig:advRec_IN_celeba} shows the results of training our model using an adversarial loss in addition to the reconstruction, using 10 or 20 masks. As expected, adding an adversarial costs to our reconstruction first leads to increased sharpness in the results, then hallucination of higher frequency details. However, the higher frequencies we hallucinate are qualitatively very different from the ones obtained with other approaches.

\paragraph{GANs.}

\begin{figure*}[h!]
	\centering
	\begin{subfigure}{\textwidth}
		\includegraphics[width=\textwidth]{figures/GAN/GAN_gen_imagenet.jpg}
		\caption{ImageNet generations trained on $64\times64$ sampled at $1024\times1024$}
	\end{subfigure}\\
	\begin{subfigure}{0.48\textwidth}
		\includegraphics[width=\textwidth]{figures/GAN/GAN_gen_celeba_2.jpg}
		\caption{CelebA generations trained on $64\times64$ sampled at $256\times256$}
	\end{subfigure}\hfill
	\begin{subfigure}{0.48\textwidth}
		\includegraphics[width=\textwidth]{figures/GAN/GAN_gen_cifar10.jpg}
		\caption{CIFAR10 generations trained on $32\times32$ sampled at $256\times256$}
	\end{subfigure}
	\caption{Randomly sampled high resolution generations from our GAN model.}
	\label{fig:GAN_generations}
\end{figure*}

The same effect is even more visible when only the adversarial cost is used. Fig. \ref{fig:GAN_generations} displays random samples of images from networks trained with our generative adversarial setting on the ImageNet, CelebA, and CIFAR10 datasets. The results appear qualitatively different on the three datasets. Note that while our model was trained on $32\times32$ images for CIFAR10 and $64\times 64$ images for ImageNet and CelebA images, we generated the output images at a $256\times256$ resolution. Using ImageNet, we generate layered and colored abstract shapes that appear similar to some abstract paintings. In contrast to common generative models, our model is able to show a notion of transparency and superposition by design in the generations. On CelebA and CIFAR10, where the image distribution presents lower or intermediate diversity, the generations seem to be simplified version of real images. 



%*********************************************************************************
\section{Conclusion}
%*********************************************************************************

We have presented a new paradigm to reconstruct images using a succession of single-color parametric layers. This decomposition, learned without supervision, enable image enhancement and editing from the masks generated for each layer.

Moreover, our experiments demonstrate that our image reconstruction results are competitive with convolution-based auto-encoders.
Finally, we show GAN generation results with a variety of original appearances, from simplified sharp faces to high frequency abstract pictures.
We think that because of its differences and its advantages for user interaction, our method can inspire new approaches.

\section*{Acknowlegments}

We would like to thank Ludovic Denoyer, Maxime Oquab, Alyosha Efros, and Yann LeCun for their helpful comments. This work was partly supported by ANR project EnHerit ANR-17-CE23-0008 and gifts from Adobe to \'Ecole des Ponts.


{\small
\bibliographystyle{plain}
\bibliography{biblio}
}

\clearpage

\twocolumn[{%
 \centering
\Large Supplementary material\\[1.5em]
}]

%\section{Appendix: supplementary material}

We first provide details of architecture choices for our model and the considered baselines. Then we display visual results of different experiments. 

\section{Architecture Details}\subsection{Feature extractor}
We use the same feature extractor for our model and for the baselines. In all our experiments, Resnet-18 is used without pre-training, we only modify the first layer depending on the number of channels of the input (6 for reconstruction, 3 for GAN setting). The output of the feature extractor $z$ is of size 512. The color is directly regressed from the extracted features using a linear layer followed by a sigmoid, that maps the $512$ parameters to a RGB color.

\subsection{Mask Generator Architecture}\begin{table}[ht]
	\caption{\label{tab:maskgen_arch} Architecture of our mask generator, applied pixelwise.}
   	\centering
   	\begin{tabular}{c}
		\toprule
	 	Input of size $K+2$,\\
	 	$K$ the size of the mask parameters $\textbf{p}_t$ \\
	    $+2$ for the concatenated coordinates $(x,y)$ \\
       	\midrule
        Linear, output size=128 \\
        GroupNorm G=32, Tanh \\
        \midrule
        Linear, output size=128 \\
        GroupNorm G=32, Tanh \\
		\midrule
		Linear, output size=128 \\
        GroupNorm G=32, Tanh \\
        \midrule
        Linear, output size=1 \\
        GroupNorm G=32, Sigmoid \\
        \midrule
        Output of size 1, $M_t(x,y)$ \\
		\bottomrule
	\end{tabular}
\end{table}

Our mask generator, a Multi-Layer Perceptron (MLP) using Tanh as intermediate activations, is detailed in Table~\ref{tab:maskgen_arch}. We found that Tanh helps produce smooth masks in opposition to ReLU (Rectified Linear Units). The mask generator takes as input the mask parameters of size $K$ concatenated with 2D pixel coordinates sampled from a grid of the desired resolution. We use group normalization with a number of groups: G=32.

\subsection{Baselines}

Given $3\times W \times H$ images, we train the baselines to reconstruct images using an $\ell_1$ loss.

\paragraph{MLP Baseline.}
The MLP baseline decoder generates an image $3\times W \times H$ from the latent code, as detailed in Table~\ref{tab:mlp}. 

\begin{table}[ht]
	\caption{\label{tab:mlp} Architecture of the MLP baseline.}
   	\centering
   	\begin{tabular}{c}
		\toprule
	 	Input $z$ of size $|z|$ \\
       	\midrule
        Linear, output size=512 \\
        BatchNorm1d, ReLU \\
        \midrule
        Linear, output size=512 \\
        BatchNorm1d, ReLU \\
		\midrule
		Linear, output size=512 \\
        BatchNorm1d, ReLU \\
        \midrule
        Linear, output size=$3\times W \times H$ \\
        Sigmoid \\
		\midrule
        Output of size  $(3,W,H)$ \\
		\bottomrule
	\end{tabular}
\end{table}\paragraph{MLP-RGB Baseline.}
The architecture is the same as our mask generator, except that we output 3 channels for RGB colors of each pixel instead of 1 channel for the mask. We also replace the hidden size by 512 so that the latent code remains the bottleneck of the reconstructed image.

\paragraph{ConvT Baseline.}
We adapt the ConvT baseline architecture from~\cite{radford2015unsupervised}, as shown in Table~\ref{tab:convT}. It consists of a sequence of transposed convolution layers to upsample the resulting image from the input latent code. The parameters consist in a kernel size $k$, stride $s$, and padding $p$.

\begin{table}[ht]
	\caption{\label{tab:convT} Architecture of the ConvT baseline, where (k: kernel size, s: stride, p: padding) of the transposed convolution layers.}
   	\centering
   	\begin{tabular}{c}
		\toprule
	 	Input $z$ of size $|z|$ \\
       	\midrule
        Linear, output size=$512\times \frac{W}{16} \times \frac{H}{16}$ \\
        BatchNorm2d, ReLU \\
        \midrule
        ConvTranspose, channels=256, $k=4, s=2, p=1$ \\
        BatchNorm2d, ReLU \\
		\midrule
		ConvTranspose, channels=128, $k=4, s=2, p=1$ \\
        BatchNorm2d, ReLU \\
        \midrule
        ConvTranspose, channels=64, $k=4, s=2, p=1$ \\
        BatchNorm2d, ReLU \\
		\midrule
		ConvTranspose, channels=3, $k=4, s=2, p=1$ \\
        BatchNorm2d, Sigmoid \\
        \midrule
        Output of size  $(3,W,H)$ \\
		\bottomrule
	\end{tabular}
\end{table}\paragraph{Resnet Baseline.}
The Resnet architecture we use is adapted from~\cite{miyato2018spectral}, and is shown in Table~\ref{tab:resnet}, where each Resnet Block is detailed in Table \ref{resnet_block}.
\begin{table}[ht]
	\caption{\label{tab:resnet} Architecture of the Resnet baseline, where (k: kernel size, s: stride, p: padding) of the convolution layers.}
   	\centering
   	\begin{tabular}{c}
		\toprule
	 	Input $z$ of size $|z|$ \\
       	\midrule
        Linear, output size=$512 \times \frac{W}{32} \times \frac{H}{32} $ \\
        BatchNorm2d, ReLU \\
        \midrule
        Resnet Block (see Table~\ref{tab:resblock}), channels=512 \\
		\midrule
		Resnet Block, channels=256 \\
        \midrule
        Resnet Block, channels=256 \\
        \midrule
        Resnet Block, channels=128 \\
		\midrule
		Resnet Block, channels=64 \\
        \midrule
        Conv2d channels=3, k=3, s=1, p=1 \\
        BatchNorm2d, Sigmoid \\
        \midrule
        Output of size  $(3,W,H)$ \\
		\bottomrule
	\end{tabular}
\end{table}\begin{table}[ht]
	\caption{\label{tab:resblock} Architecture of the Resnet Block with input channels $c_i$ and output channels $c_o$. (k: kernel size and p: padding of the convolutional layers).}
   	\centering
   	\begin{tabular}{c}
		\toprule
	 	Input $x$ of size ($c_i$ ,$w$,$h$) \\
	\end{tabular}
   	\begin{tabular}{c|c}
   	    \midrule
   	    \textbf{Residual layers} &  \textbf{Shortcut}\\
        \midrule
            Upsample  $\times 2$ & Conv2d, channels=$c_o$, $k=1$ \\
            Conv2d, channels=$c_o$ , $k=3, p=1$ & Upsample  $\times 2$\\
            BatchNorm2d, ReLU & \\
            Conv2d,  channels=$c_o$ , $k=3, p=1$ & \\
            BatchNorm2d, ReLU & \\
        \midrule
        $r$ & $s$ \\
        \midrule
	\end{tabular}
	\begin{tabular}{c}
		Output = $r+s$ of size ($c_o$, $2w$, $2h$) \\
		\bottomrule
	\end{tabular}
	\label{resnet_block}
\end{table}%--------------------------------------------------------\section{Influence of the number of masks and parameters per mask}%--------------------------------------------------------

In this section, we present a qualitative comparison of reconstructions with our model using different number of masks and parameters per mask. Results on CIFAR10 are shown in Figure~\ref{fig:compNMasks_NParams_CIFAR} and on CelebA and ImageNet in Figure~\ref{fig:compNMasks_NParams}. We observe that for similar total latent code size, reconstruction using more masks are typically qualitatively better.

\section{Mask decomposition and transfer}

Figure~\ref{fig:mask_decomposition} shows the masks obtained at each iterations of reconstruction on ImageNet and CelebA samples. We provide additional samples in Figure~\ref{fig:oCanvas_imagenet_celeba}.
Furthermore, we illustrate the transfer capability of our approach by reconstructing CIFAR10 images using alternatively a model trained on CIFAR10 and a model trained on ImageNet. We observe qualitatively that the reconstructions using the ImageNet model are more accurate.

\section{Mask interpolation}

In Figure~\ref{fig:mask_interp_random_directions}, we show masks obtained using our learned Mask MLP. Each row corresponds to jittering a given mask parameter $\textbf{p}$ in a random direction $\textbf{q}$, 
$$\textbf{p}_k = \textbf{p} + k*\textbf{q}$$
where $\lVert\textbf{q}\rVert_2 = 1$ and $k$ takes values in 10 evenly spaced samples from $[-2,2]$.

\clearpage\setlength{\tabcolsep}{1pt}\begin{figure*}[htb]
\begin{center}
\begin{tabular}{cccccc}
%%%% CIFAR10
Targets & $T=5$, $K=10$ & $T=10$, $K=5$ & $T=10$, $K=10$ & $T=10$, $K=20$ & $T=20$, $K=10$ \\
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/1/000.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/1/003.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/1/004.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/1/002.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/1/005.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/1/001.jpg}\\

\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/2/000.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/2/003.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/2/004.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/2/002.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/2/005.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/2/001.jpg}\\

\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/3/000.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/3/003.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/3/004.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/3/002.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/3/005.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/cifar10/3/001.jpg}\\

\end{tabular}
\end{center}
\vspace{-4mm}
 \caption{Comparing the influence of number of masks $T$ and number of parameters $K$ on CIFAR10.}
\label{fig:compNMasks_NParams_CIFAR}
\end{figure*}\setlength{\tabcolsep}{1pt}\begin{figure*}[t]
\begin{center}
\begin{tabular}{ccccccccccc}
% Coffee mug
Target & $M_1$ & $M_2$ & $M_3$ & $M_4$& $M_5$& $M_6$& $M_7$& $M_8$&$M_9$ & $M_{10}$ \\
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/target.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-13.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-15.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-17.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-19.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-20.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-21.jpg}\\
$I_0$ & $I_1$ & $I_2$ & $I_3$ & $I_4$&$I_5$ & $I_6$& $I_7$& $I_8$& $I_9$& $I_{10}$ \\
\includegraphics[width=.09\linewidth]{figures/suppmat/black_canvas.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-3.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-5.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-7.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-9.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/cup_coffee/cup_coffee-11.jpg}\\
% Celeba man-bg with details
Target & $M_1$ & $M_2$ & $M_3$ & $M_4$& $M_5$& $M_6$& $M_7$& $M_8$&$M_9$ & $M_{10}$ \\
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/target.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-13.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-15.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-17.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-19.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-20.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-21.jpg}\\
$I_0$ & $I_1$ & $I_2$ & $I_3$ & $I_4$&$I_5$ & $I_6$& $I_7$& $I_8$& $I_9$& $I_{10}$ \\
\includegraphics[width=.09\linewidth]{figures/suppmat/black_canvas.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-3.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-5.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-7.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-9.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/mask_decomp/man_bg/man_bg-11.jpg}\\

\end{tabular}
\end{center}
\vspace{-4mm}
 \caption{Mask decomposition using 10 masks on ImageNet and CelebA samples.}
\label{fig:mask_decomposition}
\end{figure*}\setlength{\tabcolsep}{1pt}\begin{figure*}[t]
\begin{center}
\begin{tabular}{cccccc}

%%%% CelebA
Targets & $T=5, K=10$ & $T=10, K=5$ & $T=10, K=10$  & $T=10, K=20$ & $T=20, K=10$ \\
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/1/000.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/1/001.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/1/004.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/1/002.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/1/005.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/1/003.jpg}\\

\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/2/000.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/2/001.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/2/004.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/2/002.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/2/005.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/2/003.jpg}\\


\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/4/000.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/4/001.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/4/004.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/4/002.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/4/005.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/celeba/4/003.jpg}\\

%%% ImageNet
Targets & $T=5, K=10$ & $T=10, K=5$ & $T=10, K=10$ & $T=10, K=20$ & $T=20, K=10$  \\
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/3/000.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/3/001.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/3/004.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/3/002.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/3/005.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/3/003.jpg}\\

\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/4/000.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/4/001.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/4/004.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/4/002.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/4/005.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/4/003.jpg}\\

\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/2/000.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/2/001.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/2/004.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/2/002.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/2/005.jpg}&
\includegraphics[width=.164\linewidth]{figures/suppmat/nMasks_nParam/imagenet/2/003.jpg}\\

\end{tabular}
\end{center}
\vspace{-4mm}
 \caption{Comparing the influence of number of masks $T$ and number of parameters $K$.}
\label{fig:compNMasks_NParams}
\end{figure*}\setlength{\tabcolsep}{1pt}\begin{figure*}[t]
\begin{center}
\begin{tabular}{ccccccccccc}
% penguin
$I_1$ & $I_3$ & $I_5$ & $I_7$& $I_9$& $I_{11}$& $I_{13}$& $I_{15}$&$I_{17}$ & $I_{19}$ & Target \\
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/1/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/1/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/1/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/1/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/1/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/1/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/1/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/1/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/1/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/1/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/1/imagenet_20_10_029.jpg}\\

\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/2/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/2/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/2/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/2/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/2/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/2/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/2/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/2/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/2/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/2/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/2/imagenet_20_10_038.jpg}\\

\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/3/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/3/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/3/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/3/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/3/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/3/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/3/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/3/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/3/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/3/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/3/imagenet_20_10_037.jpg}\\

\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/4/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/4/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/4/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/4/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/4/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/4/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/4/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/4/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/4/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/4/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/4/imagenet_20_10_004.jpg}\\

\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/5/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/5/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/5/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/5/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/5/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/5/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/5/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/5/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/5/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/5/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/imagenet/5/imagenet_20_10_015.jpg}\\

\\
%% on Celeba
$I_1$ & $I_3$ & $I_5$ & $I_7$& $I_9$& $I_{11}$& $I_{13}$& $I_{15}$&$I_{17}$ & $I_{19}$ & Target \\
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/1/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/1/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/1/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/1/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/1/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/1/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/1/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/1/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/1/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/1/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/1/celeba_20_10_016.jpg}\\

\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/2/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/2/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/2/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/2/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/2/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/2/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/2/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/2/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/2/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/2/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/2/celeba_20_10_010.jpg}\\

\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/3/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/3/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/3/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/3/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/3/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/3/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/3/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/3/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/3/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/3/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/3/celeba_20_10_026.jpg}\\

\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/4/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/4/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/4/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/4/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/4/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/4/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/4/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/4/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/4/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/4/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/4/celeba_20_10_019.jpg}\\

\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/5/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/5/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/5/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/5/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/5/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/5/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/5/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/5/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/5/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/5/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/celeba/5/celeba_20_10_027.jpg}\\

\end{tabular}
\end{center}
\vspace{-4mm}
 \caption{Consecutive output of our model on ImageNet samples (top 5 rows) and CelebA (bottom 5 rows).}
\label{fig:oCanvas_imagenet_celeba}
\end{figure*}\setlength{\tabcolsep}{1pt}\begin{figure*}[t]
\begin{center}
\begin{tabular}{ccccccccccc}
% cifar
$I_1$ & $I_3$ & $I_5$ & $I_7$& $I_9$& $I_{11}$& $I_{13}$& $I_{15}$&$I_{17}$ & $I_{19}$ & Target \\
CIFAR10 & & & & & & & & & &\\
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/1/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/1/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/1/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/1/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/1/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/1/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/1/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/1/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/1/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/1/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet_20_10_007.jpg}\\
%imagenet
ImageNet & & & & & & & & & &\\
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/1/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/1/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/1/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/1/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/1/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/1/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/1/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/1/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/1/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/1/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet_20_10_007.jpg}\\
% cifar
CIFAR10 & & & & & & & & & &\\
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/2/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/2/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/2/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/2/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/2/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/2/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/2/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/2/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/2/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/2/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet_20_10_014.jpg}\\
%imagenet
ImageNet & & & & & & & & & &\\
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/2/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/2/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/2/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/2/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/2/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/2/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/2/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/2/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/2/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/2/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet_20_10_014.jpg}\\

% cifar
CIFAR10 & & & & & & & & & &\\
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/3/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/3/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/3/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/3/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/3/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/3/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/3/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/3/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/3/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/cifar10/3/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet_20_10_023.jpg}\\
%imagenet
ImageNet & & & & & & & & & &\\
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/3/oCanvas_0.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/3/oCanvas_2.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/3/oCanvas_4.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/3/oCanvas_6.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/3/oCanvas_8.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/3/oCanvas_10.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/3/oCanvas_12.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/3/oCanvas_14.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/3/oCanvas_16.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet/3/oCanvas_18.jpg}&
\includegraphics[width=.09\linewidth]{figures/suppmat/oCanvas/cifar10/imagenet_20_10_023.jpg}\\

\end{tabular}
\end{center}
\vspace{-4mm}
 \caption{Comparing reconstructions on CIFAR10 dataset using model trained on CIFAR10 (top row) and another trained on ImageNet (bottom row) of each sample using 20 masks.}
\label{fig:oCanvas_cifar10_transfer}
\end{figure*}\setlength{\tabcolsep}{1pt}\begin{figure*}[t]
\begin{center}
\begin{tabular}{cccccccccc}
%%%% interpolation
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/1/000.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/1/001.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/1/002.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/1/003.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/1/004.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/1/005.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/1/006.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/1/007.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/1/008.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/1/009.jpg}\\

\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/2/000.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/2/001.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/2/002.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/2/003.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/2/004.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/2/005.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/2/006.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/2/007.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/2/008.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/2/009.jpg}\\

\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/3/000.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/3/001.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/3/002.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/3/003.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/3/004.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/3/005.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/3/006.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/3/007.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/3/008.jpg}&
\includegraphics[width=.1\linewidth]{figures/suppmat/interpolation/3/009.jpg}\\

\end{tabular}
\end{center}
\vspace{-4mm}
 \caption{Jittering masks parameters in random directions: Each row corresponds to jittering along a sampled direction. The mask generator is trained on ImageNet using 20 masks.}
\label{fig:mask_interp_random_directions}
\end{figure*}

\end{document}



