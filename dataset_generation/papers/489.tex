
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt, conference, compsocconf]{IEEEtran}
% Add the compsocconf option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{balance}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{color, soul}
\usepackage{url}
\usepackage{textcomp}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage[caption=false]{subfig}
\usepackage{wrapfig}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}



% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\IEEEoverridecommandlockouts
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Computational Histological Staining and Destaining of Prostate Core Biopsy RGB Images with Generative Adversarial Neural Networks}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

\author{\IEEEauthorblockN{Aman Rana$^{1}$, Gregory Yaunery$^{1}$, Alarice Lowe$^{2}$, Pratik Shah$^{1\dagger}$ \thanks{$^{\dagger}$Corresponding author: \textit{pratiks@media.mit.edu}}} \\
\IEEEauthorblockA{$^{1}$MIT Media Lab, Massachusetts Institute of Technology, Cambridge, MA, USA \\ Email: \{arana,gyauney,pratiks\}@media.mit.edu}
\IEEEauthorblockA{$^{2}$Brigham and Women's Hospital, Harvard Medical School, Boston, MA, USA \\ Email: alowe@bwh.harvard.edu}
\thanks{Published at 2018 17th IEEE International Conference on Machine Learning and Applications}
\thanks{Â© 2018 IEEE.  Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.}
\thanks{DOI: 10.1109/ICMLA.2018.00133}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{
%    \IEEEauthorblockN{Aman Rana$^{1}$} \and
%    \IEEEauthorblockN{Gregory Yauney$^{1}$} \and
%    \IEEEauthorblockN{Alarice Lowe$^{2}$} \and
%    \IEEEauthorblockN{Pratik Shah$^{1}$}
%
%    \IEEEauthorblockA{
%    \begin{tabular}
%        $^{2}$Massachusetts Institute of Technology,
%        Cambridge, MA, USA\\
%        \{arana,gyauney,pratiks\}@media.mit.edu
%    } \and
%    \IEEEauthorblockA{$^{2}$Brigham and Women's Hospital\\
%        Harvard Medical School,
%        Boston, MA, USA\\
%        alowe@bwh.harvard.edu
%    }
%}

% make the title area
\maketitle


\begin{abstract}
Histopathology tissue samples are widely available in two states: paraffin-embedded unstained and non-paraffin-embedded stained whole slide RGB images (WSRI). Hematoxylin and eosin stain (H\&E) is one of the principal stains in histology but suffers from several shortcomings related to tissue preparation, staining protocols, slowness and human error. We report two novel approaches for training machine learning models for the computational H\&E staining and destaining of prostate core biopsy RGB images. The staining model uses a conditional generative adversarial network that learns hierarchical non-linear mappings between whole slide RGB image (WSRI) pairs of prostate core biopsy before and after H\&E staining. The trained staining model can then generate computationally H\&E-stained prostate core WSRIs using previously unseen non-stained biopsy images as input. The destaining model, by learning mappings between an H\&E stained WSRI and a non-stained WSRI of the same biopsy, can computationally destain previously unseen H\&E-stained images. Structural and anatomical details of prostate tissue and colors, shapes, geometries, locations of nuclei, stroma, vessels, glands and other cellular components were generated by both models with structural similarity indices of ~0.68 (staining) and ~0.84 (destaining). The proposed staining and destaining models can engender computational H\&E staining and destaining of WSRI biopsies without additional equipment and devices.
\end{abstract}

\begin{IEEEkeywords}
digital histopathology, computational staining, deep learning, H\&E staining, GAN, prostate core biopsy
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
Histopathology involves the visual examination of the structure and morphology of a stained tissue section under a microscope by a pathologist for diagnosis of various abnormalities. A histopathological analysis following H\&E staining is considered the gold standard for the diagnosis for the majority of cancer types in liver, prostate, lung, kidney and other organs, and a variety of other diseases in humans and model systems used for biomedical research \cite{rubins_pathology}. H\&E stain consists of two components - hematoxylin dye that selectively stains the nuclei dark blue and eosin dye that stains the cytoplasm and stroma various shades of pink and the red blood cells dark red to facilitate vivid visualization and discernment of abnormalities in biopsy \cite{staining_protocol}. H\&E staining of tissue biopsies and subsequent visual examination by pathologists present several challenges such as variability and inconsistencies introduced by tissue preparation and staining protocols, human errors and also requires significant processing time and costs \cite{sources_of_variability}.  Whole slide imaging (WSI) is a method to capture super high-resolution RGB images of stained pathology slides up to 40x resolutions; provides a way to normalize stain variation and can provide valuable diagnostic value in digital format \cite{farahani2015whole,cho2017neural,zanjani2018stain,bug2017context,macenko2009method,mukhopadhyay2018whole}.

Previous reports describe approaches for recovering stained images of tissue biopsy using hyperspectral and multispectral imaging systems and multichannel image segmentation methods. Bautista \textit{et al.} compare linear and non-linear mappings of the spectral transmittance data between nonstained and H\&E-stained multispectral images resulting in computationally stained multi-spectral images that are then converted to RGB format \cite{bautista}. Bayramo\~glu \textit{et al.} use hyperspectral transmittance spectra of the nonstained images and the corresponding microscopy images of H\&E-stained slides to learn non-linear mappings between these image pairs using a conditional generative adversarial network (cGAN) \cite{bayramoglu}. Another study describes unsupervised segmentation for low-contrast multichannel color nonstained pathology images by non-linearly mapping such images to an increased number of channels using an empirical kernel map in combination with non-negative matrix factorization \cite{kopriva2015unsupervised}. Amrania \textit{et. al} propose an instrument that use a bespoke IR filters at different wavelengths to capture nuclei and cytoplasm in the tissue \cite{amrania2012digistain}. Others have reported using computational analysis and the non-linear mapping of spectral data from chemical imaging using infrared (IR) microscopy from non-stained images to stain cellular structures; and combining fluorescence and reflectance mosaics to generate virtual H\&E images \cite{mayerich2015stain,bini2011confocal}. However, all these methods suffer from significant limitations such as a) excitation by specific wavelengths (UV light) and acquisition of specialized hyperspectral, auto-fluorescence, fluorescence, multispectral images or repeated acquisition of IR spectra using expensive systems \cite{bautista,bayramoglu,kopriva2015unsupervised,mayerich2015stain,rivenson2018deep}; b) stain only a few cellular components with low accuracies and limited colors \cite{bautista}; c) loss of information in the stained images is quite significant precluding evaluation for diagnostic needs \cite{bayramoglu}; d) mappings are incapable of processing complex functions and high dimensional data to fully represent in the H\&E-stained images \cite{bautista,kopriva2015unsupervised,mayerich2015stain}. 

We tackle the problems of digital H\&E staining of nonstained paraffin-embedded WSRI to circumvent the manual staining process, and digital H\&E destaining of stained images. To our knowledge, we are the first to report digital staining and destaining of these existing slide images at the point-of-care without additional equipment or devices. We also devise a novel loss function that enforces tissue structure preservation in GAN outputs.


% \onecolumn
\begin{figure*}[ht]
	\subfloat[]{
    	\includegraphics[width=\textwidth]{images/process_figure}
        \label{fig: process_fig}
    } \\
    \subfloat[]{
    	\includegraphics[width=0.37\textwidth]{images/forward_model}
        \label{fig: generator_input}
    }
    \hfill
    \vline
    \hfill
    \subfloat[]{
    	\includegraphics[width=0.57\textwidth]{images/reverse_model}
        \label{fig: discriminator_input}
    }
  	\caption{Preprocessing flowchart and whole slide RGB images (WSRI) of prostate core biopsy used by generative machine learning models for computational H\&E staining and destaining. (a) Data acquisition, H\&E staining, registration and patch creation process for generating input images for machine learning models (b) Computational staining models use nonstained images (input) and generate predicted H\&E stained images (output); (c) The computational destaining model uses H\&E stained images (input) and generates predicted destained images (output) that were then validated using a secondary staining model (restained output)}
\end{figure*}
% \endgroup

\section{Proposed methodology}
\subsection{Data collection and preprocessing}
The Partners Human Research Committee (Boston, MA) approved protocol 2014P002435, after which excess material from prostate core biopsies performed in the course of routine clinical care (2014-2017) at Brigham and Women's Hospital (BWH), Boston, MA, were obtained for this study. Briefly, prostate core biopsy specimens were immediately fixed in 10\% formalin, paraffin embedded, cut into 4-micron thick sections and placed on standard glass slides that were placed in archival storage at room temperature. Nonstained paraffin-embedded slides were scanned with the Aperio ScanScope XT system (Leica Biosystems, Buffalo Grove, IL) at 20$\times$ magnification. Subsequently, nonstained paraffin-embedded slides were stained with H\&E on the Agilent Dako Autostainer (Agilent, Santa Clara, CA), and these stained slides were re-scanned on the Aperio ScanScope XT at 20$\times$ magnification at the Harvard Medical School Tissue Microarray \& Imaging Core (TMIC). Nonstained-stained image pairs were registered using Adobe Photoshop (Adobe Systems, San Jose, CA).

The size of pathology WSRIs was too large (approx. 40,000$\times$40,000 pixels) to input directly into standard deep learning architectures.  We used a sliding window to extract 1024$\times$1024 pixel patches from each registered WSRI nonstained-stained pair. The sliding window stride was set at one-fourth the patch size resulting in 52,196 patch-pairs from 19 WSRI pairs. Images generated by computational staining and destaining machine learning models were compared to ground truth H\&E-stained images acquired from the TMIC by a structural similarity (SSIM) index and by an expert pathologist.

\subsection{Network architecture}
A generative adversarial network (GAN) is a type of deep learning architecture that consists of two network components: a generator $G$ that tries to generate realistic outputs from the given input and a discriminator $D$ that learns a binary classification to differentiate between the synthetic images produced by $G$ from the real images \cite{goodfellow2014generative} in the training dataset. A conditional GAN (cGAN) is a GAN where the output is additionally conditioned on an input image \cite{cGAN_paper}. cGANs are well suited for generative tasks for images and photographs \cite{bayramoglu,cGAN_paper,neffgenerative}.

Computational staining and destaining of patches extracted from nonstained and H\&E-stained WSRI was performed using a novel loss function for the cGAN architecture \cite{cGAN_paper}. The U-net architecture was used to create the generator while PatchGAN was used for the discriminator. The pix2pix architecture was updated with the following changes: layers were added to both the encoder and decoder part of the cGAN U-net model to facilitate input images of 1024$\times$1024 pixels; and the loss function was modified for better results. The cGAN network with the default loss function, GAN loss + L1 loss, generated computationally stained images that contained high-level tiling noise (data not shown). Multiple regularization terms were tested for increased preservation of structural information in the generated images. Pearson's correlation coefficient (CC) was chosen as it reduced high level tiling artifacts in the generated images. Pearson's correlation coefficient term was calculated between the output and target image. The overall loss function with the Pearson\'s correlation coefficient regularization term is:

\begin{align*}
\mathcal{L}_{cGAN}(G, D) &=\mathbb{E}_{x,y}[log D(x, y)]\ + \\ & \ \ \ \ \alpha \mathbb{E}_{x,z}[log(1-D(x,\ G(x, z)))]\\
\mathcal{L}_{L1}(G) &= \mathbb{E}_{x,y,z}(\parallel y-G(x,z) \parallel_{1})\\
\mathcal{L}_{correlation\ coeff}(G) &= \mathbb{E}_{x,y,z}(corr\_coef(y,\ G(x,z)))
\label{equation_1}
\end{align*}

Our final loss function is:
\begin{align*}
G^{*} =\ & arg\ \underset{G}{min}\ \underset{D}{max}\ \mathcal{L}_{cGAN}(G, D)\ + \\ & \lambda \mathcal{L}_{L1}(G) + \gamma \mathcal{L}_{correlation\ coeff}(G)
\end{align*}

where $x$ is the input image, $y$ is the target image and $z$ is the random noise, added as dropout in our work. $\mathcal{L}_{cGAN}(G, D)$ is the cGAN loss function, $\mathcal{L}_{L1}(G)$ is the L1 loss between the output of the generator and the target image, and $\mathcal{L}_{correlation\ coeff}(G)$ is the proposed term that calculated the Pearson\'s correlation coefficient between the generator output and target image. $\alpha=1$, $\lambda=100$ and $\gamma=10$ gave best results.

\begin{figure*}[h]
	\begin{center}
  		\includegraphics[width=\textwidth]{images/pred_grid}
	\end{center}
	\caption{Representative input and output images from computational staining and destaining networks.  Images in panel L show input in row (a), target (ground truth) in row (b) and output generated by the computational staining network in row (c).  Images in panel R show input in row (a), computational destaining network generated output in row (b) and output generated by the secondary staining network in row (c). Arrows represent microscopic morphological and structural features of prostrate core biopsy samples.}
    \label{fig: model_results}
\end{figure*}

\subsection{Training}
Two separate machine learning models were trained: 1) a staining model that generates H\&E-stained prostate core WSRI using previously unseen paraffin embedded non-stained RGB WSRI as input, and 2) a destaining model that reverses the process and computationally destains previously unseen H\&E-stained RGB images. Both models were trained using 40,148 patches from 14 WSRIs and validated on 12,048 patches from 5 WSRIs. Images used for training and those used for validating performance did not overlap. The discriminator was trained for every generator training step. Both networks were trained for 10 epochs each using Adam optimization and a batch size of one on a TITAN X GPU (NVIDIA, Santa Clara, CA) with 12 GB of VRAM and CUDA acceleration to speed up training. One epoch of training (40,148 training patches) took approximately 10 GPU hours. The patches were randomly flipped and dropout was used to prevent over-fitting.


\section{Results}

% What can we see from the grid figure
\subsection{Computational H\&E staining}
Results from all staining and destaining networks are shown in Figure 2. Output images from the computational staining network [Figure 2 (L) (c); Supplementary figure \ref{supp_fig: staining_grid}] accurately predict the spatial location and silhouette of the tissue. The computationally stained output images were compared to corresponding target TMIC H\&E-stained images using SSIM (0.6760) and CC (0.6878).

% \begin{table}
%     \caption{Performance of the computational staining network}
%     \begin{tabular*}{\columnwidth}{l @{\extracolsep{\fill}}r}
%         \hline
%         \textbf{Parameter} & \textbf{Staining} \\
%         & \textbf{network} \\
%         \hline
%         SSIM 					& 0.6760 \\
%         Correlation coefficient & 0.6878 \\
%         \hline
%     \end{tabular*}
%     \label{table: staining_results_table}
% \end{table}

Examination by an expert pathologist showed that the computational staining network predicts the presence of different histological structures and cell types such as prostatic glands, prostatic stroma, nerve, adipocytes and vascular spaces, but rarely predicts a structure that does not exist (arrows in Figure 2 (L) (c); Supplementary figure \ref{supp_fig: staining_grid}). The morphology of the prostatic stroma is replicated best. The cellular detail of the prostatic gland epithelium is not represented well, with loss of cell polarity, nuclear location, and cytoplasmic features. Some structures, such as adipocytes and nerve, are suggested (arrows in Figure 2 (L) (c); Supplementary figure \ref{supp_fig: staining_grid}).

In a computationally stained patch shown in Figure 2 (c) I, the presence and morphologic appearance of prostatic glands (arrows labeled 1) and stroma (arrows labeled 2) are depicted as the increased cellular density identified at the periphery of the tissue composed of epithelioid cells and relative hypocellularity in the center composed of spindle cells with indistinct cytoplasmic edges, respectively. The palisaded nature of the prostatic gland nuclei and the distinct edges present on the prostatic gland cytoplasm can be improved (Figure 2 (c) I). In Figure 2 (c) II, the location and presence of adipocytes (arrows labeled 1) and nerve (arrows labeled 2) are represented accurately. 
% \hlc[red]{We also reconstructed entire prostate biopsy cores using computationally stained H\&E patches stained by the network and found the accuracy observed in individual patches generalize well.}

\begin{table}
    \caption{Performance of the secondary staining network using outputs from the computational destaining network; as compared to actual H\&E stained images.}
    \begin{tabular*}{\linewidth}{l @{\extracolsep{\fill}}r}
        \hline
        \textbf{Parameter} & \textbf{Destaining} \\
        & \textbf{network} \\
        \hline
        SSIM 					& 0.84 \\
        Correlation coefficient & 0.897 \\
        \hline
    \end{tabular*}
    \label{table: destaining_model_results}
\end{table}

\subsection{Computational destaining of H\&E images}
The output from the computational destaining network [Figure 2 (R) (b); Supplementary figure \ref{supp_fig: destaining_grid}] shows that the trained model can successfully destain H\&E stained RGB images. We validated accuracy of the destaining network by restaining the generated destained computational images by a secondary H\&E staining model and comparing to ground truth TMIC stained images [Figure 1 (c)]. Output images from the secondary network showed high similarities to the ground truth TMIC H\&E-stained images [Figure 2 (R) (c); Supplementary figure \ref{supp_fig: destaining_grid}] (Table \ref{table: destaining_model_results}). The destaining model identified the location and silhouette of all tissue present on the slides; its representation of morphologic features was also accurate. The presence of a clear basement membrane interface between glands and stroma, prostatic epithelium nuclear location, and cytoplasmic membrane detail are present in nearly all images shown in Figure 2 (R) (c) and supplementary figure \ref{supp_fig: destaining_grid}.

In individual computationally restained patches shown in Figure 2 (c) (IV and V), all structures are easily identifiable morphologically, including improved nuclear polarity/location (arrows labeled 1), crisper cytoplasmic borders (arrows labeled 2), and a more identifiable basement membrane interface between glands and stroma (arrows labeled 3). Rare structures, such as the glandular epithelium (bottom right in Figure 2 (c) VI arrow labeled 1), were not distinctly identifiable in the test images.

\section{Discussion and future work}
In this study, we report fully trained cGAN computational staining and destaining models, which learn highly non-linear mappings between high resolution nonstained and H\&E-stained RGB image pairs of prostate core biopsy tissue samples. The computational staining model predicted location and silhouette of the prostate tissue, different histological structures/cell types such as prostatic glands, prostatic stroma, nerve, adipocytes and vascular spaces and associated colors in our validation images with good accuracies.

Bautista \textit{et al.} attempt to classify cellular components (like nuclei, cytoplasm and red blood cells) and white spaces using multispectral nonstained and H\&E-stained images. The classification is discrete and uses biopsy samples from serial sections of the same tissue \cite{bautista}. On the other hand, our method is able to classify a gamut of color intensities and uses nonstained and stained whole slide RGB images from the same tissue biopsy slide for each patient. Kopriva \textit{et al.} produce a segmentation of low-contrast multi-channel nonstained images, whereas our method predicts the RGB intensities of H\&E-stained WSRIs without specifying a segmentation. The segmentations produced by the method described in Kopriva \textit{et al.} visually correspond to the ground truth but show imprecise classification for cellular structures \cite{kopriva2015unsupervised}. Our method's prediction of the visual qualities of H\&E-stained images allows the predicted images to be used for additional interpretation by medical experts and segmentation. Mayerich \textit{et al.} train a two layer artificial neural network to learn a mapping from FT-IR spectroscopy nonstained image pixels to the corresponding RGB color intensity, but it does not accurately recover spatial resolution and necessitates custom and variable methods of sample preparation and requires specialized imaging systems which are neither standardized nor readily accessible in pathology laboratories \cite{mayerich2015stain}. Bayramo\~glu use the transmittance spectra of hyperspectral nonstained images and corresponding microscopy images of size 1000$\times$1000 px and train a cGAN architecture to report generated H\&E-stained lung biopsy images. The generated images are low resolution and suffer from information loss (0.38 SSIM index) \cite{bayramoglu}.

Detailed expert pathologist analyses outline successes and challenges in staining of paraffin-embedded WSRI microscopic structures that quantitative image-based metrics like SSIM and correlation coefficient may not address \cite{pambrun2015limitations}. Input image pairs (nonstained and H\&E stained) used for training in our work may have differences in field of view, illumination and focal planes resulting in decrease in registration and mapping accuracies for the computational staining and destaining models. The suitability of the generated images for tumor diagnosis has not been evaluated. We are actively investigating methods to control for these factors to achieve higher accuracies. Because of a lack of discernible features and associated translucency in paraffin-embedded nonstained biopsy images \ref{supp_fig: destaining_grid}, a secondary H\&E staining model was used to check the accuracy of the destaining network. The destaining network performed with higher accuracies than our staining model. Higher accuracies observed in the results from the destaining network could be attributed to the greater amounts of structural information that is preserved from the input stained images. In summary, the models described in this paper do not require hyperspectral or multispectral imaging systems or additional biochemical steps. The proposed models preserve microscopic morphological and sub-cellular structures in the H\&E stained and destained images of paraffin embedded prostrate core biopsy samples making them practical. To our knowledge, this is the first study describing the computational staining and destaining of RGB images of whole slide prostate core biopsy sections using cGAN.


% Setting figure counter to 0
\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{figure}{0}

%% Supplementary results
\section{Supplementary material}

% Staining 
\subsection{Computational staining model results}
Supplementary figure \ref{supp_fig: staining_grid} shows computationally H\&E stained output patches and corresponding input images and ground truth patches.

% Destaining
\subsection{Computational destaining model results}
Supplementary figure \ref{supp_fig: destaining_grid} shows computationally H\&E destained output patches; corresponding computationally restained patches and input and ground truth patches.

\balance
\bibliographystyle{unsrt}
\bibliography{bibliography}

\onecolumn

\begin{figure}
  \begin{center}
    \includegraphics[width=0.9\linewidth]{supplementary_material/staining_grid.pdf}
    \caption{Input image patches (a), ground truth image patches (b) and computationally H\&E stained output image patches (c)}
    \label{supp_fig: staining_grid}
  \end{center}
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics[width=0.90\linewidth]{supplementary_material/destaining_grid.pdf}
    \caption{H\&E stained image patches (a), non-stained image patches (b), computationally destained image patches (c), computationally restained output image patches (d)}
    \label{supp_fig: destaining_grid}
  \end{center}
\end{figure}

\end{document}

