\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
% \usepackage{nips_2018}
% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2018}
%\setcitestyle{numbers}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}


\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{float}
\usepackage{wrapfig}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{authblk}

\newcommand{\R}{\mathbb{R}}

\newcounter{ass_counter}
\newcounter{thm_counter}
\newtheorem{assumption}[ass_counter]{Assumption}

\title{Can We Gain More from Orthogonality Regularizations in Training Deep CNNs?}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  {\bf Nitin Bansal}\hspace{0.5cm} {\bf Xiaohan Chen}\hspace{0.5cm} {\bf Zhangyang Wang} \\
  Department of Computer Science and Engineering\\
  Texas A\&M University,\\College Station, TX 77843, USA \\
  \texttt{\{bansa01, chernxh, atlaswang\}@tamu.edu} \\
}


\begin{document}

\maketitle
\begin{abstract}
This paper seeks to answer the question: \textit{as the (near-) orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how can we enforce it in more effective and easy-to-use ways?} We develop novel orthogonality regularizations on training deep CNNs, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property. These plug-and-play regularizations can be conveniently incorporated into training almost any CNN without extra hassle. We then benchmark their effects on state-of-the-art models: ResNet, WideResNet, and ResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100, SVHN and ImageNet. We observe consistent performance gains after applying those proposed regularizations, in terms of both the final accuracies achieved, and faster and more stable convergences. We have made our codes and pre-trained models publicly available.\footnote{\url{https://github.com/nbansal90/Can-we-Gain-More-from-Orthogonality}}.
\end{abstract}

\section{Introduction}
Despite the tremendous success of deep convolutional neural networks (CNNs) \cite{krizhevsky2012imagenet}, their training remains to be notoriously difficult both theoretically and practically, especially for state-of-the-art ultra-deep CNNs. Potential reasons accounting for such difficulty lie in multiple folds, ranging from vanishing/exploding gradients \cite{glorot2010understanding}, to feature statistic shifts \cite{ioffe2015batch}, to the proliferation of saddle points \cite{dauphin2014identifying}, and so on. To address these issues, various solutions have been proposed to alleviate those issues, examples of which include parameter initialization \cite{saxe2013exact}, residual connections \cite{he2016deep}, normalization of internal activations \cite{ioffe2015batch}, and second-order optimization algorithms \cite{dauphin2014identifying}.

This paper focuses on one type of structural regularizations: \textit{orthogonality}, to be imposed on linear transformations between hidden layers of CNNs. The orthogonality implies energy preservation, which is extensively explored for filter banks in signal processing and guarantees that energy of activations will not be amplified \cite{zhou2006special}. Therefore, it can stabilize the distribution of activations over layers within CNNs \cite{rodriguez2016regularizing,desjardins2015natural} and make optimization more efficient. \cite{saxe2013exact} advocates orthogonal initialization of weight matrices, and theoretically analyzes its effects on learning efficiency using deep linear networks. Practical results on image classification using orthogonal initialization are also presented in \cite{mishkin2015all}. More recently, a few works \cite{jia2016improving,harandi2016generalized,ozay2016optimization,xie2017all,huang2017orthogonal} look at (various forms of) enforcing orthogonality regularizations or constraints throughout training, as part of their specialized models for applications such as classification \cite{xie2017all} or person re-identification \cite{sun2017svdnet}. They observed encouraging result improvements. However, a dedicated and thorough examination on the effects of orthogonality for training state-of-the-art general CNNs has been absent so far.

Even more importantly, how to evaluate and enforce orthogonality for non-square weight matrices does not have a sole optimal answer. As we will explain later, existing works employ the most obvious but not necessarily appropriate option. We will introduce a series of more sophisticated regularizers that lead to larger performance gains.


This paper investigates and pushes forward various ways to enforce orthogonality regularizations on training deep CNNs. Specifically, we introduce three novel regularization forms for orthogonality, ranging from the double-sided variant of standard Frobenius norm-based regularizer, to utilizing Mutual Coherence (MC) and Restricted Isometry Property (RIP) tools \cite{candes2005decoding,donoho2006compressed,wang2016sparse}. Those orthogonality regularizations have a plug-and-play nature, i.e., they can be incorporated with training almost any CNN without hassle. We extensively evaluate the proposed orthogonality regularizations on three state-of-the-art CNNs: ResNet \cite{he2016deep}, ResNeXt \cite{xie2017aggregated}, and WideResNet \cite{zagoruyko2016wide}. In all experiments, we observe the consistent and remarkable accuracy boosts (e.g., \textbf{2.31\%} in CIFAR-100 top-1 accuracy for WideResNet), as well as faster and more stable convergences, \textit{without any other change made to the original models}. It implies that many deep CNNs may have not been unleashed with their full powers yet, where orthogonality regularizations can help.
Our experiments further reveal that larger performance gains can be attained by designing stronger forms of orthogonality regularizations. We find the RIP-based regularizer, which has better analytical grounds to characterize near-orthogonal systems \cite{zhang2011sparse}, to consistently outperform existing Frobenius norm-based regularizers and others.

\section{Related Work}
To remedy unstable gradient and co-variate shift problems, \cite{glorot2010understanding,he2015delving} advocated near constant variances of each layer's output for initialization. \cite{ioffe2015batch} presented a major breakthrough in stabilizing training, via ensuring each layerâ€™s output to be identical distributions which reduce the internal covariate shift.  \cite{salimans2016weight} further decoupled the norm of the weight vector from its phase(direction) while introducing independences between minibatch examples, resulting in a better optimization problem.
Orthogonal weights have been widely explored in Recurrent Neural Networks (RNNs) \cite{pascanu2013difficulty,dorobantu2016dizzyrnn,arjovsky2016unitary,mhammedi2016efficient,vorontsov2017orthogonality,wisdom2016full} to help avoid gradient vanishing/explosion.  \cite{pascanu2013difficulty} proposed a soft constraint technique to combat vanishing gradient, by forcing the Jacobian matrices to preserve energy measured by Frobenius norm. The more recent study \cite{vorontsov2017orthogonality} investigated the effect of soft versus hard orthogonal constraints on the performance of RNNs, the former by specifying an allowable range for the maximum singular value of the transition matrix and thus allowing for its small intervals around one.


In CNNs, orthogonal weights are also recognized to stabilize the layer-wise distribution of activations \cite{rodriguez2016regularizing} and make optimization more efficient. \cite{saxe2013exact,mishkin2015all} presented the idea of orthogonal weight initialization in CNNs, which is driven by the norm-preserving property of orthogonal matrix: a similar outcome which BN tried to achieve. \cite{saxe2013exact} analyzed the non-linear dynamics of CNN training. Under simplified assumptions, they concluded that random orthogonal initialization of weights will give rise to the same convergence rate as unsupervised pre-training, and will be superior than random Gaussian initialization.
However, a good initial condition such as orthogonality does not necessarily sustain throughout training. In fact, the weight orthogonality and isometry will break down easily when training starts, if not properly regularized \cite{saxe2013exact}. Several recent works \cite{harandi2016generalized,ozay2016optimization,huang2017orthogonal} considered Stiefel manifold-based hard constraints of weights. \cite{harandi2016generalized} proposed a Stiefel layer to guarantee fully connected layers to be orthogonal by using Reimannian gradients, without considering similar handling for convolutional layers; their performance reported on VGG networks \cite{simonyan2014very} were less than promising. \cite{ozay2016optimization} extended Riemannian optimization to convolutional layers and require filters within the same channel to be orthogonal. To overcome the challenge that CNN weights are usually rectangular rather than square matrices, \cite{huang2017orthogonal} generalized Stiefel manifold property and formulated an Optimization over Multiple Dependent Stiefel Manifolds (OMDSM) problem. Different from \cite{ozay2016optimization}, it ensured filters across channels to be orthogonal. A related work \cite{jia2016improving} adopted a Singular Value Bounding (SVB) method, via explicitly thresholding the singular values of weight matrices between a pre-specified narrow band around the value of one.

The above methods
\cite{jia2016improving,harandi2016generalized,ozay2016optimization,huang2017orthogonal} all fall in the category of enforcing ``hard orthogonality constraints'' into optimization (\cite{jia2016improving} could be viewed as a relaxed constraint), and have to repeat singular value decomposition (SVD) during training. The cost of SVD on high-dimensional matrices is expensive even in GPUs, \textit{which is one reason why we choose not to go for the ``hard constraint'' direction in this paper}. Moreover, since CNN weight matrices cannot exactly lie on a Stiefel manifold as they are either very ``thin'' or ``fat'' (e.g., $W^T W = I$ may never happen for an overcomplete ``fat'' $W$ due to rank deficiency of its gram matrix), special treatments are needed to maintain the hard constraint. For example, \cite{huang2017orthogonal} proposed group based orthogonalization to first divide an over-complete weight matrix into ``thin'' column-wise groups, and then applying Stiefel manifold constraints group-wise. The strategy was also motivated by reducing the computational burden of computing large-scale SVDs. Lately, \cite{balestriero2018spline,balestriero2018mad} interpreted CNNs as Template Matching Machines, and proposed a penalty term to force the templates to be orthogonal with each other, leading to significantly improved classification performance and reduced overfitting with no change to the deep architecture.

A recent work \cite{xie2017all} explored orthogonal regularization, by enforcing the Gram matrix of each weight matrix to be close to identity under Frobenius norm. It constrains orthogonality among filters in one layer, leading to smaller correlations among learned features and implicitly reducing the filter redundancy. Such a soft orthonormal regularizer is differentiable and requires no SVD, thus being computationally cheaper than its ``hard constraint'' siblings. However, we will see later that Frobenius norm-based orthogonality regularization is only a rough approximation, and is inaccurate for ``fat'' matrices as well. The authors relied on a backward error modulation step, as well as similar group-wise orthogonalization as in \cite{huang2017orthogonal}. We also notice that \cite{xie2017all} displayed the strong advantage of enforcing orthogonality in training the authors' self-designed plain deep CNNs (i.e. without residual connections). However, they found fewer performance impacts when applying the same to training prevalent network architectures such as ResNet \cite{he2016deep}.
In comparison, our orthogonality regularizations can be added to CNNs as ``plug-and-play'' components, without any other modification needed. We observe evident improvements brought by them on most popular ResNet architectures.


Finally, we briefly outline a few works related to orthogonality in more general senses. One may notice that enforcing matrix to be (near-)orthogonal during training will lead to its spectral norm being always equal (or close) to one, which links between regularizing orthogonality and spectrum. In \cite{wang2016analysis}, the authors showed that the spectrum of Extended Data Jacobian Matrix (EDJM) affected the network performance, and proposed a spectral
soft regularizer that encourages major singular values of EDJM to be closer to the largest one. \cite{keskar2016large} claimed that the maximum eigenvalue of the Hessian predicted the generalizability of CNNs. Motivated by that, \cite{yoshida2017spectral} penalized the spectral norm of weight matrices
in CNNs. A similar idea was later extended in \cite{miyato2018spectral} for training generative adversarial networks, by proposing a spectral normalization technique to normalize the spectral norm/Lipschitz norm of the weight matrix to be one.

\section{Deriving New Orthogonality Regularizations}

In this section, we will derive and discuss several orthogonality regularizers. Note that those regularizers are applicable to both fully-connected and convolutional layers. The default mathematical expressions of regularizers will be assumed on a fully-connected layer $W$ $ \in $ $^{m \times n}$ ($m$ could be either larger or smaller than $n$). For a convolutional layer $ C $  $ \in $ $^{S \times H\times C\times M}$, where $S, H, C, M$ are filter width, filter height, input channel number and output channel number, respectively, we will first reshape $C$ into a matrix form $W'$ $ \in $ $ {m' \times n'}$, where $m' = S \times H \times C$ and $n' = M$. The setting for regularizing convolutional layers follows \cite{xie2017all,huang2017orthogonal} to enforces orthogonality across filter, encouraging filter diversity. All our regularizations are directly amendable to almost any CNN: there is no change needed on the network architecture, nor any other training protocol (unless otherwise specified).

\subsection{Baseline: Soft Orthogonality Regularization}
Previous works \cite{xie2017all,balestriero2018spline,balestriero2018mad} proposed to require the Gram matrix of the weight matrix to be close to identity, which we term as Soft Orthogonality  (SO) regularization:
\begin{equation}
\begin{aligned}
\text{(SO)} \qquad \lambda ||W^T W - I||_F^2,
\end{aligned}
\label{SO}
\end{equation}
where $\lambda$ is the regularization coefficient (the same hereinafter). It is a straightforward relaxation from the ``hard orthogonality'' assumption \cite{harandi2016generalized,ozay2016optimization,huang2017orthogonal,wang2018learning} under the standard Frobenius norm, and can be viewed as a different weight decay term  limiting the set of parameters close to a Stiefel manifold rather than inside a hypersphere. The gradient is given in an explicit form: $4 \lambda W (W^TW - I)$, and can be directly appended to the original gradient w.r.t. the current weight $W$.

However, SO (\ref{SO}) is flawed for an obvious reason: the columns of $W$ could possibly be mutually orthogonal, if and only if $W$ is undercomplete ($m \ge n$). For overcomplete $W$ ($m < n$), its gram matrix $W^TW\in\R^{n \times n}$ cannot be even close to identity, because its rank is at most $m$, making $||W^T W - I||_F^2$ a biased minimization objective. In practice, both cases can be found for layer-wise weight dimensions. The authors of \cite{huang2017orthogonal,xie2017all} advocated to further divide overcomplete $W$ into undercomplete column groups to resolve the rank deficiency trap. In this paper, we choose to simply use the original SO version (\ref{SO}) as a fair comparison baseline.

The authors of \cite{xie2017all} argued against the hybrid utilization of the original $\ell_2$ weight decay and the SO regularization. They suggested to stick to one type of regularization all along training. Our experiments also find that applying both together throughout training will hurt the final accuracy. Instead of simply discarding $\ell_2$ weight decay, we discover a \textit{scheme change} approach which is validated to be most beneficial to performance, details on this can be found in Section 4.1.


\subsection{Double Soft Orthogonality Regularization}

The double soft orthogonality regularization extends SO in the following form:
\begin{equation}
\begin{aligned}
\text{(DSO)} \qquad \lambda (||W^T W - I||_F^2 + ||WW^T  - I||_F^2).
\end{aligned}
\label{DSO}
\end{equation}
Note that an orthogonal $W$ will satisfy $W^T W = WW^T  = I$; an overcomplete $W$ can be regularized to have small $||WW^T  - I||_F^2$ but will likely have large residual $||W^TW  - I||_F^2$, and vice versa for an under-complete $W$. DSO is thus designed to cover both over-complete and under-complete $W$ cases; for either case, at least one term in (\ref{DSO}) can be well suppressed, requiring either rows or columns of $W$ to stay orthogonal. It is a straightforward extension from SO.

Another similar alternative to DSO is ``selective'' soft orthogonality regularization, defined as: $\lambda ||W^T W - I||_F^2$, if $m > n$; $\lambda ||WW^T  - I||_F^2$ if $m \le n$. Our experiments find that DSO always outperforms the selective regularization, therefore we only report DSO results.


\subsection{Mutual Coherence Regularization}

The mutual coherence \cite{donoho2006compressed} of $W$ is defined as:
\begin{equation}
\begin{aligned}
\mu_W= \underset{i \neq j}{\text{max}}\frac{|\langle w_i, w_j \rangle|}{||w_i||\cdot||w_j||},
\end{aligned}
\label{MC_original}
\end{equation}
where $w_i$ denotes the $i$-th column of $W$, $i = 1, 2, ..., n$. The mutual coherence (\ref{MC_original}) takes values between [0,1], and measures the highest correlation between any two columns of $W$. In order for $W$ to have orthogonal or near-orthogonal columns, $\mu_W$ should be as low as possible (zero if $m \ge n$).

We wish to suppress $\mu_W$ as an alternative way to enforce orthogonality. Assume $W$ has been first normalized to have unit-norm columns, $\langle w_i, w_j \rangle$ is essentially the $(i,j)$-the element of the Gram matrix $W^TW$, and $i \neq j$ requires us to consider off-diagonal elements only. Therefore, we propose the following mutual coherence (MC) regularization term inspired by (\ref{MC_original}:
\begin{equation}
\begin{aligned}
\text{(MC)} \qquad \lambda||W^TW - I||_\infty.
\end{aligned}
\label{MC}
\end{equation}
Although we do not explicitly normalize the column norm of $W$ to be one, we find experimentally that minimizing (\ref{MC}) often tends to implicitly encourage close-to-unit-column-norm $W$ too, making the objective of (\ref{MC}) a viable approximation of mutual coherence (\ref{MC_original})\footnote{We also tried to first normalize columns of $W$ and then apply (\ref{MC}), without finding any performance benefits.}.

The gradient of $||W^TW - I||_\infty$ could be explicitly solved by applying a smoothing technique to the nonsmooth $\ell_\infty$ norm, e.g., \cite{lin2015optimized}. However, it will invoke an iterative routine each time to compute $\ell_1$-ball proximal projection, which is less efficient in our scenario where massive gradient computations are needed. In view of that, we turn to using auto-differentiation to approximately compute the gradient of (\ref{MC}) w.r.t. $W$.

\subsection{Spectral Restricted Isometry Property Regularization}

Recall that the RIP condition~\cite{candes2005decoding} of $W$ assumes:
\begin{assumption}\label{asm}
    For all vectors $z$ $ \in $ $\R^n$ that is $k$-sparse, there exists a small $\delta_W \in (0,1)$ s.t. $(1-\delta_W) \leq \frac{||Wz||^2}{||z||^2} \leq (1+\delta_W)$.
\end{assumption}
The above RIP condition essentially requires that every set of columns in $W$, with cardinality no larger than $k$, shall behave like an orthogonal system. If taking an extreme case with $k = n$, RIP then turns into another criterion that enforces the entire $W$ to be close to orthogonal. Note that both mutual incoherence and RIP are well defined for both under-complete and over-complete matrices.

We rewrite the special RIP condition with $k = n$ in the form below:
\begin{equation}
\begin{aligned}
\left|\frac{||Wz||^2}{||z||^2} - 1\right| \le \delta_W, \, \forall z \in \R^n
\end{aligned}
\label{RIP}
\end{equation}
Notice that $\sigma (W) = \sup_{z \in \R^n, z \neq \boldsymbol{0}} \frac{||Wz||}{||z||}$ is the spectral norm of $W$, i.e., the largest singular value of $W$. As a result, $\sigma (W^TW - I) = \underset{{z \in \R^n, z}\neq 0} {\sup}|\frac{||Wz||^2}{||z||^2} - 1|$. In order to enforce orthogonality to $W$ from an RIP perspective, one may wish to minimize the RIP constant $\delta_W$ in the special case $k = n$, which according to the definition should be chosen as $\underset{{z \in \R^n, z}\neq 0} {\sup}|\frac{||Wz||^2}{||z||^2} - 1|$ as from (\ref{RIP}). Therefore, we end up equivalently minimizing the spectral norm of $W^TW - I$:
\begin{equation}
\begin{aligned}
\text{(SRIP)} \qquad \lambda\cdot \sigma (W^TW - I).
\end{aligned}
\label{SRIP}
\end{equation}
It is termed as the Spectral Restricted Isometry Property (SRIP) regularization.

\textit{The above reveals an interesting hidden link}: regularizations with spectral norms were previously investigated in \cite{yoshida2017spectral,miyato2018spectral}, through analyzing small perturbation robustness and Lipschitz constant. The spectral norm re-arises from enforcing orthogonality when RIP condition is adopted. But compared to the  spectral norm (SN) regularization \cite{yoshida2017spectral} which minimizes $\sigma (W)$, SRIP is instead enforced on $W^TW - I$. Also compared to \cite{miyato2018spectral} requiring the spectral norm of $W$ to be exactly 1 (developed for GANs), SRIP requires \textit{all singular values of $W$ to be close to 1}, which is essentially \textbf{stricter} because the resulting $W$ needs also be \textit{well conditioned}.

We again refer to auto differentiation to compute the gradient of (\ref{SRIP}) for simplicity. However, even computing the objective value of (\ref{SRIP}) can invoke the computationally expensive EVD. To avoid that, we approximate the computation of spectral norm using the power iteration method. Starting with a randomly initialized $v \in \R^n$, we iteratively perform the following procedure a small number of times (2 times by default) :
\vspace{-0.5em}
\begin{equation}
\begin{aligned}
u \leftarrow (W^TW - I)v, v \leftarrow (W^TW - I)u, \sigma (W^TW - I) \leftarrow \frac{||v||}{||u||}.
\end{aligned}
\label{power}
\vspace{-0.5em}
\end{equation}
With such a rough approximation as proposed, SRIP reduces computational cost from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn^2)$, and is practically much faster for implementation.

\section{Experiments on Benchmarks}

First of all, we will base our experiments on several popular state-of-the-art models: ResNet\cite{he2016deep,he2016identity} (including several different variants), Wide ResNet\cite{zagoruyko2016wide} and ResNext\cite{xie2017aggregated}. For fairness, all pre-processing, data augmentation and training/validation/testing splitting are strictly identical to the original training protocols in \cite{zagoruyko2016wide,he2016deep,he2016identity,xie2017aggregated}. All hyper-parameters and architectural details remain unchanged too, unless otherwise specified.

We structure the experiment section in the following way. In the first part of experiments, we design a set of intensive experiments on CIFAR 10 and CIFAR-100, which consist of 60,000 images of size 32$\times$32 with a 5-1 training-testing split, divided into 10 and 100 classes respectively. We will train each of the three models with each of the proposed regularizers, and compare their performance with the original versions, in terms of both final accuracy and convergence. In the second part, we further conduct experiments on ImageNet and SVHN datasets. In both parts, we also compare our best performer SRIP with existing regularization methods with similar purposes.

 \paragraph{Scheme Change for Regularization Coefficients}
All the regularizers have an associated regularization coefficient denoted by $\lambda$, whose choice play an important role in the regularized training process. Correspondingly, we denote the regularization coefficient for the $\ell_2$ weight decay used by original models as $\lambda_2$.
From experiments, we observe that fully replacing $\ell_2$ weight decay with orthogonal regularizers will accelerate and stabilize training at the beginning of training, but will negatively affect the final accuracies achievable. We conjecture that while the orthogonal parameter structure is most beneficial at the initial stage, it might be overly strict when training comes to the final ``fine tune'' stage, when we should allow for more flexibility for parameters. In view of that, we did extensive ablation experiments and identify a switching scheme between two regularizations, at the beginning and late stages of training. Concretely, we gradually reduce $\lambda$ (initially 0.1-0.2) to $10^{-3}$, $10^{-4}$ and $10^{-6}$, after 20, 50 and 70 epochs, respectively, and finally set it to zero after 120 epochs. For $\lambda_2$, we start with $10^{-8}$; then for SO/DSO regularizers, we increase $\lambda_2$ to $10^{-4}$/$5 \times 10^{-4}$, after 20 epochs. For MC/SRIP regularizers, we find them insensitive to the choice of $\lambda_2$, potentially due to their stronger effects in enforcing $W^TW$ close to $I$; we thus stick to the initial $\lambda_2$ throughout training for them. Such an empirical ``scheme change'' design is found to work nicely with all models, benefiting both accuracy and efficiency. The above $\lambda$/$\lambda_2$ choices apply to all our experiments.


As pointed out by one anonymous reviewer, applying orthogonal regularization will change the optimization landscape, and its power seems to be a complex and dynamic story throughout training. In general, we find it to show a strong positive impact at the early stage of training (not just initialization), which concurs with previous observations. But such impact is observed to become increasingly negligible, and sometime (slightly) negative, when the training approaches the end. That trend seems to be the same for all our regularizers.

 \begin{figure*}[ht]
    \begin{center}
        \includegraphics[width=1.00\linewidth]{final_resnet_cifar10f.png}
        \\
        \includegraphics[width=1.00\linewidth]{final_resnet_cifar100f.png}
    \end{center}
    \vspace{-1em}
    \caption{Validation curves during training for ResNet-110. Top: CIFAR-10; Bottom: CIFAR-100;}
        \vspace{-1em}
    \label{fig:resnet_validation1}
\end{figure*}


\subsection{Experiments on CIFAR-10 and CIFAR-100}
We employ three model configurations on the CIFAR-10 and CIFAR-100 datasets:

\vspace{-0.8em}
 \paragraph{ResNet 110 Model \cite{he2016deep}}
The 110-layer ResNet Model \cite{he2016deep} is a very strong and popular ResNet version. It uses Bottleneck Residual Units, with a formula setting given by $p = 9n + 2$, where \textit{n} denotes the total number of convolutional blocks used and \textit{p} the total depth. We use the Adam optimizer to train the model for 200 epochs, with learning rate starting with 1e-2, and then subsequently decreasing to $10^{-3}$, $10^{-5}$ and $10^{-6}$, after 80, 120 and 160 epochs, respectively.

\vspace{-0.8em}
\paragraph{Wide ResNet 28-10 Model \cite{zagoruyko2016wide}}
For the Wide ResNet model \cite{zagoruyko2016wide}, we use depth 28 and $k$ (width) 10 here, as this configuration gives the best accuracies for both CIFAR-10 and CIFAR-100, and is (relatively) computationally efficient. The model uses a Basic Block B(3,3), as defined in ResNet \cite{he2016deep}. We use the SGD optimizer with a Nesterov Momentum of 0.9 to train the model for 200 epochs. The learning rate starts at 0.1, and is then decreased by a factor of 5, after 60, 120 and 160 epochs, respectively. We have followed all other settings of \cite{zagoruyko2016wide} identically.

\vspace{-0.8em}
\paragraph{ResNext 29-8-64 Model \cite{xie2017aggregated}}
For ResNext Model \cite{xie2017aggregated}, we consider the 29-layer architecture with a cardinality of 8 and widening factor as 4, which reported the best state-of-the-art CIFAR-10/CIFAR-100 results compared to other contemporary models with similar amounts of trainable parameters. We use the SGD optimizer with a Nesterov Momentum of 0.9 to train the model for 300 epochs. The learning starts from 0.1, and decays by a factor of 10 after 150 and 225 epochs, respectively.

\vspace{-0.8em}
\paragraph{Results}
Table~\ref{table:cifar_accuracy} compares the top-1 error rates in the three groups of experiments. To summarize, SRIP is obviously the winner in almost all cases (except the second best for ResNet-110, CIFAR-100), with remarkable performance gains, such as an impressive \textbf{2.31\%} top-1 error reduction for Wide ResNet-28-10. SO acts a surprisingly strong baseline and is often only next to SRIP. MC can usually outperform the original baseline but remains inferior to SRIP and SO. DSO seems the most ineffective among all four, and might perform even worse than the original baseline. We also carefully inspect the training curves (in term of validation accuracies w.r.t epoch numbers) of different methods on CIFAR-10 and CIFAR-100, with ResNet-110 curves shown in Fig.~\ref{fig:resnet_validation1} for example. All starting from random scratch, we observe that all four regularizers significantly accelerate the training process in the initial training stage, and maintain at higher accuracies throughout (most part of) training, compared to the un-regularized original version. The regularizers can also stabilize the training in terms of less fluctuations of the training curves. We defer a more detailed analysis to Section 4.3.


 \begin{table}
    \center
    \caption{Top-1 error rate comparison by ResNet 110, Wide ResNet 28-10 and ResNext 29-8-64 on CIFAR-10 and CIFAR-100. * indicates results by us running the provided original model.}
    \label{table:cifar_accuracy}
    \begin{tabular}{p{4cm} p{2.7cm} p{2.7cm}p{2.5cm}}
        \hline
        Model& Regularizer & CIFAR-10 & CIFAR-100 \\
        \hline
        ResNet-110 \cite{he2016deep}&None &7.04*&25.42*\\
        & SO    & 6.78 & \textbf{25.01}\\
        & DSO   & 7.04 & 25.83 \\
        & MC    & 6.97 & 25.43\\
        & SRIP   & \textbf{6.55}  & 25.14 \\
        \hline
        Wide ResNet 28-10 \cite{zagoruyko2016wide}&None&4.16*&20.50*\\
        & SO    & 3.76 & 18.56 \\
        & DSO   & 3.86 & 18.21 \\
        & MC    & 3.68 & 18.90 \\
        & SRIP  & \textbf{3.60} & \textbf{18.19} \\
        \hline
        ResNext 29-8-64 \cite{xie2017aggregated}&None&3.70*&18.53*\\
        &SO&3.58 &17.59\\
        &DSO&3.85&19.78\\
        &MC&3.65&17.62\\
        &SRIP&\textbf{3.48}&\textbf{16.99}\\
        \hline
    \end{tabular}
    \vspace{-1em}
\end{table}

Besides, we validate the helpfulness of scheme change. For example, we train Wide ResNet 28-10 with SRIP, but without scheme change (all else remains the same). We witness a $0.33\%$ top-1 error increase on CIFAR-10, and $0.90\%$ on CIFAR-100, although still outperforming the original un-regularized models. Other regularizers perform even worse without scheme change.

\vspace{-0.8em}
\paragraph{Comparison with Spectral Regularization}
We compare SRIP with the spectral regularization (\textbf{SR}) developed in \cite{yoshida2017spectral}: $\frac{\lambda_s}{2} \sigma(W)^2$, with the authors' default $\lambda_s = 0.1$. All other settings in \cite{yoshida2017spectral} have been followed identically. We apply the SR regularization to training the Wide ResNet-28-10 Model and the ResNext 29-8-64 Model. For the former, we obtain a top-1 error rate of \textbf{3.93\%} on CIFAR-10, and \textbf{19.08\%} on CIFAR-100. For the latter, the top-1 error rate is \textbf{3.54\%} for CIFAR-10, and \textbf{17.27\%} for CIFAR-100. Both are inferior to SRIP results from the same settings of Table~\ref{table:cifar_accuracy}.

\vspace{-0.8em}
\paragraph{Comparison with Optimization over Multiple Dependent Stiefel Manifolds \textbf{OMDSM}}
We also compare SRIP with OMDSM developed in \cite{huang2017orthogonal}, which makes a fair comparison with ours, on soft regularization forms versus hard constraint forms of enforcing orthogonality. This work trained Wide ResNet 28-10 on CIFAR-10 and CIFAR-100 and got error rates \textbf{3.73\%} and \textbf{18.76\%} respectively, both being inferior to SRIP (3.60\% for CIFAR-10 and 18.19\% for CIFAR-100).

\vspace{-0.8em}
\paragraph{Comparison with Jacobian Norm Regularization}
A recent work \cite{sokolic2017robust} propounds the idea of using the norm of the CNN Jacobian as a training regularizer. The paper used a variant of Wide ResNet \cite{zagoruyko2016wide} with 22 layers of width 5, whose original top-1 error rate was \textbf{6.66\%} on on CIFAR-10, and and reported a reduced error rate of \textbf{5.68\%} with their proposed regularizer. We trained this same model using SRIP over the same augmented full training set, achieving \textbf{4.28\%} top-1 error, that shows a large gain over the Jacobian norm-based regularizer.

\subsection{Experiments on ImageNet and SVHN}
We extend the experiments to two larger and more complicated datasets: ImageNet and SVHN (Street View House Numbers). Since SRIP clearly performs the best in the above experiments, among the proposed four, we will focus on comparing SRIP only.

\begin{wraptable}{r}{0.58\textwidth}
    \center
        \vspace{-2em}
        \caption{Top-5 error rate comparison on ImageNet.}
        \label{table:imagenet_svhn}
        \begin{tabular}{lll}
            \hline
            Model & Regularizer  & ImageNet \\
            \hline
            ResNet 34 \cite{he2016deep}  & None & 9.84 \\
            & OMDSM \cite{huang2017orthogonal}&9.68\\
            & SRIP & \textbf{8.32} \\
            \hline
            Pre-Resnet 34 \cite{he2016identity}  & None & 9.79 \\
            & OMDSM \cite{huang2017orthogonal}&9.45\\
            & SRIP & \textbf{8.79}  \\
             \hline
            ResNet 50 \cite{he2016deep} & None & 7.02 \\
            & SRIP  & \textbf{6.87}  \\
            \hline
        \end{tabular}
    \vspace{-1em}
\end{wraptable}

\vspace{-0.8em}
\paragraph{Experiments on ImageNet} We train ResNet 34, Pre-ResNet 34 and ResNet 50 \cite{he2016identity} on the ImageNet dataset with and without SRIP regularizer, respectively. The training hyperparameters settings are consistent with the original models. The initial learning rate is set to 0.1, and decreases at epoch 30, 60, 90 and 120 by a factor of 10. %As for the scheme change strategy, the initial value of $\lambda$ constant was changed to 1e-6.
The top-5 error rates are then reported on the ILSVRC-2012 val set, with single model and single-crop. \cite{huang2017orthogonal} also reported their top-5 error rates with both ResNet 34 and Pre-ResNet 34 on ImageNet. As seen in Table \ref{table:imagenet_svhn}.
SRIP clearly outperforms the best for all three models.


\vspace{-0.8em}
\paragraph{Experiments on SVHN} On the SVHN dataset, we train the original Wide ResNet 16-8 model, following its original implementation in \cite{zagoruyko2016wide} with initial learning 0.01 which decays at epoch 60,120 and 160 all by a factor of 5. We then train the SRIP-regularized version with no change made other than adding the regularizer. While the original Wide ResNet 16-8 gives rise to an error rate of \textbf{1.63\%}, SRIP reduces it to \textbf{1.56\%}.


\subsection{Summary, Remarks and Insights}
From our extensive experiments with state-of-the-art models on popular benchmarks, we can conclude the following points:
\begin{itemize}
\vspace{-0.6em}
    \item In response to the question in our title: \textit{Yes, we can gain a lot from simply adding orthogonality regularizations into training}. The gains can be found in both final achievable accuracy and empirical convergence.

    For the former, the three models have obtained (at most) 0.49\%, 0.56\%, and 0.22\% top-1 accuracy gains on CIFAR-10, and 0.41\%, 2.31\%, and 1.54\% on CIFAR-100, respectively. For the latter, positive impacts are widely observed in our training and validation curves (Figure \ref{fig:resnet_validation1} as a representative example), in particular faster and smoother curves at the initial stage.
    Note that those impressive improvements are obtained with no other changes made, and is extended to datasets such as  ImageNet and SVHN.

    \item With its nice theoretical grounds, SRIP is also the best practical option among all four proposed regularizations. It consistently performs the best in achieving the highest accuracy as well as accelerating/stabilizing training curves. It also outperforms other recent methods utilizing spectral norm \cite{yoshida2017spectral}, hard orthogonality \cite{huang2017orthogonal}, and Jacobian norm \cite{sokolic2017robust}

    \item Despite its simplicity (and potential estimation bias), SO is a surprisingly robust baseline and frequently ranks second among all four. We conjecture that SO benefits from its smooth form and continuous gradient, which facilitates the gradient-based optimization, while both SRIP and MC have to deal with non-smooth problems.

    \item DSO does not seem to be helpful. It often performs worse than SO, and sometimes even worse than the un-regularized original model. We interpret it by recalling how the matrix $W$ is constructed (Section 3 beginning): enforcing $W^TW$ close to $I$ has ``inter-channel'' effects (i.e., requiring different output channels to have orthogonal filter groups); whereas enforcing $WW^T$ close to $I$ enforce ``intra-channel'' orthogonality (i.e., same spatial locations across different filter groups have to be orthogonal). The former is a better accepted idea. Our results on DSO seems to provide further evidence (from the counter side) that orthogonality should be primarily considered for ``inter-channel'', i.e., between columns of $W$.

    \item MC brings in certain improvements, but not as significantly as SRIP. We notice that  (\ref{MC}) will approximate (\ref{MC_original}) well only when $W$ has unit columns. While we find minimizing (\ref{MC}) generally has the empirical results of approximately normalizing $W$ columns, it is not exactly enforced all the time. As we observed from experiments, large deviations of column-wise norms could occur at some point of training and potentially bring in negative impacts. We plan to look for re-parameterization of $W$ to ensure unit norms throughout training, e.g., through integrating MC with weight normalization \cite{salimans2016weight}, in future work.

    \item In contrast to many SVD-based hard orthogonality approaches, our proposed regularizers are light to use and incur negligible extra training complexity. Our experiments show that the per-iteration (batch) running time remains almost unchanged with or without our regularizers. Additionally, the improvements by regularization prove to be stable and reproducible. For example, we tried to train Wide ResNet 28-10 with SRIP from three different random initializations (all other protocols unchanged), and find that the final accuracies very stable (deviation smaller than $0.03\%$), with best accuracy $3.60\%$.




    \vspace{-0.5em}
\end{itemize}

\section{Conclusion}
We presented an efficient mechanism for regularizing different flavors of orthogonality, on several state-of-art convolutional deep CNNs \cite{zagoruyko2016wide,he2016deep,xie2017aggregated}. We showed that in all cases, we can achieve better accuracy, more stable training curve and smoother convergence. In almost all times, the novel SRIP regularizer outperforms all else consistently and remarkably. Those regularizations demonstrate outstanding generality and easiness to use, suggesting that orthogonality regularizations should be considered as standard tools for training deeper CNNs. As future work, we are interested to extend the evaluation of SRIP to training RNNs and GANs. Summarizing results, a befitting quote would be: \textit{Enforce orthogonality in training your CNN and by no means will you regret!}


\bibliographystyle{unsrt}
\bibliography{egbib}

\end{document}
