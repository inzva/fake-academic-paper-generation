
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% CUSTOM PACKAGES
\usepackage{booktabs}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows,fit,backgrounds, patterns}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}

\begin{document}

% paper title% Titles are generally capitalized except for words such as a, an, and, as,% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually% not capitalized unless they are the first or last word of the title.% Linebreaks \\ can be used within to get better formatting as desired.% Do not put math or special symbols in the title.\title{Semantic Segmentation of Pathological Lung Tissue with Dilated Fully Convolutional Networks}% PREVIOUS: Semantic Segmentation of Lung Tissue with Fully Convolutional Networks and Semi-supervised Learning% author names and IEEE memberships% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break% a structure at a ~ so this keeps an author's name from being broken across% two lines.\author{Marios~Anthimopoulos,~\IEEEmembership{Member,~IEEE,}
        Stergios~Christodoulidis,~\IEEEmembership{Member,~IEEE,}
        Lukas~Ebner,
        Thomas~Geiser,
        Andreas~Christe,
        and~Stavroula~Mougiakakou*,~\IEEEmembership{Member,~IEEE}% <-this % stops a space

% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.

\thanks{Manuscript received \today}% <-this % stops a space
\thanks{This research was carried out within the framework of the IntACT research project, supported by Bern University Hospital,``Inselspital'' and the Swiss National Science Foundation (SNSF) under Grant 156511.}% <-this % stops a space
\thanks{M. Anthimopoulos and S. Christodoulidis contributed equally to this work. The asterisk indicates the corresponding author.}% <-this % stops a space
\thanks{M. Anthimopoulos is with the ARTORG Center for Biomedical Engineering
Research, University of Bern, 3008 Bern, Switzerland, and the Department of Emergency
Medicine, Bern University Hospital ``Inselspital'', 3010 Bern, Switzerland
(e-mail: marios.anthimopoulos@artorg.unibe.ch).}% <-this % stops a space
\thanks{S. Christodoulidis is with the ARTORG Center for Biomedical Engineering
	Research, University of Bern, 3008 Bern, Switzerland (e-mail:
	stergios.christodoulidis@artorg.unibe.ch).}% <-this % stops a space
\thanks{T. Geiser is with the University Clinic for Pneumonology, Bern University Hospital ``Inselspital'',
3010 Bern, Switzerland (e-mail: thomas.geiser@insel.ch)}% <-this % stops a space
\thanks{L. Ebner and A. Christe are with the Department of Diagnostic,
Interventional and Pediatric Radiology, Bern University Hospital ``Inselspital'',
3010 Bern, Switzerland (e-mails: lukas.ebner@insel.ch; andreas.christe@insel.ch).}% <-this % stops a space
\thanks{S. Mougiakakou* is with the Department of Diagnostic, Interventional
and Pediatric Radiology, Bern University Hospital ``Inselspital'', 3010
Bern, Switzerland, and the ARTORG Center for Biomedical Engineering Research, University of Bern, 3008 Bern Switzerland (e-mail:
stavroula.mougiakakou@artorg.unibe.ch).}
\thanks{Source code available at: https://github.com/intact-project/LungNet}}% The paper headers% The only time the second header will appear is for the odd numbered pages% after the title page when using the twoside option.%% *** Note that you probably will NOT want to include the author's ***% *** name in the headers of peer review papers.                   ***% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if% you desire.% make the title area\maketitle% As a general rule, do not put math, special symbols or citations% in the abstract or keywords.\begin{abstract}

Early and accurate diagnosis of interstitial lung diseases (ILDs) is crucial for making treatment decisions, but can be challenging even for experienced radiologists. The diagnostic procedure is based on the detection and recognition of the different ILD pathologies in thoracic CT scans, yet their manifestation often appears similar. In this study, we propose the use of a deep purely convolutional neural network for the semantic segmentation of ILD patterns, as the basic component of a computer aided diagnosis (CAD) system for ILDs. The proposed CNN, which consists of convolutional layers with dilated filters, takes as input a lung CT image of arbitrary size and outputs the corresponding label map. We trained and tested the network on a dataset of 172 sparsely annotated CT scans, within a cross-validation scheme. The training was performed in an end-to-end and semi-supervised fashion, utilizing both labeled and non-labeled image regions. The experimental results show significant performance improvement with respect to the state of the art.


\end{abstract}% Note that keywords are not normally used for peerreview papers.\begin{IEEEkeywords}
Interstitial lung disease, Fully convolutional neural networks, Dilated convolutions, Texture segmentation, Semi-supervised learning
\end{IEEEkeywords}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%ADDITIONAL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% If you want to put a publisher's ID mark on the page you can do it like% this:%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}% Remember, if you use this you must call \IEEEpubidadjcol in the second% column for its text to clear the IEEEpubid mark.% use for special paper notices%\IEEEspecialpapernotice{(Invited Paper)}% For peer review papers, you can put extra information on the cover% page as needed:% \ifCLASSOPTIONpeerreview% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}% \fi%% For peerreview papers, this IEEEtran command inserts a page break and% creates the second title. It will be ignored for other modes.% \IEEEpeerreviewmaketitle%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\IEEEPARstart{I}{nterstitial} lung disease (ILD) is a group of more than 200 chronic lung disorders characterized by inflammation and scarring of the lung tissue that leads to respiratory failure. ILD accounts for 15 percent of all cases seen by pulmonologists and can be caused by autoimmune disease, genetic abnormalities, infections, drugs or long-term exposure to hazardous materials. In many cases the cause remains unknown and the disease is described as idiopathic. The diagnosis of ILD is mostly performed by radiologists and is usually based on the assessment of the different ILD pathologies in high resolution computed tomography (HRCT) thoracic scans. Early diagnosis is crucial for making treatment decisions, while misdiagnosis may lead to life-threatening complications~\cite{society1999diagnosis}. Extensive research has been conducted on the development of computer-aided diagnosis (CAD) systems, which are able to support clinicians and improve their diagnostic performance. The basic characteristics of such a system are the automatic detection and recognition of the pathological lung tissue. Pathological tissue is usually manifested as various textural patterns in the CT scan. This ILD pattern recognition procedure is traditionally performed by a local texture classification scheme that slides across the images and outputs a map of pathologies, which is later used to reach a final diagnosis. For texture recognition, a great variety of handcrafted image features and machine learning classifiers have been utilized.

Recently, deep artificial neural networks (ANN) and, in particular, deep convolutional neural networks (CNNs) have gained a lot of attention after their impressive results in the ImageNet Large Scale Visual Recognition Competition in 2012~\cite{ILSVRC15}. Networks of this kind have existed for decades~\cite{lecun98}, but have only recently managed to achieve adequate performance mainly due to the large volumes of available annotated data, the  massive parallelization capabilities of GPUs and a few design tricks. The potential benefits of deep learning techniques in medical image analysis have also been investigated recently and the first results have been promising~\cite{greenspan2016tmiguest}. In~\cite{AnthimoTMI} , we designed, trained and tested a deep CNN as a fixed-scale, local texture classifier that outperformed traditional methods in ILD pattern classification. However, training large networks on medical images can often be challenging due to the lack of databases that are adequately sized to satisfy the needs of these models. Medical data are scarce and collecting them is a difficult and time consuming process, while their annotation has to be performed by multiple specialists to ensure its validity. To this end, in~\cite{christodoulidis2017multi}, we investigated the potential use of general texture image databases, in a multi-source transfer learning scheme that yielded significant improvements in performance. One remaining limitation in these works is the local nature of the classifier that requires rigorous scanning of the input image with a sliding window, and simultaneous aggregation of the results in the output. This classification scheme can be significantly time-consuming while it may also ignore less local but useful information, if the input size is not appropriately configured.

In this study, we propose the use of a deep fully-convolutional network for the problem of ILD pattern recognition that uses dilated convolutions and is trained in an end-to-end and semi-supervised manner. The proposed CNN takes as input a lung HRCT image of arbitrary size and outputs the corresponding label map, thus avoiding the limitations of a sliding window model. Additionally, the utilization of non-labeled image regions in the learning procedure, permits robust training of larger models and proves to be particularly useful when using databases with sparse annotations.

%While last, we share the code that was used for the purposes of this paper at %https://github.com/intact-project/.
\section{Related Work}% intro
In this section, we provide a short review of the recent advances in deep learning for computer vision, followed by a brief overview of previous studies on ILD pattern classification

\subsection{Deep CNNs for Computer Vision}% DL early years
Although CNNs have existed for decades~\cite{lecun98}, they only became widely popular after their remarkable success in the ImageNet challenge of 2012~\cite{ILSVRC15}. The winning approach of the competition~\cite{krizhevsky2012imagenet}, also known as AlexNet, was a deep CNN with five convolutional and three dense layers, each followed by a rectified linear unit (ReLU), while increased strides were used for the max pooling and convolution operations to gradually down-sample the feature maps. Dropout~\cite{Dropout} and data augmentation were also utilized, in order to prevent overfitting. Since then, the proposed deep CNNs have led to continuous improvements in the results on ImageNet and other datasets, mainly by enhancing their architecture and by increasing their depth and width. The VGG network~\cite{simonyan2014very} reduced the size of the kernels to 3$\times$3, while increasing the number of layers to 19. GoogleNet~\cite{szegedy2015going} used consecutive inception modules, where different convolutional and pooling operations are performed in parallel with their outputs merged. This approach drastically reduced the number of parameters while improving the results. ResNet~\cite{he2016deep} introduced skip connections between layers which permitted the training of networks with hundreds of layers and pushed the limits of deep CNNs even further. The batch normalization (BN) technique~\cite{IoffeBN} also supported these developments, by regularizing and accelerating the training procedure.

Many of the already proposed CNNs have recently been adapted to perform semantic segmentation, rather than just image classification. The term semantic segmentation refers to the task of assigning a class label to every pixel of an image. A simple approach to this task is to use any fixed-scale classification method under a sliding window scheme and then to aggregate the results to build a label map. However, this could be highly inefficient as local image features would have to be recalculated multiple times for adjacent positions of the input window. Luckily, the convolutional layers (with the appropriate padding) in a typical CNN produce feature maps that maintain spatial correspondence with the input image. Therefore, input images of any size can be fed to the network and each pixel can be classified, on the basis of the values of the respective feature map position. This can be achieved by utilizing convolutional layers of size 1 $\times$ 1 that serve as local dense layers and, these networks are therefore often referred to as fully convolutional (FCNs). 

However, the spatial correspondence between the input and output of a CNN, can be disrupted by the use of down-sampling operations such, as strided pooling and convolution. Down-sampling of the feature maps is often used to increase the receptive field of the network with respect to the input, as well as to reduce the amount of computational load. In order to restore the original size of the input, researchers have used encoder-decoder architectures, where the encoder usually adopts a well-known architecture such as VGG~\cite{simonyan2014very} and the decoder reverses the process by mapping the feature representation back to the input data space. To this end, upsampling operations and transposed convolution ~\cite{zeiler2014visualizing} (also known as fractionally strided convolution or ``deconvolution'') have been used for semantic segmentation~\cite{long2015fully}. Alternatively, in~\cite{badrinarayanan2015segnet} and~\cite{noh2015learning}, max unpooling has been used as the inverse operation of each max pooling layer, where the pairs of pooling/unpooling layers are coupled by transferring the max indices from the encoder to the decoder. In~\cite{Unet}, a similar architecture was proposed for biomedical image segmentation, with additional skip connections that concatenate the feature maps of an encoding layer to the feature maps of the same-scale decoding layer.

%% This is the figure for the dilated convolutions\begin{figure}
    \begin{subfigure}[b]{.15\textwidth}
        \centering
        \begin{tikzpicture}
    
        \draw[step=0.25cm,color=black] (0,0) grid (2+0.25,2+0.25);    
        \draw[fill=orange, opacity=0.4] (0.75,0.75) rectangle ++(0.75,0.75);

        \end{tikzpicture}
        \caption{D = 1}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.15\textwidth}
        \centering
        \begin{tikzpicture}
        
        \draw[step=0.25cm,color=black] (0,0) grid (2+0.25,2+0.25);    
        
        \draw[fill=orange, opacity=0.4] (0.5,0.5) rectangle ++(0.25,0.25);
        \draw[fill=orange, opacity=0.4] (1,0.5) rectangle ++(0.25,0.25);
        \draw[fill=orange, opacity=0.4] (1.5,0.5) rectangle ++(0.25,0.25);
        
        \draw[fill=orange, opacity=0.4] (0.5,1) rectangle ++(0.25,0.25);
        \draw[fill=orange, opacity=0.4] (1,1) rectangle ++(0.25,0.25);
        \draw[fill=orange, opacity=0.4] (1.5,1) rectangle ++(0.25,0.25);
        
        \draw[fill=orange, opacity=0.4] (0.5,1.5) rectangle ++(0.25,0.25);
        \draw[fill=orange, opacity=0.4] (1,1.5) rectangle ++(0.25,0.25);
        \draw[fill=orange, opacity=0.4] (1.5,1.5) rectangle ++(0.25,0.25);
        
        \end{tikzpicture}
        \caption{D = 2}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.15\textwidth}
        \centering
        \begin{tikzpicture}
        
        \draw[step=0.25cm,color=black] (0,0) grid (2+0.25,2+0.25);    
        
        \draw[fill=orange, opacity=0.4] (0.25,0.25) rectangle ++(0.25,0.25);
        \draw[fill=orange, opacity=0.4] (1,0.25) rectangle ++(0.25,0.25);
        \draw[fill=orange, opacity=0.4] (1.75,0.25) rectangle ++(0.25,0.25);
        
        \draw[fill=orange, opacity=0.4] (0.25,1) rectangle ++(0.25,0.25);
        \draw[fill=orange, opacity=0.4] (1,1) rectangle ++(0.25,0.25);
        \draw[fill=orange, opacity=0.4] (1.75,1) rectangle ++(0.25,0.25);
        
        \draw[fill=orange, opacity=0.4] (0.25,1.75) rectangle ++(0.25,0.25);
        \draw[fill=orange, opacity=0.4] (1,1.75) rectangle ++(0.25,0.25);
        \draw[fill=orange, opacity=0.4] (1.75,1.75) rectangle ++(0.25,0.25);

        \end{tikzpicture}
        \caption{D = 3}
    \end{subfigure}
    \caption{Dilated convolution kernels. D denotes the dilation rate}
    \label{fig:dilation}
\end{figure}

Recently, some CNNs for semantic segmentation have been proposed that use dilated convolutions to increase the receptive field, instead of downsampling the feature maps. Dilated convolution, also called \`{a}trous, is the convolution with kernels that have been dilated by inserting zero holes (\`{a}trous in French) between the non-zero values of a kernel. This was originally proposed for efficient wavelet decomposition in a scheme also known as ``algorithme \`{a}trous''~\cite{atrous}.  Figure~\ref{fig:dilation} shows examples of kernels with different dilation rates. Dilated convolution can increase the receptive field without increasing the number of parameters, as opposed to normal convolution. Moreover, feature maps are densely computed on the original image resolution without the need for downsampling. In~\cite{YuDilatedSegm}, a CNN module with dilated convolutions was designed to aggregate multiscale contextual information and improve the performance of state-of-the-art semantic segmentation systems. The module has eight convolutional layers with exponentially increasing dilation rates (i.e. 1, 1, 2, 4, 8, 16), resulting in an exponential increase in the receptive field, while the number of parameters is only grown linearly. Similarly, expansion of the receptive field was achieved in~\cite{Enet} by integrating dilated convolutions in a bottleneck module that was designed for efficiency. In~\cite{chen2016deeplab}, the \`{a}trous spatial pyramid pooling (ASPP) scheme is proposed that uses multiple parallel dilated convolutional layers, in order to capture information from multiple scales.

\subsection{ILD Pattern Classification}

Over the last twenty years, numerous approaches have been proposed for the problem of ILD pattern recognition, which is generally regarded as a texture classification problem. Most of the proposed methods involve hand-crafted texture features which are fed to machine learning classifiers, and locally recognize lung tissue within a sliding window framework. In one of the early studies~\cite{uppaluri1999computer}, the adaptive multiple feature method (AMFM) was proposed, which utilizes a combination of gray-level histograms, co-occurrence and run-length matrices, as well as fractal analysis parameters. For the classification, a Bayesian classifier was used. In~\cite{sluimer2003computer}, a filter bank of Gaussian and Laplacian kernels was applied on the input images and the histogram moments of the responses were fed to a linear discriminant classifier. The simple, yet powerful, Local Binary Pattern (LBP) descriptor has also been proposed~\cite{sorensen2010quantitative}, combined with a k-nearest neighbors classifier. In~\cite{antimopoulos2014dct}, a random forest classifier was utilized that was trained on local DCT features. More recently, some proposed methods have adopted unsupervised feature extraction techniques, such as bag of features~\cite{gangeh2010texton, foncubierta2011using} and sparse representation models~\cite{zhao2013classification, vo2011multiscale}. 

Lately, a few methods have been proposed that utilize CNNs for lung pattern classification. Most of these are still designed under a patch-wise scheme where a square patch is fed to the CNN, while the output consists of the probabilities for this patch to belong to each class. Although the strong descriptive capabilities of modern CNNs are commonly attributed to their depth, the first studies utilized rather shallow architectures. A modified RBM that resembles a convolutional layer was used in~\cite{van2014learning}, whereas in~\cite{li2014medical}, a CNN was proposed with one convolutional and three fully-connected layers. More recently, some attempts have also been made to utilize deeper architectures. In our previous work~\cite{AnthimoTMI}, a CNN with five convolutional and three dense layers was designed and trained on ILD data, while in~\cite{christodoulidis2017multi} its performance was improved using knowledge transfer from other domains. In another study~\cite{wang2017multi}, a CNN with three convolutional and one dense layer was fed with rotational invariant Gabor-LBP representations of lung tissue patches. Finally, in~\cite{shin2016deep} and~\cite{gao2017holistic}, the authors utilized well established pretrained CNN architectures such as AlexNet and GoogleNet which were further finetuned for detecting possible ``presence/absence'' of pathologies at a slice level. However, these architectures were designed to classify natural color images with size 224 $\times$ 224, so the authors had to resize the images and artificially generate three channels by applying different Hounsfield unit (HU) windows.
\section{Materials and Methods}
This section presents the proposed fully convolutional neural network for semantic lung tissue segmentation. Prior to this, we describe the materials used for training and testing the network.

\subsection{Materials}
For the purposes of this study, we compiled a dataset of 172 HRCT scans, each corresponding to a unique ILD or healthy subject. The dataset contains 109 cases from the publicly available multimedia database of interstitial lung diseases~\cite{depeursinge2012building} by the Geneva University Hospital (HUG), along with 63 cases from Bern University Hospital - ``Inselspital'' (INSEL), as collected by the authors. The scans were acquired between 2003 and 2015 using different scanners and acquisition protocols. The INSEL scans are volumetric, while the HUG scans have a 10-15mm spacing. The slice thickness is 1-2mm for both datasets.

% INSEL DB acquisition study date interval 2006-2015% HUG DB acquisition study date interval 2003-2008

Two experienced radiologists from INSEL annotated or re-annotated ILD typical pathological patterns, as well as healthy tissue in both databases\footnote{ITK-snap was used for the annotation process, http://www.itksnap.org}. A lung field segmentation mask was also provided for each case. In total six types of tissue were considered: normal, ground glass opacity, micronodules, consolidation, reticulation and honeycombing. It should be emphasized that these annotations do not cover the entire lung field, but only the most typical manifestations of the listed ILD patterns. This protocol was followed in both databases since it permits the annotation of more scans for the same effort and thus increases data diversity. On the other hand, sparse annotations also introduce challenges. Non-annotated lung areas have to be excluded from both supervised training and evaluation. Another challenging characteristic of the databases is the uneven distribution of the considered classes across the cases. Table~\ref{tab:db-stats} provides statistics for the entire dataset, while figure~\ref{fig:slice} presents a sample CT lung slice along with the given annotations.

\begin{table}
\centering
\caption{Data statistics across the considered classes i.e. Healthy (H), Ground Glass Opacity (GGO), Micronodules (MN), Consolidation (Cons), Reticulation (Ret) and Honeycombing (HC).}
\label{tab:db-stats}
\begin{tabular}{@{}cccccccc@{}}
\toprule
~ & H     & GGO   & MN    & Cons & Ret    & HC    & Totals\\ 
\midrule
\#Pixels$\times10^{5}$  & 92.5  & 27.7  & 35.8  & 7.08 & 28.2   & 20.1  & 211.4\\ 
\#Cases        & 66    & 82    & 15    & 46   & 81     & 47    & 172\\
\bottomrule
\end{tabular}
\end{table}\begin{figure}

    \centering
        \begin{subfigure}{3.2in}
        \includegraphics[width=3.2in]{slice.png}
    \end{subfigure}
    \par\medskip
    \begin{subfigure}{2.5in}
        \centering
        \includegraphics[width=2.5in]{caption_fig2.png}
    \end{subfigure}
    ~
    
    \caption{A typical slice with annotations. The white border line denotes the lung field segmentation, the blue denotes healthy tissue, the purple micronodules and the red the honeycombing pattern.}
    \label{fig:slice}
\end{figure}\subsection{Methods}
In this study, we propose the use of a deep purely convolutional network for the problem of lung tissue semantic segmentation. The network is inspired by~\cite{YuDilatedSegm} and consists of solely convolutional layers that use dilated kernels to increase the receptive field, instead of downsampling the feature maps. This kind of network has been shown to be suitable for similar dense prediction problems that require high resolution precision. The proposed network (Fig.~\ref{fig:architecture}) has 13 convolutional layers and a total receptive field of 287$\times$287. Specifically, each of the first ten layers has 32 kernels of size 3$\times$3 and dilation rates 1, 1, 2, 3, 5, 8, 13, 21, 34 and 55, respectively. We chose not to increase the dilation rates exponentially, as is commonly done, in order to avoid extreme gridding problems that have been reported in several studies~\cite{WangCYLHHC17},~\cite{YuKF17}. Instead, we use the first terms of the Fibonacci sequence as dilation rates; this mitigates the gridding problem by providing a less steep dilation rate increase and thus denser sampling.

The output of the first 10 layers, as well as the input of the network, are concatenated, thus leading to 1+10$\times$32 = 321 feature maps, which are passed through a dropout layer with a rate of 0.5 and fed to the rest of the network. This concatenation is allowed by the lack of pooling layers and the appropriate zero padding for each convolution and brings several benefits. It permits the aggregation of features from all different scales and levels of abstraction, while it also facilitates the flow of gradients thought the network and therefore allows faster training. The last three layers have 1$\times$1 kernels and play the role of locally dense layers that reduce the feature dimensionality for each pixel from 321 to 128, 32 and finally 6, which is the number of classes considered. The output is converted into a probability distribution by the softmax function.

A BN layer follows each convolution and is based on the batch statistics in both training and test time. This is permitted, as the batch size is one, so there is always a full batch during inference. This approach has been proposed before, under the term instance normalization (InstanceNorm)~\cite{TextureNetworks}, and has exhibited good performance in texture synthesis, image stylization and image to image translation~\cite{IsolaZZE16}. InstanceNorm provides invariance to intensity and contrast shifts, which makes the features adaptive for each slice and could mitigate problems caused by different CT scanners and reconstruction kernels. We also found that adding the normalized activations to the non-normalized ones, before passing them through the ReLU function (Fig.~\ref{fig:block}) substantially improves the results. This instance normalization skip connection cancels the mean normalization of activations (when the trainable parameters have not been trained), while it performs a kind of feature contrast enhancement which reduces the importance of variance shift without providing complete invariance to the latter.

The network was trained by minimizing the categorical cross entropy using the Adam optimizer~\cite{Adam} with a learning rate of 0.0001. The dense nature of the considered classification problem combined with the sparse available annotations, resulted in two issues. Firstly, large parts of the dataset were not annotated, and so could not be used for either supervised training or testing. Secondly, the distribution of the considered classes in the dataset was highly imbalanced, a fact that can be challenging for any classification method. We tackled both problems by scaling the considered loss and accuracy with appropriate weighting schemes computed for each set. All pixels corresponding to annotated areas were assigned a weight inversely proportional to the number of samples of its class in the specific set. In this way, all classes contributed equally to the considered metrics. Furthermore, we employed a semi-supervised learning technique to additionally exploit non-labeled areas of the data. We added an extra term to the supervised loss function, which corresponds to the entropy of the network's output on the areas that do not participate in the supervised learning. This entropy minimization technique has been used in different applications such as in ~\cite{entr_min_semi_sup} yielding significant improvements in performance. Similarly in ~\cite{lee2013pseudo} the technique of pseudo-labelling was introduced, where the network classifies non-annotated regions and then uses them as ground truth for fine-tuning. Semi-supervised learning techniques of this kind are based on the cluster assumption i.e. samples from the same class tend to form compact clusters. By minimizing the entropy of the network's output, the decision boundaries are driven away from areas densely populated by learning samples. If the cluster assumption holds and there is no large overlap between the classes this method may increase the network's generalization ability. It acts equivalently to manifold learning and includes self-learning as a special case, as it increases the confidence of the classifier. The influence of the semi-supervised term is controlled by an appropriate weight, which is scaled relatively with the proportion of the unlabeled regions versus the annotated ones. Hence, the loss for a pixel $x$ with output $\mathbf{\hat{y}}$ is:

\begin{equation}
\mathcal{L}(x, \mathbf{\hat{y}}) = 
\left \{ 
\begin{matrix}
-\sum_{i=1}^{C}w_s^{i} y_i\log(\hat{y_i}), & \text{when }\mathbf{y}\text{ is given} \\\\
-\alpha w_u\sum_{i=1}^{C}\hat{y_i}\log(\hat{y_i}), & \text{otherwise}
\end{matrix}
\right.
\end{equation}

where $\mathbf{y}$ is the true label in one-hot encoding, $C$ is the number of classes, $w_s^{i}$ is the supervised weight for class $i$ (which is inversely proportional to the number of samples of the class), $\alpha$ is a scaler and $w_u$ the unsupervised weight.

The training procedure stops when the network does not significantly improve its performance on the validation set for 50 epochs. The performance is assessed in terms of weighted (balanced) accuracy, while an improvement is considered significant if the relative increase in performance is at least 0.5\%. In order to artificially increase the volume of training data and avoid overfitting, we transformed the images using flips and rotations, which are considered label-preserving in this domain. The augmentation was performed online i.e. for each training image in each epoch, one operation out of all eight combinations of flip and rotate is randomly selected and applied.

%% This is the figure with the architecture\begin{figure}
\centering
\begin{tikzpicture}
    \begin{scope}[on grid,>=stealth', rounded corners, text centered,
    		block/.style={rectangle,draw,fill=gray!15,inner sep=5pt},
    		arrow/.style={black!90, ->}]
    		
    % nodes
    \node[block] (input) [fill=white] {Input};
    \node[block] (norm) [below=of input, xshift=-25pt, fill=red!15] {InstanceNorm};
    \node[block] (add) [circle, xshift=25pt, below=of norm, fill=white] {+};
    \node[block] (conv1) [below=of add] {(3x3), D@1};
    \node[block] (conv2) [below=of conv1] {(3x3), D@1};
    \node[block] (conv3) [below=of conv2] {(3x3), D@2};
    \node[block] (conv4) [below=of conv3] {(3x3), D@3};
    \node[block] (conv5) [below=of conv4] {(3x3), D@5};
    \node[block] (conv6) [below=of conv5] {(3x3), D@8};
    \node[block] (conv7) [below=of conv6] {(3x3), D@13};
    \node[block] (conv8) [below=of conv7] {(3x3), D@21};
    \node[block] (conv9) [below=of conv8] {(3x3), D@34};
    \node[block] (conv10) [below=of conv9] {(3x3), D@55};
    
    \node[block] (merge) [below=of conv10, fill=green!15, minimum width=3cm] {Concatenate};
    \node[block] (drop)  [below=of merge, fill=yellow!20] {Dropout};
    
    \node[block] (conv11) [below=of drop] {(1x1), D@1};
    \node[block] (conv12) [below=of conv11] {(1x1), D@1};
    \node[block] (conv13)[below=of conv12] {(1x1), D@1};
    
    \node[block] (output)[below=of conv13,fill=blue!15] {Softmax Output};

    % arrows
    \draw[arrow]
    (input) edge (norm) 
    (norm) edge (add) 
    [bend left=50]
    (input) edge (add)
    [bend right=0]
    (add) edge (conv1)
    (conv1) edge (conv2) 
    (conv2) edge (conv3)
    (conv3) edge (conv4)
    (conv4) edge (conv5)
    (conv5) edge (conv6)
    (conv6) edge (conv7)
    (conv7) edge (conv8)
    (conv8) edge (conv9)
    (conv9) edge (conv10)
    (conv10) edge (merge)
    (merge) edge (drop)
    (drop) edge (conv11)
    (conv11) edge (conv12)
    (conv12) edge (conv13)
    (conv13) edge (output)
    [bend left=40] 
    (input.east) edge (merge.east) 
    (conv1.east) edge (merge.east) 
    (conv2.east) edge (merge.east)
    (conv3.east) edge (merge.east)
    (conv4.east) edge (merge.east)
    (conv5.east) edge (merge.east)
    (conv6.east) edge (merge.east)
    (conv7.east) edge (merge.east)
    (conv8.east) edge (merge.east)
    (conv9.east) edge (merge.east);
    
    \end{scope} 
\end{tikzpicture}
\caption{The architecture of the proposed network. Each gray box corresponds to a block like the one presented in Fig.~\ref{fig:block} }
\label{fig:architecture}
\end{figure}%% This is the figure with the function block\begin{figure}
\centering
\begin{tikzpicture}
    \begin{scope}[node distance=30pt, on grid,>=stealth', rounded corners, inner sep=4pt, text centered, 
    		block/.style={rectangle,draw},
    		arrow/.style={black!90, ->}]
    
    % nodes
    \node[block,fill=green!20,text width=3cm](conv) 
    {Dilated Convolution (k,k), D@x};
    \node[block,fill=red!20,xshift=-20pt,node distance=40pt,below=of conv](bn)
    {InstanceNorm};
    \node[circle,draw,fill=white,xshift=20pt,below=of bn](merge)
    {+};
    \node[block,fill=yellow!20,below=of merge](relu)
    {ReLU};
    
    % arrows
    \draw[arrow]
    (conv) edge (bn)
    (bn) edge (merge)
    (merge) edge (relu)
    [bend left=40]
    (conv) edge (merge);
    
    % background
    \begin{pgfonlayer}{background}
    \node[block, fill=gray!15, inner sep=8pt, dashed,fit=(conv) (bn) (merge) (relu)]{};
    \end{pgfonlayer}
    
    \end{scope} 
\end{tikzpicture}
\caption{The block function of the proposed architecture. (k, k) is the size of the convolution kernel and x is the dilation rate.}
\label{fig:block}
\end{figure}
% Experimental setup description (1 page tops) Tables, Figures\section{Experimental Setup and Results}

In this section, we first present the setup of the experiments conducted, followed by the corresponding results that justify the algorithmic choices of the proposed method and compare it to the state of the art.

\subsection{Experimental Setup}

Given the relatively small size of the dataset with respect to the diversity of the problem, we adopted a 5-fold cross validation (CV) scheme to ensure the validity of the results. The data splitting was performed per scan, so tissue from one case was never present in more than one set. Specifically, the 172 scans of the dataset were divided into five non-overlapping sets, with one of them having 36 and the rest 34 scans. Every time a model was tested on a specific set, the rest of the data were used for training. On average over all folds, the number of slices was 2060 for training and 515 for testing. As principal performance metric, we used the balanced accuracy (Eq.~\ref{eq:1}), averaged over the five folds.

\begin{equation}\label{eq:1}
BACC = \frac{1}{N}\sum_{i=1}^{N}\frac{c_i}{n_i}
\end{equation}

where $N$ is the number of classes, $c_i$ is the number of correctly classified samples of class $i$ and $n_i$ is the total number of samples of class $i$. Since the slices were only sparsely annotated, the accuracy was calculated over the areas of the scans where a ground truth was available. 

In order to avoid extreme class imbalances between the different sets, data splitting was performed using a simple hill climbing technique that maximizes the entropy of the class distribution for the five sets. The methods started from an arbitrary split and then randomly swapped two cases between two sets in an attempt to find a more balanced solution. If the new solution has a higher class distribution entropy (averaged over the 5 sets), we retained it and repeated the procedure until no further improvement was possible.

To minimize the number of computations and memory requirements, we discarded part of the data that lack annotations. Hence, we cropped the left and right lung on each slice and used only the ones with relevant annotations as inputs of the networks. This is permitted by the fully-convolutional nature of the tested networks that do not require a fixed input size. For the cropping, we utilized the available lung mask, while a margin of 32 pixels was added on each side to provide context that could be useful to the networks. 

The proposed method was implemented in Python\footnote{https://github.com/intact-project/LungNet} using the Keras framework\footnote{https://github.com/fchollet/keras} with the Theano~\cite{theano} back-end. All experiments were performed under Linux OS on a machine with CPU Intel Core i7-5960X @ 3.50GHz, GPU NVIDIA GeForce Titan X, and 128 GB of RAM.

\subsection{Results}
Table~\ref{II} presents a comparison between different network configurations. The bold line corresponds to the proposed CNN, while the rest correspond to models that differ from the proposed in only one aspect, as specified in the first column. The rest of the columns provide the number of model parameters, the average inference time per (single-lung) slice, and the average balanced accuracy across the five validation sets. 

\begin{table}[]

\centering
\caption{Comparison of the different network configurations}
\begin{tabular}{@{}llll@{}}

\toprule 
\begin{tabular}[c]{@{}l@{}}Network \\ configuration\end{tabular} 
& \begin{tabular}[c]{@{}l@{}}Number of \\ parameters \\ $\times10^5$\end{tabular} 
& \begin{tabular}[c]{@{}l@{}}Average \\ inference time \\ ms\end{tabular} 
& \begin{tabular}[c]{@{}l@{}}CV balanced\\ accuracy \\ \%\end{tabular} \\ 

\midrule
w/o dilated convolutions                    & 1.30 & 51 &   68.0 \\
w/o concatenation                           & 0.93 & 53 &   72.6 \\
w/o InstanceNorm                            & 1.29 & 38 &   77.9 \\
w/o InstanceNorm skip                       & 1.30 & 57 &   78.6 \\
16 kernels/layer                            & 0.47 & 51 &   79.2 \\
Exponential dilation~\cite{YuDilatedSegm}   & 1.03 & 48 &   79.5 \\
Purely supervised                           & 1.30 & 58 &   80.6 \\
9 dilated layers                            & 1.18 & 53 &   81.3 \\
\textbf{Proposed} & \textbf{1.30} & \textbf{58} &   \textbf{81.8} \\
64 kernels/layer                            & 4.23 & 82 &   82.1 \\

\bottomrule

\end{tabular}
\label{II}
\end{table}The proposed model achieved top performance with accuracy nearly equal to 82\% and inference time 58ms. The use of 64 kernels per layer instead of 32 did indeed improve the results, yet not significantly enough and with higher inference times, whereas the network with 16 kernels performed notably worse. On reducing the dilated convolutional layers from 10 to 9, we observed a relatively small reduction in the accuracy. However, we chose to keep 10 layers, since the difference in memory and time requirements was also small and because the resulting receptive field was comparable with that of the state of the art networks used for comparison. The use of semi-supervised learning yielded an improvement of nearly 1.5\%, with no additional requirements in computational resources. We also performed an experiment with exponential increase in the dilation rates of the consecutive layers, similarly to~\cite{YuDilatedSegm} i.e. 1, 1, 2, 4, 8, 16, 32 and 64. The resulting model was smaller and faster, since 2 fewer layers were required to achieve similar receptive field, however the accuracy decreased by almost 3\%. In the case where the convolutions were not dilated, the network performed poorly, because of the radical decrease of the receptive field. The accuracy of the proposed model without any normalization was substantially poorer, probably because it could not properly handle the contrast differences among the scans caused by different CT scanners and reconstruction kernels. The use of instance normalization improved the performance by adaptively normalizing the feature contrast for each input. However, this kind of normalization also normalizes the mean intensity that could be a useful feature. By adding the InstanceNorm skip connection (Fig.~\ref{fig:block}), the accuracy improved even further. We speculate this is because the mean normalization is diminished, while the resulting variance normalization is only partially invariant to contrast shifts. Finally, omitting the concatenation of the first 10 layers also resulted in significant impairment of the results, which was expected since only 32 features are considered. 

In Fig.~\ref{fig:curves} the accuracy curves for different values of $w_u$ are presented. These curves are generated by averaging over the five folds the best accuracies achieved this far by each model in each epoch. The curve for the model without the unsupervised learning was also included for comparison. The best performing configuration proved to be the one with $w_u=0.1$, which we utilized for the training of the proposed model.

\begin{figure}
    \centering
    \includegraphics[width=3.5in]{learning_curves.pdf}
    \caption{Accuracy curves for different values of $w_u$.}
    \label{fig:curves}
\end{figure}

Table~\ref{III} presents a comparison between the proposed network and three previous studies. It has to be noted that all models used the same unsupervised weight ($w_u=0.1$) and whenever batch normalization was performed, this was based on batch statistics (instance normalization) since this yielded the best results. Fig.~\ref{fig:examples} illustrates a few segmentation results for each of the models in Table~\ref{III}.

% Please add the following required packages to your document preamble:% \usepackage{booktabs}\begin{table}[]
\centering
\caption{Comparison with previous studies}
\label{III}
\begin{tabular}{@{}llll@{}}

\toprule 
Network
& \begin{tabular}[c]{@{}l@{}}Number of \\ parameters\\ $\times10^5$\end{tabular} 
& \begin{tabular}[c]{@{}l@{}}Average \\ inference time\\ ms\end{tabular} 
& \begin{tabular}[c]{@{}l@{}}CV balanced\\ accuracy \\ \%\end{tabular} \\ 

\midrule

ILD-CNN~\cite{AnthimoTMI}               & 0.9   & 237 &   72.2 \\
Segnet~\cite{badrinarayanan2015segnet}  & 335   & 111 &   73.6 \\
U-net~\cite{Unet}                       & 310   & 88  &   77.5 \\
\midrule
\textbf{Proposed} & \textbf{1.3}   & \textbf{58}  &   \textbf{81.8} \\ 

\bottomrule
\end{tabular}
\end{table}

The first line of the table refers to our previous work~\cite{AnthimoTMI}, which has been converted into a fully convolutional network so it can accept arbitrarily sized images for input. Its low accuracy is probably due to the small receptive field (33$\times$33) and the extensive pooling. This architecture was sufficient to describe the local texture of the 32$\times$32 single-class patches in~\cite{AnthimoTMI}, but could not capture higher level structure that is present in the whole-lung dataset of this study. The results of the model in Fig.~\ref{fig:examples} show its noisy output near the lung boundaries or between patterns, where context information could be useful. Segnet~\cite{badrinarayanan2015segnet} and U-net~\cite{Unet} yielded better results, with the latter being slightly faster and substantially more accurate. Both models have a very high number of parameters and large enough receptive fields to capture any relevant information. The superior performance of U-net could be attributed to its skip connections that allow features from the lower scales to directly contribute to its output. Indeed, Fig.~\ref{fig:examples} illustrates the more detailed results of U-net as opposed to the overly smoothed areas produced by Segnet. Finally, the proposed network yielded the best results, while being faster and having far fewer parameters. The output examples in Fig.~\ref{fig:examples} indicate that the proposed model manages to keep a better balance between fine details and smooth border among the different classes. Even thought it is really difficult to visually assess the performance of the system for the different classes, there are a few examples in Fig.~\ref{fig:examples} with wrong classifications on which we can comment. Firstly, parts of the broncho-vascular tree in the third row were recognized as consolidation because of their similar densities, while accentuated terminal bronchial parts, that might be physiological as well, caused the erroneous classification of healthy areas into reticulation, in the first row. Some mistakes however are also attributed in the limited number of annotated classes. For example in row 6, there are emphysematic areas (dark area in the center of the lung) that have been annotated as healthy due to their similar density. Figure~\ref{fig:cm} shows the confusion matrix of the proposed model. As expected, many of the misclassifications occur between reticulation and honeycombing due to their similar textural appearance. Moreover, healthy tissue is often confused with reticulation probably because of the 2D sections of the bronchovascular tree that could resemble reticular patterns.

\begin{figure}
    \centering
    \includegraphics[width=3in]{cm2.pdf}
    \caption{Confusion matrix of the proposed model as calculated over the cross validation scheme. The numbers represent percentages of pixels across all validation images.}
    \label{fig:cm}
\end{figure}\begin{figure*}
    \centering
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{1\textwidth}
    	\centering
        \includegraphics[width=1\textwidth]{fig6_columns.png}
    \end{subfigure}
    ~
    ~

    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{1\textwidth}
    	\centering
        \includegraphics[width=1\textwidth]{example_13.png}
    \end{subfigure}
    ~
    
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{1\textwidth}
    	\centering
        \includegraphics[width=1\textwidth]{example_11.png}
    \end{subfigure}
    ~
    
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{1\textwidth}
    	\centering
        \includegraphics[width=1\textwidth]{example_3.png}
    \end{subfigure}
    ~
    
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{1\textwidth}
    	\centering
        \includegraphics[width=1\textwidth]{example_7.png}
    \end{subfigure}
    ~
    
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{1\textwidth}
    	\centering
        \includegraphics[width=1\textwidth]{example_1.png}
    \end{subfigure}
    ~

    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{1\textwidth}
    	\centering
        \includegraphics[width=1\textwidth]{example_9.png}
    \end{subfigure}
    ~
    
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{1\textwidth}
    	\centering
        \includegraphics[width=0.8\textwidth]{caption_fig6.png}
    \end{subfigure}
    ~
    
    \caption{Output examples for the models of Table~\ref{III}. From left to right: Ground Truth, ILD-CNN, Segnet, U-net, Proposed. Each example has a different pattern annotated. From top to bottom: Healthy (Blue), Ground Glass Opacity (Purple), Micronodules (Green), Consolidation (Yellow), Reticulation (Orange) and Honeycombing (Red).}
    \label{fig:examples}
\end{figure*}%add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.%(or a blank line to force the subfigure onto a new line)% \begin{subfigure}[b]{1\textwidth}% 	\centering%     \includegraphics[width=0.9\textwidth]{assets/example_12.png}% \end{subfigure}% ~
% Concluding and small discussion \section{Conclusions}
In this study, we proposed and evaluated a deep CNN for the semantic segmentation of pathological lung tissue on HRCT slices. The CNN is designed under a fully convolutional scheme and thus can handle variable input sizes, while it was trained in an end-to-end and semi-supervised fashion. The main characteristic of the proposed network is the use of dilated convolutions along with an instance variance normalization scheme, and multi-scale feature fusion. The training and testing of the network was performed using a cross validation scheme on a dataset of 172 cases, whereas the split of the dataset into folds was performed per case. The proposed network surpassed the highest performance in previous studies, and is much more efficient in terms of memory and computation. Future work includes the modification of the model to consider the 3D nature of lung patterns, and to account for the bronchovascular tree. The former could be achieved by a direct extension of the architecture to 3D, similarly to 3D U-Net~\cite{cciccek20163d} and V-Net~\cite{milletari2016v} or by employing a multi-planar view aggregation scheme, also referred to as 2.5D,~\cite{roth2016improving}. Alternatively, a 3D post processing scheme could be used to refine the 2D segmentation output using conditional random fields or deformation models~\cite{liu2018deep, christ2016automatic, kamnitsas2017efficient}. Finally, the result of a bronchovascular segmentation method could be utilized by the network to reduce false alarms.

% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{society1999diagnosis}
B.~SOCIETY, ``The diagnosis, assessment and treatment of diffuse parenchymal
  lung disease in adults,'' \emph{Thorax}, vol.~54, no. Suppl 1, p.~S1, 1999.

\bibitem{ILSVRC15}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, A.~C. Berg, and L.~Fei-Fei, ``{ImageNet
  Large Scale Visual Recognition Challenge},'' \emph{International Journal of
  Computer Vision (IJCV)}, vol. 115, no.~3, pp. 211--252, 2015.

\bibitem{lecun98}
Y.~Lecun, L.~Bottou, Y.~Bengio, and P.~Haffner, ``Gradient-based learning
  applied to document recognition,'' \emph{Proceedings of the IEEE}, vol.~86,
  no.~11, pp. 2278--2324, Nov 1998.

\bibitem{greenspan2016tmiguest}
H.~Greenspan, B.~van Ginneken, and R.~M. Summers, ``Guest editorial deep
  learning in medical imaging: Overview and future promise of an exciting new
  technique,'' \emph{IEEE Transactions on Medical Imaging}, vol.~35, no.~5, pp.
  1153--1159, 2016.

\bibitem{AnthimoTMI}
M.~Anthimopoulos, S.~Christodoulidis, L.~Ebner, A.~Christe, and S.~Mougiakakou,
  ``Lung pattern classification for interstitial lung diseases using a deep
  convolutional neural network,'' \emph{IEEE Transactions on Medical Imaging},
  vol.~35, no.~5, pp. 1207--1216, May 2016.

\bibitem{christodoulidis2017multi}
S.~Christodoulidis, M.~Anthimopoulos, L.~Ebner, A.~Christe, and S.~Mougiakakou,
  ``Multisource transfer learning with convolutional neural networks for lung
  pattern analysis,'' \emph{IEEE Journal of Biomedical and Health Informatics},
  vol.~21, no.~1, pp. 76--84, Jan 2017.

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' in \emph{Advances in neural information
  processing systems}, 2012, pp. 1097--1105.

\bibitem{Dropout}
\BIBentryALTinterwordspacing
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov,
  ``Dropout: A simple way to prevent neural networks from overfitting,''
  \emph{J. Mach. Learn. Res.}, vol.~15, no.~1, pp. 1929--1958, Jan. 2014.
\BIBentrySTDinterwordspacing

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{szegedy2015going}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich, ``Going deeper with convolutions,'' in
  \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition}, 2015, pp. 1--9.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition}, 2016, pp. 770--778.

\bibitem{IoffeBN}
\BIBentryALTinterwordspacing
S.~Ioffe and C.~Szegedy, ``Batch normalization: Accelerating deep network
  training by reducing internal covariate shift,'' \emph{CoRR}, vol.
  abs/1502.03167, 2015.
\BIBentrySTDinterwordspacing

\bibitem{zeiler2014visualizing}
M.~D. Zeiler and R.~Fergus, ``Visualizing and understanding convolutional
  networks,'' in \emph{European conference on computer vision}.\hskip 1em plus
  0.5em minus 0.4em\relax Springer, 2014, pp. 818--833.

\bibitem{long2015fully}
J.~Long, E.~Shelhamer, and T.~Darrell, ``Fully convolutional networks for
  semantic segmentation,'' in \emph{Proceedings of the IEEE Conference on
  Computer Vision and Pattern Recognition}, 2015, pp. 3431--3440.

\bibitem{badrinarayanan2015segnet}
V.~Badrinarayanan, A.~Handa, and R.~Cipolla, ``Segnet: A deep convolutional
  encoder-decoder architecture for robust semantic pixel-wise labelling,''
  \emph{arXiv preprint arXiv:1505.07293}, 2015.

\bibitem{noh2015learning}
H.~Noh, S.~Hong, and B.~Han, ``Learning deconvolution network for semantic
  segmentation,'' in \emph{Proceedings of the IEEE International Conference on
  Computer Vision}, 2015, pp. 1520--1528.

\bibitem{Unet}
\BIBentryALTinterwordspacing
O.~Ronneberger, P.~Fischer, and T.~Brox, ``U-net: Convolutional networks for
  biomedical image segmentation,'' \emph{CoRR}, vol. abs/1505.04597, 2015.
\BIBentrySTDinterwordspacing

\bibitem{atrous}
M.~Holschneider, R.~Kronland-Martinet, J.~Morlet, and P.~Tchamitchian, ``A
  real-time algorithm for signal analysis with the help of the wavelet
  transform,'' in \emph{Wavelets}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 1990, pp. 286--297.

\bibitem{YuDilatedSegm}
F.~Yu and V.~Koltun, ``Multi-scale context aggregation by dilated
  convolutions,'' \emph{CoRR}, vol. abs/1511.07122, 2015.

\bibitem{Enet}
A.~Paszke, A.~Chaurasia, S.~Kim, and E.~Culurciello, ``Enet: {A} deep neural
  network architecture for real-time semantic segmentation,'' \emph{CoRR}, vol.
  abs/1606.02147, 2016.

\bibitem{chen2016deeplab}
L.-C. Chen, G.~Papandreou, I.~Kokkinos, K.~Murphy, and A.~L. Yuille, ``Deeplab:
  Semantic image segmentation with deep convolutional nets, atrous convolution,
  and fully connected crfs,'' \emph{arXiv preprint arXiv:1606.00915}, 2016.

\bibitem{uppaluri1999computer}
R.~Uppaluri, E.~A. Hoffman, M.~Sonka, P.~G. Hartley, G.~W. Hunninghake, and
  G.~McLennan, ``Computer recognition of regional lung disease patterns,''
  \emph{American journal of respiratory and critical care medicine}, vol. 160,
  no.~2, pp. 648--654, 1999.

\bibitem{sluimer2003computer}
I.~C. Sluimer, P.~F. van Waes, M.~A. Viergever, and B.~van Ginneken,
  ``Computer-aided diagnosis in high resolution ct of the lungs,''
  \emph{Medical physics}, vol.~30, no.~12, pp. 3081--3090, 2003.

\bibitem{sorensen2010quantitative}
L.~Sorensen, S.~B. Shaker, and M.~De~Bruijne, ``Quantitative analysis of
  pulmonary emphysema using local binary patterns,'' \emph{IEEE transactions on
  medical imaging}, vol.~29, no.~2, pp. 559--569, 2010.

\bibitem{antimopoulos2014dct}
M.~Anthimopoulos, S.~Christodoulidis, A.~Christe, and S.~Mougiakakou,
  ``Classification of interstitial lung disease patterns using local dct
  features and random forest,'' in \emph{2014 36th Annual International
  Conference of the IEEE Engineering in Medicine and Biology Society}, Aug
  2014, pp. 6040--6043.

\bibitem{gangeh2010texton}
M.~Gangeh, L.~S{\o}rensen, S.~Shaker, M.~Kamel, M.~De~Bruijne, and M.~Loog, ``A
  texton-based approach for the classification of lung parenchyma in ct
  images,'' \emph{Medical Image Computing and Computer-Assisted
  Intervention--MICCAI 2010}, pp. 595--602, 2010.

\bibitem{foncubierta2011using}
A.~Foncubierta-Rodr{\'\i}guez, A.~Depeursinge, and H.~M{\"u}ller, ``Using
  multiscale visual words for lung texture classification and retrieval,'' in
  \emph{MICCAI International Workshop on Medical Content-Based Retrieval for
  Clinical Decision Support}.\hskip 1em plus 0.5em minus 0.4em\relax Springer,
  2011, pp. 69--79.

\bibitem{zhao2013classification}
W.~Zhao, R.~Xu, Y.~Hirano, R.~Tachibana, and S.~Kido, ``Classification of
  diffuse lung diseases patterns by a sparse representation based method on
  hrct images,'' in \emph{Engineering in Medicine and Biology Society (EMBC),
  2013 35th Annual International Conference of the IEEE}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2013, pp. 5457--5460.

\bibitem{vo2011multiscale}
K.~T. Vo and A.~Sowmya, ``Multiscale sparse representation of high-resolution
  computed tomography (hrct) lung images for diffuse lung disease
  classification,'' in \emph{Image Processing (ICIP), 2011 18th IEEE
  International Conference on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2011, pp. 441--444.

\bibitem{van2014learning}
G.~van Tulder and M.~de~Bruijne, ``Learning features for tissue classification
  with the classification restricted boltzmann machine,'' in
  \emph{International MICCAI Workshop on Medical Computer Vision}.\hskip 1em
  plus 0.5em minus 0.4em\relax Springer, 2014, pp. 47--58.

\bibitem{li2014medical}
Q.~Li, W.~Cai, X.~Wang, Y.~Zhou, D.~D. Feng, and M.~Chen, ``Medical image
  classification with convolutional neural network,'' in \emph{2014 13th
  International Conference on Control Automation Robotics Vision (ICARCV)}, Dec
  2014, pp. 844--848.

\bibitem{wang2017multi}
Q.~Wang, Y.~Zheng, G.~Yang, W.~Jin, X.~Chen, and Y.~Yin, ``Multi-scale
  rotation-invariant convolutional neural networks for lung texture
  classification,'' \emph{IEEE Journal of Biomedical and Health Informatics},
  vol.~PP, no.~99, pp. 1--1, 2017.

\bibitem{shin2016deep}
H.~C. Shin, H.~R. Roth, M.~Gao, L.~Lu, Z.~Xu, I.~Nogues, J.~Yao, D.~Mollura,
  and R.~M. Summers, ``Deep convolutional neural networks for computer-aided
  detection: Cnn architectures, dataset characteristics and transfer
  learning,'' \emph{IEEE Transactions on Medical Imaging}, vol.~35, no.~5, pp.
  1285--1298, May 2016.

\bibitem{gao2017holistic}
M.~Gao, Z.~Xu, L.~Lu, A.~P. Harrison, R.~M. Summers, and D.~J. Mollura,
  ``Holistic interstitial lung disease detection using deep convolutional
  neural networks: Multi-label learning and unordered pooling,'' \emph{arXiv
  preprint arXiv:1701.05616}, 2017.

\bibitem{depeursinge2012building}
A.~Depeursinge, A.~Vargas, A.~Platon, A.~Geissbuhler, P.-A. Poletti, and
  H.~M{\"u}ller, ``Building a reference multimedia database for interstitial
  lung diseases,'' \emph{Computerized medical imaging and graphics}, vol.~36,
  no.~3, pp. 227--238, 2012.

\bibitem{WangCYLHHC17}
\BIBentryALTinterwordspacing
P.~Wang, P.~Chen, Y.~Yuan, D.~Liu, Z.~Huang, X.~Hou, and G.~W. Cottrell,
  ``Understanding convolution for semantic segmentation,'' \emph{CoRR}, vol.
  abs/1702.08502, 2017.
\BIBentrySTDinterwordspacing

\bibitem{YuKF17}
\BIBentryALTinterwordspacing
F.~Yu, V.~Koltun, and T.~A. Funkhouser, ``Dilated residual networks,''
  \emph{CoRR}, vol. abs/1705.09914, 2017.
\BIBentrySTDinterwordspacing

\bibitem{TextureNetworks}
D.~Ulyanov, A.~Vedaldi, and V.~S. Lempitsky, ``Improved texture networks:
  Maximizing quality and diversity in feed-forward stylization and texture
  synthesis,'' \emph{CoRR}, vol. abs/1701.02096, 2017.

\bibitem{IsolaZZE16}
P.~Isola, J.~Zhu, T.~Zhou, and A.~A. Efros, ``Image-to-image translation with
  conditional adversarial networks,'' \emph{CoRR}, vol. abs/1611.07004, 2016.

\bibitem{Adam}
D.~P. Kingma and J.~Ba, ``Adam: {A} method for stochastic optimization,''
  \emph{CoRR}, vol. abs/1412.6980, 2014.

\bibitem{entr_min_semi_sup}
\BIBentryALTinterwordspacing
Y.~Grandvalet and Y.~Bengio, ``Semi-supervised learning by entropy
  minimization,'' in \emph{Proceedings of the 17th International Conference on
  Neural Information Processing Systems}, ser. NIPS'04.\hskip 1em plus 0.5em
  minus 0.4em\relax Cambridge, MA, USA: MIT Press, 2004, pp. 529--536.
\BIBentrySTDinterwordspacing

\bibitem{lee2013pseudo}
D.-H. Lee, ``Pseudo-label: The simple and efficient semi-supervised learning
  method for deep neural networks,'' in \emph{Workshop on Challenges in
  Representation Learning, ICML}, vol.~3, 2013, p.~2.

\bibitem{theano}{Theano Development Team}, ``{Theano: A {Python} framework for fast computation
  of mathematical expressions},'' \emph{arXiv e-prints}, vol. abs/1605.02688,
  May 2016.

\bibitem{cciccek20163d}{\"O}.~{\c{C}}i{\c{c}}ek, A.~Abdulkadir, S.~S. Lienkamp, T.~Brox, and
  O.~Ronneberger, ``3d u-net: learning dense volumetric segmentation from
  sparse annotation,'' in \emph{International Conference on Medical Image
  Computing and Computer-Assisted Intervention}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2016, pp. 424--432.

\bibitem{milletari2016v}
F.~Milletari, N.~Navab, and S.-A. Ahmadi, ``V-net: Fully convolutional neural
  networks for volumetric medical image segmentation,'' in \emph{3D Vision
  (3DV), 2016 Fourth International Conference on}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2016, pp. 565--571.

\bibitem{roth2016improving}
H.~R. Roth, L.~Lu, J.~Liu, J.~Yao, A.~Seff, K.~Cherry, L.~Kim, and R.~M.
  Summers, ``Improving computer-aided detection using convolutional neural
  networks and random view aggregation,'' \emph{IEEE transactions on medical
  imaging}, vol.~35, no.~5, pp. 1170--1181, 2016.

\bibitem{liu2018deep}
F.~Liu, Z.~Zhou, H.~Jang, A.~Samsonov, G.~Zhao, and R.~Kijowski, ``Deep
  convolutional neural network and 3d deformable approach for tissue
  segmentation in musculoskeletal magnetic resonance imaging,'' \emph{Magnetic
  resonance in medicine}, vol.~79, no.~4, pp. 2379--2391, 2018.

\bibitem{christ2016automatic}
P.~F. Christ, M.~E.~A. Elshaer, F.~Ettlinger, S.~Tatavarty, M.~Bickel,
  P.~Bilic, M.~Rempfler, M.~Armbruster, F.~Hofmann, M.~DAnastasi
  \emph{et~al.}, ``Automatic liver and lesion segmentation in ct using cascaded
  fully convolutional neural networks and 3d conditional random fields,'' in
  \emph{International Conference on Medical Image Computing and
  Computer-Assisted Intervention}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2016, pp. 415--423.

\bibitem{kamnitsas2017efficient}
K.~Kamnitsas, C.~Ledig, V.~F. Newcombe, J.~P. Simpson, A.~D. Kane, D.~K. Menon,
  D.~Rueckert, and B.~Glocker, ``Efficient multi-scale 3d cnn with fully
  connected crf for accurate brain lesion segmentation,'' \emph{Medical image
  analysis}, vol.~36, pp. 61--78, 2017.

\end{thebibliography}
% that's all folks

\end{document}


